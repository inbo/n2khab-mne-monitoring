#+title: Steps Journal
#+author: Falk Mielke

This file documents the steps I undertook to set up the MNE database structure.

* General
** server setup
SCHEDULED: <2025-05-31 Sat>

+ set up the SQL server on a web hosting service
+ running on arch linux; super user and normal user
+ access via =ssh=:
    + =ssh -p <port> <normaluser>@<host-ip>=
+ postgres database initialized and postgis installed

full details: [[https://github.com/falkmielke/agenda/blob/main/notes/20250602135718-inbopostgis.org]]

#+begin_quote
*Currently, each new IP must be registered in* =/var/lib/postgres/data/pg_hba.conf= *to be able to connect to the database.*
#+end_quote
/cf./ [[https://wiki.archlinux.org/title/PostgreSQL#Optional_configuration]]

** sql user management
SCHEDULED: <2025-05-31 Sat>

... is done directly on the server.

#+begin_src sh :eval no
ssh -p <port> <normaluser>@<host-ip>
su - root
su - postgres
createuser -p <port> --interactive

psql -p <port>
# ALTER USER <user> WITH ENCRYPTED PASSWORD '<password>';
#+end_src

new username needs to be added to =pg_hba.conf=!


** database setup
SCHEDULED: <2025-06-02 Mon>

connect (ssh via keys):
#+begin_src sh :eval no
ssh -p <port> <normaluser>@<host-ip>
su - root
su - postgres
#+end_src


drop-create:
#+begin_src sh :eval no
# dropdb <database> -p <port>
createdb <database> -O <owner> -p <port>
#+end_src


postgis extension:

#+begin_src sh :eval no
psql -U <owner> -h <host-ip> -p <port> -d <database> -W
#+end_src

#+begin_src sql :eval no
\c <database>
CREATE EXTENSION postgis;
CREATE EXTENSION postgis_topology;
CREATE EXTENSION fuzzystrmatch;
CREATE EXTENSION postgis_tiger_geocoder;

#+end_src


** database structure
SCHEDULED: <2025-06-02 Mon>-<2025-06-05 Thu>

A python script is available here:
+ =n2khab-mne-monitoring/900_database_organization/GenerateDatabase.py=

This ideally uses a virtual environment, set up as follows:
#+begin_src sh :eval no
cd <project_folder>
python -m venv .dbinit
source .dbinit/bin/activate
pip install --upgrade pip
pip install --upgrade -r python_requirements.txt
# pip freeze > python_requirements.txt # to feed back updated requirements
#+end_src


Then to run it, with a database connection config file (as described in the script) in place:
#+begin_src sh :eval no
source .dbinit/bin/activate
# python GenerateDatabase.py
python 020_create_databases.py
#+end_src


The script works on "=csv="s which have a fixed structure to define shema's, tables, columns.

An example google file can be found [[https://docs.google.com/spreadsheets/d/1BTtG-A2ASjbF7IhYYb8FDcQLBZtSQbLbElUUYeofHNM/edit?usp=drive_link][here]],
+the =csv='s are generated by "File >> Download >> Comma Separated Values (.csv)" for each sheet in the document.+
the =csv='s are generated by "File >> Download >> OpenDocument (.ods)" which can be converted to =.csv= with the =ODStoCSVs= python function.
Those comma separated text files are stored in the =db_structure= subfolder.


Because tables and roles are organized in schema's,
it might be necessary to
#+begin_src sql
SET search_path TO public,<schema>;
SHOW search_path;
-- \dt+
#+end_src



* Biotieker, outbound

** setup database
SCHEDULED: <2025-06-05 Thu>

| database name | loceval          |
| schema        | loceval_outbound |
| peer user     | Ward             |

#+begin_src sh :eval no
psql -h <host> -p <port> -U <user> -d <database> -W
#+end_src

** location/activity calendar
SCHEDULED: <2025-06-05 Thu>

*goal:*
+ get a table with point geometry of sample target locations to the database
+ options to filter by date, potentially FAs, ...

Database creation in [[file:020_create_databases.py]]
Gradual refinement in [[file:100_sample_location_tests.qmd][100_sample_location_tests.qmd]]

TODO:
+ [X] =n2khab_data/10_raw/grtsmaster_habitats= issue: =n2khab::read_GRTSmh()= requires capitals -> was my fault
+ [X] =loceval_outbound=: =Calendar= links to =ActivityGroups=, but not to =ActivitySequences=?
  -> the sequence is in the dates.


** ODS download and conversion
SCHEDULED: <2025-06-06 Fri>

It is now possible to *download the google sheet as ODS* and convert it to csv automatically.

** constraints
SCHEDULED: <2025-06-06 Fri>

The option to specify constraints was added.
More info here: [[https://www.postgresql.org/docs/current/ddl-constraints.html]]

Most useful are
+ =UNIQUE= (e.g. for columns which are pk, but blocked by geometry fid)
+ value ranges, e.g. =CHECK (year > 0)=.


** multiple schemas
SCHEDULED: <2025-06-06 Fri>

In- and outbound schemas can be defined within the same file.
foreign keys across schema's are possible.


** pgmodeler import
SCHEDULED: <2025-06-06 Fri>

[[https://pgmodeler.io]]

A great tool to visualize relational databases.

It crashed earlier on syncing the "Plantentuin" pilot project,
but it worked fine now for importing the database and visualizing relations.

Will be re-used ad hoc later.

** sequence issues
SCHEDULED: <2025-06-06 Fri>

Due to a misconfiguration, qgis attempts to create a new location id upon attribute save.

Added an "=EXPOST=" sheet to run after succesful creation.
#+begin_src sql
GRANT USAGE ON SEQUENCE "loceval_outbound"."seq_locationcalendar_id" TO ward;
#+end_src

/update:/ Because this was such a common issue, the GRANT USAGE is now integrated for all sequences.

(sequences can be listed with =\ls+=)

** binaries, images, BLOB
SCHEDULED: <2025-06-06 Fri>

Managed to upload a binary with qgis.
Will require download/restoration tests and good disk space management.

** Visits
SCHEDULED: <2025-06-06 Fri>
a derived table showing a subset of the calendar

** stratum et al. to Location Calendar
SCHEDULED: <2025-06-10 Tue>

+ used an upstream object of =fag_grts_calendar_...= to have scheme/stratum/module/panel for calendar
+ TODO: some locations were not unique

#+begin_src R
  fag_stratum_grts_calendar_2025_attribs_sf %>%
    select(
      scheme,
      module_combo_code,
      panel_set,
      stratum,
      targetpanel,
      field_activity_group,
      grts_address_final,
      date_start
    )
#+end_src


** TODO CONCEPT
SCHEDULED: <2025-06-10 Tue>

+ will require additional schema's (=metadata=, e.g. for =username=, =activity_groups=, or sequences) and outbound/inbound databases (=collected= / =mnm_datacollection=)
+ database mirrors (dev/prod)
