#+title: Steps Journal
#+author: Falk Mielke

This file documents the steps I undertook to set up the MNE database structure.

* General
** server setup
SCHEDULED: <2025-05-31 Sat>

+ set up the SQL server on a web hosting service
+ running on arch linux; super user and normal user
+ access via =ssh=:
    + =ssh -p <port> <normaluser>@<host-ip>=
+ postgres database initialized and postgis installed

full details: [[https://github.com/falkmielke/agenda/blob/main/notes/20250602135718-inbopostgis.org]]

#+begin_quote
*Currently, each new IP must be registered in* =/var/lib/postgres/data/pg_hba.conf= *to be able to connect to the database.*
#+end_quote
/cf./ [[https://wiki.archlinux.org/title/PostgreSQL#Optional_configuration]]

** sql user management
SCHEDULED: <2025-05-31 Sat>

... is done directly on the server.

#+begin_src sh :eval no
ssh -p <port> <normaluser>@<host-ip>
su - root
su - postgres
createuser -p <port> --interactive

psql -p <port>
# ALTER USER <user> WITH ENCRYPTED PASSWORD '<password>';
#+end_src

new username needs to be added to =pg_hba.conf=!


** database setup
SCHEDULED: <2025-06-02 Mon>

connect (ssh via keys):
#+begin_src sh :eval no
ssh -p <port> <normaluser>@<host-ip>
su - root
su - postgres
#+end_src


drop-create:
#+begin_src sh :eval no
# dropdb <database> -p <port>
createdb <database> -O <owner> -p <port>
#+end_src


postgis extension:

#+begin_src sh :eval no
psql -U <owner> -h <host-ip> -p <port> -d <database> -W
#+end_src

#+begin_src sql :eval no
\c <database>
CREATE EXTENSION postgis;
CREATE EXTENSION postgis_topology;
CREATE EXTENSION fuzzystrmatch;
CREATE EXTENSION postgis_tiger_geocoder;

#+end_src


** database structure
SCHEDULED: <2025-06-02 Mon>-<2025-06-05 Thu>

A python script is available here:
+ =n2khab-mne-monitoring/900_database_organization/GenerateDatabase.py=

This ideally uses a virtual environment, set up as follows:
#+begin_src sh :eval no
cd <project_folder>
python -m venv .dbinit
source .dbinit/bin/activate
pip install --upgrade pip
pip install --upgrade -r python_requirements.txt
# pip freeze > python_requirements.txt # to feed back updated requirements
#+end_src


Then to run it, with a database connection config file (as described in the script) in place:
#+begin_src sh :eval no
source .dbinit/bin/activate
# python GenerateDatabase.py
python 020_create_databases.py
#+end_src


The script works on "=csv="s which have a fixed structure to define shema's, tables, columns.

An example google file can be found [[https://docs.google.com/spreadsheets/d/1BTtG-A2ASjbF7IhYYb8FDcQLBZtSQbLbElUUYeofHNM/edit?usp=drive_link][here]],
+the =csv='s are generated by "File >> Download >> Comma Separated Values (.csv)" for each sheet in the document.+
the =csv='s are generated by "File >> Download >> OpenDocument (.ods)" which can be converted to =.csv= with the =ODStoCSVs= python function.
Those comma separated text files are stored in the =db_structure= subfolder.


Because tables and roles are organized in schema's,
it might be necessary to
#+begin_src sql
SET search_path TO public,<schema>;
SHOW search_path;
-- \dt+
#+end_src



* Biotieker, outbound

** setup database
SCHEDULED: <2025-06-05 Thu>

| database name | loceval          |
| schema        | loceval_outbound |
| peer user     | Ward             |

#+begin_src sh :eval no
psql -h <host> -p <port> -U <user> -d <database> -W
#+end_src

** location/activity calendar
SCHEDULED: <2025-06-05 Thu>

*goal:*
+ get a table with point geometry of sample target locations to the database
+ options to filter by date, potentially FAs, ...

Database creation in [[file:020_create_databases.py]]
Gradual refinement in [[file:100_sample_location_tests.qmd][100_sample_location_tests.qmd]]

TODO:
+ [X] =n2khab_data/10_raw/grtsmaster_habitats= issue: =n2khab::read_GRTSmh()= requires capitals -> was my fault
+ [X] =loceval_outbound=: =Calendar= links to =ActivityGroups=, but not to =ActivitySequences=?
  -> the sequence is in the dates.


** ODS download and conversion
SCHEDULED: <2025-06-06 Fri>

It is now possible to *download the google sheet as ODS* and convert it to csv automatically.

** constraints
SCHEDULED: <2025-06-06 Fri>

The option to specify constraints was added.
More info here: [[https://www.postgresql.org/docs/current/ddl-constraints.html]]

Most useful are
+ =UNIQUE= (e.g. for columns which are pk, but blocked by geometry fid)
+ value ranges, e.g. =CHECK (year > 0)=.
