#+title: Steps Journal
#+author: Falk Mielke

This file documents the steps I undertook to set up the MNE database structure.

* General
** server setup
SCHEDULED: <2025-05-31 Sat>

+ set up the SQL server on a web hosting service
+ running on arch linux; super user and normal user
+ access via =ssh=:
    + =ssh -p <port> <normaluser>@<host-ip>=
+ postgres database initialized and postgis installed

full details: [[https://github.com/falkmielke/agenda/blob/main/notes/20250602135718-inbopostgis.org]]

#+begin_quote
*Currently, each new IP must be registered in* =/var/lib/postgres/data/pg_hba.conf= *to be able to connect to the database.*
#+end_quote
/cf./ [[https://wiki.archlinux.org/title/PostgreSQL#Optional_configuration]]

** sql user management
SCHEDULED: <2025-05-31 Sat>

... is done directly on the server.

#+begin_src sh :eval no
ssh -p <port> <normaluser>@<host-ip>
su - root
su - postgres
createuser -p <port> --interactive

psql -p <port>
ALTER USER <user> WITH ENCRYPTED PASSWORD '<password>';
#+end_src

+ new username needs to be added to =pg_hba.conf=!
+ consider updating =TeamMembers= table.


** database setup
SCHEDULED: <2025-06-02 Mon>

connect (ssh via keys):
#+begin_src sh :eval no
ssh -p <port> <normaluser>@<host-ip>
su - root
su - postgres
#+end_src


drop-create:
#+begin_src sh :eval no
# dropdb <database> -p <port>
createdb <database> -O <owner> -p <port>
#+end_src


postgis extension:

#+begin_src sh :eval no
psql -U <owner> -h <host-ip> -p <port> -d <database> -W
#+end_src

#+begin_src sql :eval no
\c <database>
CREATE EXTENSION postgis;
CREATE EXTENSION postgis_topology;
CREATE EXTENSION fuzzystrmatch;
CREATE EXTENSION postgis_tiger_geocoder;

#+end_src


** database structure
SCHEDULED: <2025-06-02 Mon>-<2025-06-05 Thu>

A python script is available here:
+ =n2khab-mne-monitoring/900_database_organization/GenerateDatabase.py=

This ideally uses a virtual environment, set up as follows:
#+begin_src sh :eval no
cd <project_folder>
python -m venv .dbinit
source .dbinit/bin/activate
pip install --upgrade pip
pip install --upgrade -r python_requirements.txt
# pip freeze > python_requirements.txt # to feed back updated requirements
#+end_src


Then to run it, with a database connection config file (as described in the script) in place:
#+begin_src sh :eval no
source .dbinit/bin/activate
# python GenerateDatabase.py
python 110_init_loceval.py
#+end_src


The script works on "=csv="s which have a fixed structure to define shema's, tables, columns.

An example google file can be found [[https://docs.google.com/spreadsheets/d/1BTtG-A2ASjbF7IhYYb8FDcQLBZtSQbLbElUUYeofHNM/edit?usp=drive_link][here]],
+the =csv='s are generated by "File >> Download >> Comma Separated Values (.csv)" for each sheet in the document.+
the =csv='s are generated by "File >> Download >> OpenDocument (.ods)" which can be converted to =.csv= with the =ODStoCSVs= python function.
Those comma separated text files are stored in the =db_structure= subfolder.


Because tables and roles are organized in schema's,
it might be necessary to
#+begin_src sql :eval no
SET search_path TO public,<schema>;
SHOW search_path;
-- \dt+
#+end_src


** database authentification

*** proper use of keyring
SCHEDULED: <2025-09-05 Fri>

was using it wrong
wrote a tutorial about it
https://github.com/inbo/tutorials/pull/365

*** qfield authentification
SCHEDULED: <2025-10-13 Mon>

see PRJ_MNM/160/educational

** photo capture
SCHEDULED: <2025-10-13 Mon>

+ copy whole folders from devices
+ using "biggest" file -> store as raw; convert to =DCIM/= with reduced resolution
+ script [[file:./005_photos.org]] to collect photos
+ distribution via google drive and project zips 
+ project zipping was automated [[file:./004_QField_distribute_projects.org]]
  
  
** YAD menu for maintenance scripts
SCHEDULED: <2025-10-14 Tue>

there is now a menu to click-work through maintenance scripts.
+ [[file:./090_all_maintenence_menu.sh]]

** Spontaneous postgres v18.0 major upgrade
SCHEDULED: <2025-12-03 Wed>

+ there were issues with a crashing qgis
+ due to a rolling progress bar, I suspected the server
+ server seemed fine; but because I was working on a certain (regular) user and heavily extending the qgis layer structure, I still suspected server load / server load prevention (e.g. "max connections") to cause the issue
+ decided to go for postgres setting, yet that is best done on latest version

DECISION: postgresql/postgres-libs upgrade via pacman
REVELATION: pg_update [[https://gis.stackexchange.com/q/261526][cannot handle the postGIS extension]]!
What a bummer.

+ backup was from 10:16, restore was ~11:30

Restore worked flawlessly!
... except that I now have to recreate all the database mirrors.

https://www.bytebase.com/blog/what-is-new-in-postgres-18-for-developer/

** change of superuser

I [[https://www.postgresql.org/docs/current/sql-alterdatabase.html][switched]] to a new superuser.

#+begin_src sql
ALTER USER <new_owner> WITH SUPERUSER;
ALTER USER <old_owner> WITH NOSUPERUSER;
ALTER DATABASE <db_name> OWNER TO <new_owner>;
#+end_src

https://www.postgresql.org/docs/current/sql-reassign-owned.html
#+begin_src sh
REASSIGN OWNED BY <old_owner> TO <new_owner>;
#+end_src

+ adjust =.conf= file.

But: this also involves resetting the =EXPOST= functions.

#+begin_src sql
GRANT ALL ON SCHEMA pg_catalog TO <new_owner>;
#+end_src

+ adjust structure in google sheet

#+begin_src sql
DROP FUNCTION IF EXISTS "public"."sync_mod" CASCADE;
CREATE FUNCTION sync_mod() RETURNS trigger AS $sync_mod$
BEGIN
  NEW.log_update := current_timestamp;
  NEW.log_user := current_user;
  RETURN NEW
  ;
END;
$sync_mod$ LANGUAGE plpgsql;

#+end_src

+ grep all =OWNER TO <new_owner>= and =GRANT= statements from a backup dump file, change user, execute.

+ test if everything is correct by dump-restoring the production mirror to staging

In fact, the extensions did not change hands; let's see how this plays out.


* loceval

** setup database
SCHEDULED: <2025-06-05 Thu>

| database name | loceval          |
| schema        | loceval_outbound |
| peer user     | ward             |

#+begin_src sh :eval no
psql -h <host> -p <port> -U <user> -d <database> -W
#+end_src

** location/activity calendar
SCHEDULED: <2025-06-05 Thu>

*goal:*
+ get a table with point geometry of sample target locations to the database
+ options to filter by date, potentially FAs, ...

Database creation in [[file:020_create_databases.py]]
Gradual refinement in [[file:100_sample_location_tests.qmd][100_sample_location_tests.qmd]]

TODO:
+ [X] =n2khab_data/10_raw/grtsmaster_habitats= issue: =n2khab::read_GRTSmh()= requires capitals -> was my fault
+ [X] =loceval_outbound=: =Calendar= links to =ActivityGroups=, but not to =ActivitySequences=?
  -> the sequence is in the dates.


** ODS download and conversion
SCHEDULED: <2025-06-06 Fri>

It is now possible to *download the google sheet as ODS* and convert it to csv automatically.

** constraints
SCHEDULED: <2025-06-06 Fri>

The option to specify constraints was added.
More info here: [[https://www.postgresql.org/docs/current/ddl-constraints.html]]

Most useful are
+ =UNIQUE= (e.g. for columns which are pk, but blocked by geometry fid)
+ value ranges, e.g. =CHECK (year > 0)=.


** multiple schemas
SCHEDULED: <2025-06-06 Fri>

In- and outbound schemas can be defined within the same file.
foreign keys across schema's are possible.


** pgmodeler import
SCHEDULED: <2025-06-06 Fri>

[[https://pgmodeler.io]]

A great tool to visualize relational databases.

It crashed earlier on syncing the "Plantentuin" pilot project,
but it worked fine now for importing the database and visualizing relations.

Will be re-used ad hoc later.

** sequence issues
SCHEDULED: <2025-06-06 Fri>

Due to a misconfiguration, qgis attempts to create a new location id upon attribute save.

Added an "=EXPOST=" sheet to run after succesful creation.
#+begin_src sql :eval no
GRANT USAGE ON SEQUENCE "loceval_outbound"."seq_locationcalendar_id" TO ward;
#+end_src

/update:/ Because this was such a common issue, the GRANT USAGE is now integrated for all sequences.

(sequences can be listed with =\ls+=)

** binaries, images, BLOB
SCHEDULED: <2025-06-06 Fri>

Managed to upload a binary with qgis.
Will require download/restoration tests and good disk space management.

** Visits
SCHEDULED: <2025-06-06 Fri>
a derived table showing a subset of the calendar

** stratum et al. to Location Calendar
SCHEDULED: <2025-06-10 Tue>

+ used an upstream object of =fag_grts_calendar_...= to have scheme/stratum/module/panel for calendar
+ TODO: some locations were not unique

#+begin_src r :eval no
  fag_stratum_grts_calendar_2025_attribs_sf %>%
    select(
      scheme,
      module_combo_code,
      panel_set,
      stratum,
      targetpanel,
      field_activity_group,
      grts_address_final,
      date_start
    )
#+end_src


** Schema Re-Organisation
SCHEDULED: <2025-06-10 Tue>

We now use simple names:
+ metadata
+ outbound
+ inbound

separation is based on user rights and purpose


** TeamMembers
SCHEDULED: <2025-06-10 Tue>

to refer to users who upload stuff


** N2kHabTypes
SCHEDULED: <2025-06-10 Tue>

reference to habitat types and strata
+ adding =stratum_to_expect= to LocCal
+ adding =stratum_assigned= to =Visits=


** Activities - overhaul
SCHEDULED: <2025-06-11 Wed>

Some changes and discussions with Floris about the "Activities" logic.

+ removed =ActivityGroups= and =ActivitySequences=
  + sequences are already included in the calendar data via =rank=
+ ... for the sake of =GroupedActivities=
  + contains activities in different groups
+ link to calendar via =grouped_activity_id=
  + and sequence determined by =activity_sequence=
  + =LocationCalendar= and =Visits= will have multiple lines now (all scheduled activities per visit)
  + will have to see how this plays out in qgis.

*** REMINDER: Activities/Groups DO NOT SPLIT
SCHEDULED: <2025-06-13 Fri>

It seems tempting to split out the table =ActivityGroups=.
However,
=GroupedActivities= contains duplicates of some activities,
because there are also =ActivitySequences= involved.

For the future:
a. stick with the simplified =GroupedActivites=
   + advantage: single table simplicity
   + disadvantage: queries for activities or groups get complicated (DISTINCT/GROUP BY/UNIQUE)
b. split out =Activities= + =ActivityGroups= + =ActivitySequences=
   + less workable for practical purposes
   + duplication of sequence info (also comes with the calendar)
   + but: easier reference to =ActivityGroups=, which are work units for fieldwork

*** chat protocol
You, 9:50â€¯AM
#+begin_quote
FYI er zijn activities die niet in een activity sequence terecht komen:

    activities %>%
      anti_join(
      activity_sequences,
      join_by(activity)
    )

->
    activity         activity_name        is_datacollection_meâ€¦Â¹ is_field_activity
    <fct>            <fct>                <lgl>                  <lgl>
    1 GWSURFINSTALLMAT installatiemateriaaâ€¦ FALSE                  FALSE
    2 GWMAT            staalnamemateriaal â€¦ FALSE                  FALSE
    3 SOILMAT          staalnamemateriaal â€¦ FALSE                  FALSE
    4 SURFMAT          staalnamemateriaal â€¦ FALSE                  FALSE
    5 SECDATACOLL      data uit bestaande â€¦ TRUE                   FALSE
    # â„¹ abbreviated name: Â¹ is_datacollection_method
    # â„¹ 3 more variables: is_prep_activity <lgl>, is_lab_activity <lgl>,
    #   protocol <fct>

... maar het zijn maar vijf; geen "field" activities; ik veronderstel dat ze niet in de calender terecht komen.
Ik ga ze toch meenemen naar de databank, voor de app oplossen door een "fake sequence" met naam van de activity toe te kennen, maar later kijken of ze er in qgis verschijnen of niet.
#+end_quote

Floris Vanderhaeghe, 9:51â€¯AM
#+begin_quote
Klopt. Je hebt die zaken niet nodig, enkel de locatie-attributen en de veldwerkkalender. Ik had ze niet toegevoegd als element van de data, maar ivm tonen van inhoudelijke samenhang tussen field activities. Als je gewoon activity_sequences gaat joinen aan activities, verwacht ik dat je er te veel gaat krijgen. Cf semi-joins hieronder, die ze filteren, en conditioneel stellen aan module en scheme. Puur achtergrond!

    faseqs <- [...]
    faseqs_fag_fa <- [...]
#+end_quote

You, 9:54â€¯AM
#+begin_quote
Ja, precies bij die semi-joins was ik begonnen :)
Het "te veel krijgen" is voor activities niet het gevaar, maar een activity of groep missen die later niet in de metadata-tabel staat kan de databank-logica crashen ivm. "NOT NULL"-constraints.
En ook voorbereidend desktop-werk zou in de app als een "Notitie"-laag meegevoerd kunnen worden, met een link naar een "activity".
Daarom wil ik ze er liever allemaal mee hebben.
#+end_quote

Floris Vanderhaeghe, 10:03â€¯AM, Edited
#+begin_quote
Snap ik wel vanuit databankopzicht, maar volgens mij hebben de activity sequences geen toegevoegde meerwaarde voor de uitvoerder op terrein, tov de kalender + rank. Achterwege laten is dus volgens mij even goed. Niet alles uit de POC hoeft in een databank lijkt me. Niet dat het niet mag natuurlijk.

Wat wel relevant is, is om 'overdue' taken te markeren en te prioriteren, als die op dezelfde locatie en voor hetzelfde stratum al hadden moeten gebeuren tov de daar geplande taken in een huidig datuminterval taak te kunnen uitvoeren. Met andere woorden, omgaan met veldwerkvertraging op een locatie (grts_address) x stratum, op basis van datuminterval en rank. We laten niets 'achter', vertraagde zaken zijn prioritairder en blijven op de planning staan.

Let er ook op dat field acts in eenzelfde FAG uitgevoerd moeten worden tijdens eenzelfde bezoek. Dat is de reden waarom FAG de eenheid is van de veldwerkkalender.

Wellicht vertel ik hier niets nieuws ðŸ™‚
#+end_quote

You, 10:13â€¯AM
#+begin_quote
> hebben de activity sequences geen toegevoegde meerwaarde
klopt, maar activity_sequences is de enige variabele waarin ik group en activity samen vindt. Mijn bericht boven was maar een "check", want ik begin uiteraard bij de activities tabel.

> Wat wel relevant is, is om 'overdue' taken te markeren en te prioriteren
Inderdaad, dit komt er in.

> field acts in eenzelfde FAG uitgevoerd moeten worden tijdens eenzelfde bezoek
Ja, zo ga ik het ook implementeren. Ward krijgt een takenlijst, met takengroepen en onderstappen.
#+end_quote

Floris Vanderhaeghe, 10:17â€¯AM
#+begin_quote
Goed, merci!
#+end_quote



** qgis refresher
SCHEDULED: <2025-06-11 Wed>

qgis startup recently began to throw some python errors

#+begin_src python :eval no
from osgeo import gdal, ogr, osr
#+end_src


** postgres VIEWs
SCHEDULED: <2025-06-11 Wed>

working on the database from qgis would cause confusion with all the unused, technical fields.
Views are a great way out.

#+begin_src sql :eval no
DROP VIEW IF EXISTS "outbound"."TestView";
CREATE VIEW "outbound"."TestView" AS
SELECT
    ogc_fid,
    wkb_geometry,
    locationcalendar_id,
    stratum,
    teammember_assigned,
    stratum_to_expect,
    date_start,
    date_end,
    visit_planned,
    notes
FROM "outbound"."LocationCalendar";
#+end_src

Views are now part of the database definition.
I tested that notes can be stored back to the database. HOORAY!

** first qgis tests
SCHEDULED: <2025-06-11 Wed>

+ seems nice
+ live update with the server
+ forms design
+ conditional formatting

missing qfield export.


** gdal error (qgis / qfieldsync plugin)
SCHEDULED: <2025-06-12 Thu>

solved by
#+begin_src sh :eval no
pip install --no-cache --force-reinstall gdal[numpy]=="$(gdal-config --version).*" --break-system-packages
#+end_src

Then re-install =sf= and =terra= in R:
#+begin_src r :eval no
install.packages(c("sf", "terra", "lwgeom"))
#+end_src

** qfield cloud issues
SCHEDULED: <2025-06-12 Thu>

-> go with distributing zip files for now.
[[testing/documenting qgis export][see below]] for procedure.

*** (a) copy file directly
+ pg login prompted upon opening
+ but: no error messages (â˜‡ =pg_hba.conf= must allow user)

*** (b) via qfieldcloud
+ /seems/ to work fine, BUT:
+ postgis layers only supported with subscription (12EUR/month)

** filter activities for Ward
SCHEDULED: <2025-06-12 Thu>

There are many activities.
Only some are relevant for Ward.
I intuitively selected src_r[:eval no]{c("LOCEVALAQ", "LOCEVALAQ", "LOCEVALAQ", "LOCEVALTERR", "LSVIAQ", "LSVITERR")}
and @KW, @FV added src_r[:eval no]{c("SURFLENTSAMPLPOINT", "SURFLOTSAMPLPOINT")}

https://docs.google.com/spreadsheets/d/1VPloImiATO6jjxfMmd-LVKnBUjpVEc7sAtb4yL08sIk/edit?usp=drive_link

|------+-----------------------+--------------------------------------------------------------------------------------+-------------------|
| team | activity              | activity_name                                                                        | is_field_activity |
|------+-----------------------+--------------------------------------------------------------------------------------+-------------------|
| ward | LOCEVALAQ             | locatie in aquatisch habitat evalueren en eventueel vervangen                        | true              |
| ward | LOCEVALAQ             | locatie in aquatisch habitat evalueren en eventueel vervangen                        | true              |
| ward | LOCEVALAQ             | locatie in aquatisch habitat evalueren en eventueel vervangen                        | true              |
| ward | LOCEVALTERR           | locatie in terrestrisch habitat evalueren en eventueel vervangen                     | true              |
| ward | LSVIAQ                | LSVI bepalen in aquatisch habitat                                                    | true              |
| ward | LSVITERR              | LSVI bepalen in terrestrisch habitat                                                 | true              |
| ward | SURFLENTSAMPLPOINT    | vast staalnamepunt selecteren in stilstaande wateren indien er nog geen bestond      | true              |
| ward | SURFLOTSAMPLPOINT     | vast staalnamepunt selecteren in stromende wateren indien er nog geen bestond        | true              |
|------+-----------------------+--------------------------------------------------------------------------------------+-------------------|

** prune location calendar and smush duplicate locations
SCHEDULED: <2025-06-12 Thu>

were due to multiple strata and multiple target panels

+ scheme filtered (only "GW03.3" for now)
+ activities filtered: only =LOCEVAL*=
+ multi-strata pasted as =stratum1+stratum2+...=


** testing/documenting qgis export
SCHEDULED: <2025-06-12 Thu>

(at least temporarily) stored here:
https://drive.google.com/drive/folders/1VgjQ5YZ6AxYo5fz6iAkXQiOGd4Q7uZlV?usp=drive_link

*** on server:
remember to configure =pg_hba.conf=

*** on providing computer:
requires qgis with working "QFieldSync" plugin (beware of [[gdal error (qgis / qfieldsync plugin)][gdal errors]]).

1. prepare a qgis project with connected postgis layers
2. double-check qfield layer settings: =directly access data source=
3. on qfieldsync panel: =package for qfield= -> store project in a location
4. zip the export folder
5. distribute to target device

*** on target device:
requires qgis (desktop work) or qfield (fieldwork)

1. import data set
   a. qfield: "open local file", "(+)" on lower right, "import from zip"
   b. qgis: unzip, then open folder


** layer filtering by targetpanel
SCHEDULED: <2025-06-12 Thu>

syntax:
#+begin_src SQL
'targetpanel' LIKE '%03%'
#+end_src

there are helpful "show sample"/"show all" views on (optionally unfiltered) data


** postgres minor version upgrade
SCHEDULED: <2025-06-12 Thu>

> /moment of hesitation/
Well...
... as I have nothing better on the list.

#+begin_src sh :eval no
pacman -Syu
reboot
# crossing fingers!
#+end_src

Everything seems still intact.

** two databases: dev|production
SCHEDULED: <2025-06-12 Thu>

+ =loceval= is now controlled by a different superuser than =loceval_dev=.
+ adjusted config and scripts for being able to upload to both databases
+ ... also from R


** database schema's and mirrors
SCHEDULED: <2025-06-13 Fri>

+ [X] will require additional schema's (=metadata=, e.g. for =username=, =activity_groups=, or sequences) and =outbound= / =inbound= database schemes
+ [X] database mirrors (=loceval_dev= / =loceval=)

** minor fixes
SCHEDULED: <2025-06-13 Fri>

+ boolean defaults wrong due to dbWriteTable
+ indentation fault in CreateViews loop
+ worked on the wrong qgis project :/


** inbound tables and views: FreeFieldNotes, VisitNotes, PriorVisits
SCHEDULED: <2025-06-13 Fri>

+ new view: =VisitNotes=
+ renamed table: =FreeFieldNotes=
+ LocationCalendar / FieldPreparation are now aggregated by =activity_group=, which must be DISTINCTed from the =GroupedActivities=
+ =Visits= table is linked to =LocationCalendar=, therefore does not need another geometry
+ =Visits= have each activity split up! (as opposed to =activity_group= in =LocationCalendar=)
+ There is now code to recurrently update =Visits=, without deleting prior data.
+ only retaining =grts_address_final=
+ rudimentary preparation of sample replacements
+ Views: =PriorVisits= and =FreeFieldNotes= to field preparation project
+ qgis layer styles are now stored on the github repo

... and then this happens:
#+begin_quote
Layer veldbezoeksnotities:
PostGIS error while changing attributes: ERROR: cannot update view "VisitNotes"
DETAIL: Views that do not select from a single table or view are not automatically updatable.
HINT: To enable updating the view, provide
    an INSTEAD OF UPDATE trigger
    or an unconditional ON UPDATE DO INSTEAD rule.
#+end_quote
+ added functionality to use postgres RULES
+ added to "fieldwork preparation" = outbound qgis project
+ created qgis project for inbound work (draft): happy field-working!


** database change tracking
SCHEDULED: <2025-06-16 Mon>

there are new database fields:
+ =log_creator= and =log_creation= to track creation of =FieldNotes=
+ =log_user= and =log_update= to track last change of =LocationCalendar=, =Visits=, =FieldNotes=
implemented by trigger (see =expost=)


** photo attachments
SCHEDULED: <2025-06-16 Mon>

+ blob/bytea upload in qfield did not work
+ instead, separate folder (default: DCIM) and manual storage sync
+ tested and working, though not ideal


** meaningful attribute labels
SCHEDULED: <2025-06-16 Mon>

qfield seems to use the first field as identifier
+ is there a qgis setting to override?
+ otherwise re-arrange database columns

-> on Layer properties >> Labels, work with "expressions", e.g.

#+begin_src sql
"note_date" || '  (' || represent_value("teammember_id") || ')'
#+end_src

#+begin_src sql
 CASE WHEN "visit_date_planned" IS NULL THEN '' ELSE  "visit_date_planned" || ' '  END
|| "stratum"
|| ' ' || "activity_group"
|| ' (' || CASE WHEN "teammember_id" IS NULL THEN 'unassigned' ELSE represent_value("teammember_id") END || ')'
#+end_src


** backups (1): dumps
SCHEDULED: <2025-06-16 Mon>-<2025-06-18 Wed>

*** database daily diffs

#+begin_src sh :eval no

# first: dump the diff to a patch file
pg_dump -U <user> -h <host> -p <port> -d loceval_dev -N tiger -N public -W \
    | diff latest_dump.sql - \
    > $(date +"%Y%m%d")_loceval_diff.patch

# then: patch the changes into `latest`
patch latest_dump.sql -i $(date +"%Y%m%d")_loceval_diff.patch

#+end_src

Add the above to a cronjob:
https://wiki.archlinux.org/title/Cron
+ install cronie/patch/vi with =pacman -Sy cronie patch vi=
+ start&enable system process: =systemctl start cronie.service && systemctl enable cronie.service=
+ create a =.pgpass= file ([[=.pgpass= for backups][see here]]) and enter credentials; chmod it to 600.
+ =crontab -e=
  + according to https://wiki.archlinux.org/title/Cron#Crontab_format
  + to add a new line: =22 0 * * * sh ~/backups/backup_loceval.sh 2>&1 |tee -a ~/backups/backup_loceval.log=
  + (for daily 2:22 o'clock backup; note that the server is 2h behind)

here is the backup shell script:
(for the fish shell)
#+begin_src sh :eval no
# go to the backup folder
backup_folder="~/backups/loceval/"

echo ""
echo "________________________________________________________________________________"
echo "running backup $(date +"%Y%m%d%H%M") -> ${backup_folder}"

# dump database content
pg_dump -U <user> -h <host> -p <port> -d loceval -N tiger -N public \
    | diff "${backup_folder}loceval_latest.sql" - \
    > "${backup_folder}loceval_diff_$(date +"%Y%m%d%H%M").patch"

# store incrementally
patch "${backup_folder}loceval_latest.sql" -i "${backup_folder}loceval_diff_$(date +"%Y%m%d%H%M").patch"
#+end_src

-> test was good.


*** monthly dumpall madness

There also is a script to dump all databases once monthly.
We will se how that turns out.

#+begin_src sh :eval no
# go to the backup folder
backup_folder="/home/mnm/backups/dumps"

echo ""
echo "________________________________________________________________________________"
echo "monthly full dumps $(date +"%Y%m%d") -> ${backup_folder}"

# dump loceval
pg_dump -U <user> -h <host> -p <port> -d loceval -N tiger -N public \
    > "${backup_folder}/loceval_dump_$(date +"%Y%m%d").sql"
#+end_src


*** bring 'em home

just scp it to a local folder.

#+begin_src sh :eval no
scp -r -P 2406 "<ssh_user>@<host>:~/backups/*" "/data/mnm_db_backups/"

cd /data/ && tar -cvzf mnm_db_backup.tar.gz mnm_db_backups/*
#+end_src

(optionally copy to aws s3)


*** test restore
SCHEDULED: <2025-06-19 Thu>

#+begin_src sh :eval no
[postgres@host ~]$ dropdb loceval_dev -p 2407
[postgres@host ~]$ createdb loceval_dev -O <admin> -p 2407
[postgres@host ~]$ psql -p 2407 -d loceval_dev
loceval_dev=# CREATE EXTENSION postgis;
loceval_dev=# CREATE EXTENSION postgis_topology;
loceval_dev=# CREATE EXTENSION fuzzystrmatch;
loceval_dev=# CREATE EXTENSION postgis_tiger_geocoder;
loceval_dev=# \q
[user@host backup_folder]$ psql -U <admin> -h <host> -p <port> -d loceval_dev < loceval_latest.sql
#+end_src

everything seems to be in order!


** =.pgpass= for backups
SCHEDULED: <2025-06-17 Tue>

We use a [`.pgpass` file](https://stackoverflow.com/a/2893979) for credential handling.
+ create the file
+ chmod it to 600.


** backups (2): table surgery
SCHEDULED: <2025-06-18 Wed>

The more complex problem are table-wise backups and restores.
Complex because of constraints (e.g. foreign keys).

** selectively update table data
SCHEDULED: <2025-06-18 Wed>

*** concept
i.e. change table content
+ without losing connections to other tables
+ without losing prior data and ist connection

*** DONE for Python
CLOSED: [2025-06-19 Thu 12:13] SCHEDULED: <2025-06-18 Wed>

*** DONE for R
CLOSED: [2025-06-23 Mon 16:33] SCHEDULED: <2025-06-20 Fri>

+ can change table with content from `.csv`
+ can change table with content from other database
+ characteristic columns can be chosen
+ can rename characteristic columns
+ function documentation

** data update on *production*
SCHEDULED: <2025-06-24 Tue>

... to get the latest columns
+ [X] some change suggestions @FV
  + stratum -> type
  + +targetpanel+
+ [X] apply changes


** database changes with data persistence
SCHEDULED: <2025-06-24 Tue>

i.e. we can replace tables and keep the data
as outlined above, but in one script


** adjustments Orthophoto workpackage
SCHEDULED: <2025-06-24 Tue>

=LocationCalendar= -> =PriorEvaluations=

DONE: exclude/hold columns -> Py
DONE: Views store in file and adjust


** store table relation structure
SCHEDULED: <2025-06-19 Thu>

I figured that we will need the [[selectively update table data]] in R as well,
but would have trouble getting the table relation structure.

Solution:
The structure is exported from python upon database creation and can now be read to R as well.
/(Once more, I feel clumsy in R, but it does the job.)/

** extra layers qgis orthophoto assessment
SCHEDULED: <2025-06-26 Thu>

*** orthophotos
https://geo.api.vlaanderen.be/okz/wcs
okz: orthofoto kleinschaal zomer
omz: orthofoto midschaal zomer
omw: orthofoto midschaal winter

*** beheer ANB
https://metadata.vlaanderen.be/srv/dut/catalog.search#/metadata/2d9fe2d9-c204-4f02-b228-45658e7ff7fb
(as wfs)

*** preliminary habitat map
(via Floris)

*** public habitat map
from here: https://metadata.vlaanderen.be/srv/dut/catalog.search#/metadata/45cde39a-5421-85ec-d52c-4b46-3891-e876-549ea870

non-fluent (many features x wfs)


** copying a database
SCHEDULED: <2025-06-27 Fri>

+ via script =210_copy_database.org=
+ sub-scripts are tangled from that file
  + =211_create_empty_testing.py=
  + =212_production_to_testing_example.R=
  + =213_populate_testing_db.R=
+ e.g. "production" to "testing"



** TODO issues with =Locations.ogc_fid= uniqueness
SCHEDULED: <2025-06-27 Fri>

+ problems with updating sample due to temporarily lost lookup
  + cascading of =location_id= is non-trivial (but would work)
  + yet then the =ogc_fid= is the official pk (for qgis compatibility)
  + however, =ogc_fid= produces duplicates when auto-filling

+ grts address is sufficient and necessary
+ historic field assessments (from prev samples) should also remain in the "locations"

For some reason, I could apped to the "Locations" table.
+ =location_id= is not the official pk
+ reason: qgis requires =ogc_fid= for =wkb= geometry tables
+ =ogc_fid= should be handled automatically (sequence!)
  yet =dbplyr= fails trying to upload dublicates

It might be that the "append" logic of dbplyr is wrong,
or that I shoud rather use some sort of =sf::write=.

*temporary solution:*
+ disabled the fk constraints on dependent tables
+ denial strategy: "load to memory, delete all, re-upload"


** Permanent Data Captation
CLOSED: [2025-06-30 Mon] SCHEDULED: <2025-06-16 Mon>

*The database =loceval= is kind of transient (read: active, dynamic, beautiful).*
Data prepared at home or generated in the field must be captured and moved to a completely different database: =mnmdb=.
+ =LocationCalendar= is augmented and changed dynamically -> append diffs to =mnmdb=
+ =Visits= have a field to indicate they are done, but should be moved/pruned regularly
+ =FieldNotes= must be stored irrespective of the =hide= tag; images linked and moved to storage

Solved for now:
+ python script has optional =tabula_rasa= boolean arg; if =False=, data will be (re)stored during database adjustment.
+ for R, =101_update_loceval_dev.R= was surgically adjusted
+ we can copy data between databases, e.g. via a "testing"/"staging" database


** orthophoto database project
SCHEDULED: <2025-06-30 Mon>

+ incorporated adjustments suggested by Ward
+ help on-the-fly


** n2khab type "gh"
SCHEDULED: <2025-06-30 Mon>

upon request by @WT,
an extra type is added upon upload of =N2kHabTypes=:
"gh" ("geen habitat")
to tag sample units in which the expected habitat is not present any more


** =location_id= implicit foreign key
SCHEDULED: <2025-07-07 Mon>

=location_id= is not instantiated as a foreign key to avoid conflicts due to =ON DELETE SET NULL=.

Therefore, it remains as a false lookup upon data update.

+ [X] adjusted =MNMDatabaseToolbox.py::dbTable.ListDependencies(...)=
+ [X] TODO the "old" ones also require a new location_id -> write a function to UPDATE the lookup key by grts_address

The location_id can now be restored for tables with a grts_address.


** major change =GroupedActivities=
SCHEDULED: <2025-07-09 Wed>

+ found duplicate error on work with field activity calendar
+ =*_id= in =GroupedActivities= are no sequences!
+ because they were sequences, they got overwritten upon INSERT
+ which caused the same =activity_group= on different lines to get two different id's

-> corrected.

** field activity calendar
SCHEDULED: <2025-07-09 Wed>

critical request by @WT.

+ [X] structure in place
+ [X] excel sheet
+ [X] qgis project
+ [ ] forward updates to =ExtraVisits=


** replacement locations
SCHEDULED: <2025-07-09 Wed>-<2025-07-10 Thu>

critical request by @WT.

+ [X] structure in place
+ [X] view to show only active replacements
+ [X] separate update rules for different fields -> double-rule
+ [X] qgis project
+ [ ] routine to keep previous replacements intact

+ [X] re-try dev creation and then live update


** The Great Rename
SCHEDULED: <2025-07-11 Fri>

with some requests by Karen, I found it opportune to rename a lot of things.
To be applied in
+ [X] gsheet
+ [X] upload script
+ [X] sample update procedure
+ [X] all sql query scripts (especially views)

The following replacements:
| %s/SampleLocations/SampleUnits/%g       |
| %s/samplelocation_id/sampleunit_id/%g   |
| %s/SLOC/UNIT/%g                         |
| %s/SamplePolygons/SampleCells/%g        |
| %s/ExtraVisits/Visits/%g                |
| %s/extravisit_id/visit_id/%g            |
| %s/ReplacementLocations/Replacements/%g |
| %s/lut_N2k/N2k/%g                       |

+ [X] testing on =dev=
+ [X] go live
+ [X] adjust qgis project


** SampleUnitPolygons
SCHEDULED: <2025-07-11 Fri>

KW asked for polygons for WT.
So I figured where to find the polygons.

+ Floris loads polygons for replacements
  from =file.path(locate_n2khab_data(), "10_raw/habitatmap/habitatmap.gpkg")=
+ using =sf::.[]= subsetting, I get the SampleUnits polygons
+ polygon is taken for each sampleunit
  + polygons are redundantly stored if they contain multiple sample units
+ upload to table.

Added to =EXPOST= statements:
#+begin_src sql
ALTER TABLE "outbound"."SampleUnitPolygons" DROP CONSTRAINT IF EXISTS fk_SampleUnits_SampleUnitPolygons CASCADE;
ALTER TABLE "outbound"."SampleUnitPolygons" ADD CONSTRAINT fk_SampleUnits_SampleUnitPolygons FOREIGN KEY (sampleunit_id)
REFERENCES "outbound"."SampleUnits" (sampleunit_id) MATCH SIMPLE
ON DELETE CASCADE ON UPDATE CASCADE;
#+end_src


** Replacement Archives
SCHEDULED: <2025-07-11 Fri>

Replacements are tabula-rasa'd in the sample update process.
We still want to keep the original information.

The reason we move these aside is that the replacement information has to be post-processed to adjust the SampleUnits anyways, which makes the replacement visits obsolete.


** Replacement Cells
SCHEDULED: <2025-07-11 Fri>

Use of =UNION= to display the "ongoing" replacement cells together with the regular cells.


** CellMaps
SCHEDULED: <2025-07-11 Fri>

A new table for freestyle cell mapping.
Happy drawing!


** major update
SCHEDULED: <2025-07-11 Fri>

+ changes above pushed to production
+ time to trash the qgis project... was not too bad
+ uploaded and mail sent


** mirror re-organization
SCHEDULED: <2025-07-14 Mon>

order and function to the database mirrors.

*** development
+ the dev database mirror is used for structural adjustments and development of
+ new features, mostly empty, and unstable.

*** staging
+ "staging" is a rather accurate mirror of the production database,
+ but used ad hoc in times of change to back-up the data or to test the effects
+ of structural adjustments.

*** testing
+ The testing mirror is an exact copy of the production database, and
+ regularly re-copied over. Changes to the data on "testing" are non-permanent.

*** production
+ This is the live environment with real data.
+ It is the least volatile, best backed-up of our database mirrors.


** populate testing db
SCHEDULED: <2025-07-14 Mon>

+ qgis connection changing plugin
+ extra permissions for test user

** extra info for Ward
SCHEDULED: <2025-07-14 Mon>

e.g. ANB; from "accessibility" preliminary analysis

** MILESTONE: first field data by Ward
SCHEDULED: <2025-07-14 Mon>

** minor change requests Ward
SCHEDULED: <2025-07-16 Wed>

+ ++ =recovery_hints=
+ rename accessibility_revisit
+ colors
+ accessibility to SampleUnits

#+begin_src R :eval no
rename_FieldActivityCalendar <- function(fac) {
fac <- fac %>% dplyr::rename(accessibility_revisit = acceccibility_revisit)
return(fac)
}
table_modification <- c(
"FieldActivityCalendar" = function (fac) rename_FieldActivityCalendar(fac) # (almost) anything you like
)

#+end_src


** accessibility across locations -> LocationInfos

+ accessibility is +now stored on SampleUnits+ THIS was a bad idea.
+ but it is informative for all locations on a GRTS

-> and therefore must be stored separately.
[[file:./surgery/20250722_accessibility_2loceval.sql]]

+ [X] prepare + test
+ [X] apply
+ [X] mail

also moved =recovery_hints= to =LocationInfos=


** KILL rtkgps accuracy

+ extra field


** replacement notes to sample unit

Ward is storing notes about the SampleUnits in Replacements.
Now there is an extra field which is fed back to the unit (via view).


** activity or activity_group?
SCHEDULED: <2025-07-18 Fri>

What to filter for in the grouped_activity table?
-> now went for =activity_group=
also confirmed with loceval tasks that there was none forgotten


** data type issues in sync FreeFieldNotes
SCHEDULED: <2025-07-22 Tue>

see =090_sync_FreeFieldNotes.py= -> refinement necessary


** type_is_absent
SCHEDULED: <2025-09-25 Thu>

Locations where target type was not found are now flaggable.
Questions by Floris ongoing.

** TODO major overhaul: poc update
SCHEDULED: <2025-10-09 Thu>

[[file:./surgery/20250924_poc_update_loceval.org]]


/post mortem/:

+ [ ] cascaded updates via temp tables
+ [ ] to_archive / to_update via temp table
+ [X] =mnmgwdb= adjustment: 
  =ALTER TABLE "outbound"."LocationEvaluations" RENAME COLUMN scheme TO schemes;=
+ [ ] adjust maintenance scripts
+ [ ] =mhq areas= new logic!
+ [?!] 900_database_organization/.dbinit/lib/python3.13/site-packages/pandas/io/sql.py:1737: SAWarning: Did not recognize type 'geometry' of column 'wkb_geometry'
  self.meta.reflect(bind=self.con, only=[table_name], views=True)
+ [X] Replacements
+ [X] LocationCells and ReplacementCells
+ [ ] ?? Habitatmap Polygons
+ [ ] ?? LocationAssessments (ofo's)

+ [ ] restore =testing= project

+ TODO: new subprocess: copy RandomPoints to google sheet upon update 
+ TODO: there is still an issue with orphaned SpecialActivities; e.g. WHERE grts_address = 437685 -> 8826293;

+ TODO: location_infos don't get replaced; re-work 094 in R?
  
  
* mnmgwdb

** the next database
SCHEDULED: <2025-06-30 Mon>

copied =loceval= structure
to re-activate the fieldwork calendar

+ create databases =mnmfield= and =mnmfield_dev=
+ create and authorize extra users

#+begin_src sh
createuser  -p 2407 -S <user>

psql -p <port>
ALTER USER <user> WITH ENCRYPTED PASSWORD '<password>';

vim /var/lib/postgres/data/pg_hba.conf
systemctl ...
#+end_src

+ append =~/.pgpass=
+ instantiate backups (extra script, cronjob, logs, tested)
+ database creation script


** file renames
SCHEDULED: <2025-06-30 Mon>

for better organization.


** debug init script
SCHEDULED: <2025-07-01 Tue>

working!


** fill with data
SCHEDULED: <2025-07-02 Wed>

+ [X] Locations
+ [X] LocationAssessments
+ [X] FieldActivityCalendar
+ [X] Visits


** create tester user
SCHEDULED: <2025-07-02 Wed>

(user who can only access the =_dev= databases,
so that I do not have to hijack other people's accounts all the time)


** qgis outbound app
SCHEDULED: <2025-07-02 Wed>

TODO:
+ ++landowner
+ re-upload bug


** RESTART (5): mnmgwdb
SCHEDULED: <2025-07-14 Mon>

+ with experience and input from =loceval=
+ new name: =mnmgwdb=

+ remove local replacement logic
+ all work based on =stratum=, not on =type=
  + e.g. =N2kHabType= --> =++stratum= --> =N2kHabStrata=
+ =LocationEvaluations=
  + assembled notes on all prior notes and field visits
  + hung up on the SampleLocations, which is where they go
+ unfilter GroupedActivities
+ +CellMaps+ (on hold)

** adjusted db structure
SCHEDULED: <2025-07-15 Wed>-<2025-07-17 Wed>

+ does not work on SampleUnits, but distinct SampleLocations
+ sufficient to plan fieldwork by activity_group for now,
  + later, there might be specific star-structure tables with extra data per activity group, linked to/branching from =Visits=
+ =SSPSTaPas=, a simple lookup table.
+ views
+ data inflow from =loceval=
+ =LocationInfos= to get permanent info about certain locations


** replacements: ad hoc solution
SCHEDULED: <2025-07-17 Thu>

+ via query from loceval and lookup
=sample_locations <- sample_locations %>% relocate_grts_replacements()=


** testing mirror
SCHEDULED: <2025-07-18 Fri>

+ a database mirror to work on
+ script to copy data


** sync FreeFieldNotes
SCHEDULED: <2025-07-18 Fri>

a script to copy back-and-forth FreeFieldNotes between the databases.


** WAIT sync LocationInfos
SCHEDULED: <2025-07-18 Fri>

prepared a script to sync accessibility between the two databases
but have to wait for =loceval= surgery.


** update LocationEvaluations and Replacements for =mnmgwdb=
SCHEDULED: <2025-07-18 Fri>

location evaluation work can now be script-pushed no =mnmgwdb=.

** activity or activity_group?
SCHEDULED: <2025-07-23 Wed>

What to filter for in the grouped_activity table?
-> now went for =activity_group= (confirmed by Floris)

double checked with =loceval= that no activities were missing


** duplicate watina_code
SCHEDULED: <2025-07-23 Wed>

+ added fields
+ updated views
+ removed old field


** FieldWork view
SCHEDULED: <2025-07-23 Wed>

major update.

+ concept: multiple tables per activity group
  +
+ one view to rule them all
+ shared index of =*Activities= tables
  + via =EXPOST= code
  + =COALESCE= in view


** FieldWork "app"
SCHEDULED: <2025-07-24 Thu>

+ personalized
+ translated
+ dynamic


** missing =location_id= in =mnmgwdb=
SCHEDULED: <2025-07-25 Fri>

location_id 527 / grts_address 6314694
had lost their connection.
Probably in the course of [[replacement duplication]].
Got manually re-established.


** watina codes decision
SCHEDULED: <2025-07-25 Fri>

| even | 2 | piezometer | (filter on deep 60 cm)       |
| odd  | 1 | peilbuis   | (filter almost whole length) |


** replacement duplication
SCHEDULED: <2025-07-25 Fri>-<2025-07-28 Mon>

When local replacement is performed (=loceval=),
it can happen that multiple (two or three) *sample units at the same location*
end up in different places.
For =mnmgwdb=, this means that
- the affected =SampleLocation=
- and all downstream items (=FieldworkCalender= planning, =Visits=, ...)
must be done for any of the split locations.


This is now solved in a preliminary way with [[file:./092_push_loceval_to_mnmgwdb.py]] script.

The script is complex, convoluted, and prone to fail.
Embarassing, and not to my personal standards, but functioning.

[!] the proper way to handle this will be an upstream adjustment in the POC, with a POC re-upload.


** better LocationCells sync/display
SCHEDULED: <2025-07-28 Mon>

upating the =LocationCells= (i.e. square polygons from GRTS raster)
which now include replacement locations

also, view adjusted to just show those currently in the sample.


** replacement affects =LocationInfos=
SCHEDULED: <2025-07-28 Mon>

Replacements are now stored on the server: =archive.ReplacementData=.

When pushing back and forth the =LocationInfos=, those are taken into account.

#+begin_src sql :eval no
SELECT * FROM "outbound"."LocationInfos"
WHERE grts_address = 23238
  OR grts_address = 23091910
  OR grts_address = 6314694
;
#+end_src


** DONE a stray point in Chartreuzenbos/Holsbeek
CLOSED: [2025-07-28 Mon]

SELECT * FROM "outbound"."FieldworkPlanning" WHERE grts_address = 115846;
#+begin_quote
Chartreuzenbos bij Holsbeek -> vervangcel gekozen, maar "lokale vervanging nodig" niet aan.
Is gepland als type =9190=, maar vervangcel =9130_end= vastgesteld.

Het is niet duidelijk waar de peilbuis naartoe moet:
- ofvel op de originele cel (<- "geen vervanging")
- ofwel op de vervangcel 1 (<- "vervangcel gekozen")
- ofwel nergens (andere type dan gepland).
#+end_quote


#+begin_quote
dat was mijn oefenpunt, maar blijkbaar niet alles netjes teruggezet. Bij deze staat alles normaal netjes terug op null/standaardinstelling
#+end_quote


** coordinates and google map navigation
SCHEDULED: <2025-07-29 Tue>

There is now a =Coordinates= table, and a view, providing links to start google navigation.
Both =loceval= and =mnmgwdb=.


** fieldwork app v1
SCHEDULED: <2025-07-29 Tue>

minor feature requests implemented:
+ =watina_code_used_1 TO watina_code_used_1_peilbuis=
+ =watina_code_used_2 TO watina_code_used_2_piezometer=
+ =photo_soil TO photo_soil_1_peilbuis=
+ ="inbound"."WellInstallationActivities".photo_soil_2_piezometer=
+ =lims_code TO recipient_code=
+ ="inbound"."ChemicalSamplingActivities".project_code=
+ ="inbound"."WellInstallationActivities".soilprofile_notes=
+ ="inbound"."WellInstallationActivities".soilprofile_unclear=

Further:
views and qfield projects adjusted
release veldwerk-app


** random placement
SCHEDULED: <2025-07-29 Tue>

#+begin_quote
give random positions in the vicinity of cell centre,
but not in the MHQ target area,
as polar coordinates.
Also provide cell mapping
and MHQ quadrant
#+end_quote

+ on non-forest sample units, we spare an MHQ square of 3x3m NorthWest plus 3m buffer
+ on forest sample units,
  + we spare a 16x16 square centered plus 2m buffer
  + but only if assessment was done before
+ all within the target polygon

spatially balanced random sampling:
+ use =spbal::BAS=
+ on a 1x1 fake polygon
+ scale to (radius, angle)
+ use polygon coords as *polar* and re-transform back to cartesian
  + this will make central points more likely
+ use =grts_address= as random seed for repro
+ sample many points, then intersect with cell constraints
+ keep first 20, ranked by order of appearance (not: distance)

+ upload script
+ view to only show the ones which have =loceval=


** cellmaps to mnmgwdb
SCHEDULED: <2025-07-29 Tue>

just a quick, raw transfer


** random points: refinement
SCHEDULED: <2025-07-31 Thu>

+ show Lambert for RTKGPS
+ round values
+ correct MHQ area

** replacement f**k-up
SCHEDULED: <2025-07-31 Thu>

+ single replacements were not moved due to a bug.
+ e.g. 17318 -> 4211622

** MHQ zone to polygons
SCHEDULED: <2025-07-31 Thu>

done.

** qfield audio attachments
SCHEDULED: <2025-07-31 Thu>

because why not?

** multiple bugs in local replacement sync
SCHEDULED: <2025-08-18 Mon>

most notable,
+ duplication was not applied for single type replacements -> replacements missing
+ field activity calendar was plainly duplicated, which is wrong (it diverges per type)

** handling of sequences
SCHEDULED: <2025-08-19 Tue>

sequence handling is a bit tricky: they keep auto-incrementing, causing duplicates/replacements to keep counting; foreign keys did not always work (due to CASCADE on R table update workaround); resetting them is fishy

But here we go

#+begin_src R :eval no
  sequence_key <- glue::glue('"{table_key[[1]]}".seq_{pk}')
  nextval_query <- glue::glue("SELECT last_value FROM {sequence_key};")
  current_highest <- DBI::dbGetQuery(db_target, nextval_query)[[1, 1]]
  execute_sql(
    db_target,
    glue::glue("SELECT setval('{sequence_key}', {nextval});"),
    verbose = verbose
  )

#+end_src

#+begin_src sql :eval no
SET search_path TO public,"metadata","outbound","inbound","archive";
\ds+

SELECT * FROM pg_get_serial_sequence('"outbound"."LocationInfos"', 'locationinfo_id');
SELECT nextval(pg_get_serial_sequence('"outbound"."LocationInfos"', 'locationinfo_id')) AS new_id;
SELECT setval(pg_get_serial_sequence('"outbound"."LocationInfos"', 'locationinfo_id'), 720);

#+end_src


** dynamic update field activity calendar
SCHEDULED: <2025-08-20 Wed>

replacements required return to field activity calendar
because different types replaced to separate grts just duplicating FAC was insufficient
FACal may diverge between types.


** safety net: memory backup whilst upload/cascade
SCHEDULED: <2025-08-21 Thu>

typical errors to catch:
+ column renames
+ key duplicates
+ null constraints

should all be safe now: upload stops and restores previous data
based on table and dependencies

** two replacements to the same grts
SCHEDULED: <2025-08-21 Thu>

special case where two replacements end up on the same GRTS
produced duplicate location
errors should be caught now, but keeping an eye open.

Temporarily, this will cause double planning for Tom.


** minor change requests Y
SCHEDULED: <2025-08-25 Mon>

+ correct google link for navigation
+ type to FieldWork view


** correct upload procedure
SCHEDULED: <2025-08-25 Mon>

some errors because SSPSTaPas had become NULL
then, the "identical" request upon fieldwork calendar upload does not find items
and erroneously re-uploads them

work in progress..


** duplicate LocationInfos and disconnected Visits
SCHEDULED: <2025-08-26 Tue>

There was a quadruplicate row in LocationInfos for grts  871030
mnmgwdb# DELETE FROM "outbound"."LocationInfos" WHERE locationinfo_id IN (698, 697, 696);

the query
loceval# SELECT * FROM "outbound"."LocationInfos" WHERE grts_address = 871030;
returns two identical rows, which is plausible, because they are two types on different =location_id=.

There were visits without a fieldworkcalendar_id; corrected in the 093_ script.


** disconnected FieldworkCalendar events
SCHEDULED: <2025-08-26 Tue>

during the update =093_=, fieldworkcalendar_id is lost in some dependent tables;
most importantly =Visits=.

NOTES
+ this happens NOT during =update_cascade_lookup= (UCL) of fieldworkcalendar
+ UCL of =Visits= still fails for duplicates!
+ some recovery procedures are assembled in 093 with =if (FALSE)=
  might be good to outsource them (e.g. WIA and CSA)
+ Deletion was one issue, because it did not CASCADE.
+ [X] TODO there are still 99 WIA disconnected (also no visit_id); should be restored via sloc/activity

** WIA/CSA quickfix
SCHEDULED: <2025-08-28 Thu>

script =901_*= for temporary re-link
as well as several recipes to delete missing connections
yet they return by means of =093_*=

** multi-polygon cell maps
SCHEDULED: <2025-09-04 Thu>

... now all get random points, by means of a simple =%>% sf::st_union()=.


** Fieldwork Planning ++ =has_installation= and =count_days_ws=
SCHEDULED: <2025-09-10 Wed>
SCHEDULED: <2025-09-10 Wed>

quick and welcome request by TDD
then extra request to count days.


** data consistency dashboard -> improved consistency
SCHEDULED: <2025-09-22 Mon>

+ [[file:./040_consistency_dashboard.qmd]]
+ used immediately to re-link several disconnected entries [[file:./surgery/20250922_missing_stratum_mnmgwdb.sql]]


** TODO [#A] fieldwork_id conflict
SCHEDULED: <2025-09-24 Wed>

For some reason, CSA and WIA got the same IDs.
This should not have happened.
I temporarily increased the CSA ID's by 10000;
For a more permanent fix:

+ [ ] =skip_sequence_reset= for those tables
+ [ ] implement a check for fieldwork_id uniqueness (check view =FieldWork=)
+ [ ] find the error, putatively in poc update procedure

** type_is_absent -> excluded
SCHEDULED: <2025-09-25 Thu>

sample units where biotics could not find the target type are marked;
those are now flagged as "excluded" in fieldwork planning

** type_is_not_absent_anymore -> unexcluded
SCHEDULED: <2025-09-29 Mon>

forgot to restore those that received a type in ex post correction.

** tap water!
SCHEDULED: <2025-11-14 Fri>

new columns to indicate the source of field water
ALTER TABLE "inbound"."WellInstallationActivities" ADD COLUMN used_water_from_tap boolean DEFAULT NULL;
COMMENT ON COLUMN "inbound"."WellInstallationActivities".used_water_from_tap IS E'whether or not tap water was used for installation';

ALTER TABLE "inbound"."WellInstallationActivities" ADD COLUMN used_water_source varchar DEFAULT NULL;
COMMENT ON COLUMN "inbound"."WellInstallationActivities".used_water_source IS E'source of the water used for facilitating installation';

Ik heb aan de tab "installatie" toegevoegd
1. `used_water_from_tap` = "gebruik kraanwater" -> gelieve aanvinken indien kraanwatergebruik
2. `used_water_source` = "gebruik water bron" -> optioneel aangeven waar spoelwater vandaan kwam, ook bij niet-kraanwater uit de omgeving (bv. "WC HT Brussel", "beekje dichtbij", "vlesje Spa van de colruyt")

** non-absent placement 4163858
SCHEDULED: <2025-11-19 Wed>

(via Lise)
+ 4163858<29329682//7140_mrd was "excluded" for "type_is_absent", because in =loceval= the note said "Ander type binnen mozaÃ¯ek"; however replacement and mapping were good.
+ conclusion: this was a victim of the auto-detection of "absent" cells.

loceval=# UPDATE "outbound"."SampleUnits" SET type_is_absent = FALSE WHERE grts_address = 4163858;
mnmgwdb=# UPDATE "outbound"."FieldworkCalendar" SET excluded = FALSE, excluded_reason = NULL WHERE grts_address IN (4163858, 29329682);


** QField Anormalities (I): Misplaced Archive Tags

Some visits have been done but their table entry is flagged as archived, whereas an empty, updated version is unarchived but does not contain the =visit_done=.

Strategy:
+ first delete the empty, untouched, but "non-archive" activities of all types: [[file:./surgery/20251128_reconnect_calendar.org]]
  + this removed 148 calendar entries (and associated Special Activities)
+ then, fix the calendar update: [[file:./092_update_facalendar.R]]

GOOD TO KNOW:
- Entries can be reactivated AND changed in the same round.
  + Although, not exactly at the same round: had to run the calendar update script twice.
- In the case of date adjustments: =date_end= was updated afterwards. Makes sense.


** QGIS mulit-layer style
SCHEDULED: <2025-12-03 Wed>

*Advanced cake symbology.*
I tried around how to display the intersection of "planning/done", "visit/done", possible "issues", and on the other hand the different ACTIVITYGROUPS.

+ combinations of =done_planning= and =visit_done= now yield a symbol fill color
+ symbol rotation by activity group; yet those are in different layers


* Major R Code Overhaul (202508)
** Code File Organization
SCHEDULED: <2025-08-31 Sun>

The former =MNMDatabaseToolbox= has been split into multiple files:

+ =MNMLibraryCollection.R=: list of and import functions for common libraries
+ =MNMDatabaseConnection.R= a "sqiss army knife" with tools for databank interaction
+ =MNMDatabaseTesting.R=: collection of tests of the database connection
+ =MNMDatabaseToolbox.R=: scripts for easier upload
+ =MNMCodeCementary.R=: functions which are obsolete, but too good to throw away

** Database Connection
SCHEDULED: <2025-09-01 Mon>

The database connection starts as a list with all the connection attributes which were there before.
Then, functions are added for easier database interaction.

Please refer to the "table of contents" in this file.


** Handling Spatial Tables
SCHEDULED: <2025-09-03 Wed>

It turns out that handling of spatial data can be simplified by converting
=sf= objects into =as_tibble=.
They retain the spatial info but work exactly as tibbles in the conversion.


** major overhaul
SCHEDULED: <2025-09-02 Tue>-<2025-09-03 Tue>

Incorporating the new logic into the old scripts.
+ [ ] =030_copy_database.org=
+ [X] =070_update_POC.qmd=
+ [X] =093_update_facalendar.R=
+ [X] =094_replaced_LocationCells.R=
+ [X] =096_update_wgs84_coordinates.R=
+ [X] =120_upload_loceval.R=
+ [X] =131_mhq_areas.R=
+ [ ] =220_upload_mnmgwdb.R=
+ [X] =230_random_placementpoints.R=
+ [X] =231_mhq_areas.R=

** some issues on the way
*** 871030
+ double replacement by =loceval= (same cell, same target, two types)
+ replaced for =mnmgwdb=
+ gets fed back to =loceval= as two =LocationInfos=

==> need to un-replace cells
==> postponed: another overhaul is coming.

manually removed; see [[file:surgery/20250903_missing_871030.sql]]

*** DONE duplicate LocationCells
SCHEDULED: <2025-09-04 Thu>

Those were regular =LocationCells= which also turned in for a second time due to the =extra_cells= and replacement procedure.
Solved by an extra extra-cell anti-join.

#+begin_src sql :eval no
SELECT * FROM "metadata"."LocationCells" WHERE location_id IN (42, 181, 219, 409, 431, 468, 471, 476, 478);
#+end_src

*** DONE Disconnected Visits
CLOSED: [2025-09-04 Thu]
none any more.

#+begin_src sql :eval no
SELECT * FROM "inbound"."Visits" WHERE fieldworkcalendar_id IS NULL;
#+end_src


*** DONE Double Field Work
CLOSED: [2025-09-04 Thu]
gone.

#+begin_src sql :eval no
SELECT DISTINCT visit_id, count(*) AS n FROM "inbound"."FieldWork" GROUP BY visit_id ORDER BY n DESC;
#+end_src


** MILESTONE ran POC update for =mnmgwdb=
SCHEDULED: <2025-09-18 Thu>

Done. With some bumps.

+ We now have a bunch of new field activities. I begin to think that, long term, a testing mirror with just a subset of the data would be good to have.
+ 871030 required manual adjustment of the stratum, because stratum initiation lookup faulted. Should be good now and forever.
+ 219694 produced duplicates in FieldWork by duplicate WIA/CSA; more trouble expected
+ tables lost links repeatedly; will have to double check the =update_cascade_lookup=; but we can live with that given there now is a script to link them back
+ some new sf::geometry issue crashes the MHQ Polygons and random points. Because why not.
+ merge back dbinit_pocupdate



* learning the hard way

** =dbWriteTable(..., overwrite = TRUE, ...)= is a bad idea!
SCHEDULED: <2025-07-07 Mon>

Overwriting a table is a "DROP/CREATE" procedure and therefore leads to a loss in access roles.

+ Just don't do it!
+ Instead, manually "DELETE+INSERT",
  wherein the insert can be =dbWriteTable(..., overwrite = FALSE, append = TRUE, ...)=.

** =dbReadTable(...)= is better avoided, too
SCHEDULED: <2025-07-07 Mon>

... as it leads to data table inconsistencies with =dplyr.=
Specifically, there is trouble with combining =DBI::dbReadTable= and =dplyr::tbl= (example: get =grts_address= for combined =Locations=.
+ =distinct= does not mean =unique= :/


** loss of links due to "=append_tabledata="
SCHEDULED: <2025-07-07 Mon>

The function =append_tabledata= checks which data rows are present in the table,
and only uploads the novel ones.
Problem is that foreign keys are neglected:
+ If the old data was linked to another table,
+ and that table was also updated prior to the re-upload,
+ then links will be lost.

Actually, I had =update_datatable_and_dependent_keys()= in place for this.
Time to use it.

+ [X] defined =update_cascade_lookup= to bring the procedures together and handle reference columns.

** KILL update rule avoidance
CLOSED: [2025-09-18 Thu 11:11] SCHEDULED: <2025-07-08 Tue>

Update rules are nice,
but for some technical tasks, it is better to circumvent them.

+ implement (de-)activating update rules
  no better not


** TODO Polygon Memory Size
SCHEDULED: <2025-07-11 Fri>

The habitat map spatraster is quite big;
and laptop memory is limited.

I got trouble once because too little memory was left to get sample unit polygons.
-> is there a long term solution?

** appending locations
SCHEDULED: <2025-07-18 Fri>

creating a new locations works when explicitly passing the next =ogc_fid= AND =location_id=.
Omitting either of them in the insert will fail. :shrug:


** take primary keys seriously
SCHEDULED: <2025-07-28 Mon>

During replacement relocation [[replacement duplication][(see above)]], I at some point deactivated pk checks for some tables.
I later found out that the checks correctly triggered errors: rows were duplicated prior to upload, which was logically correct, but not general (and thus overlooked).

Lesson repeated: pk constraints are good; always assume that they work well.


** major programming bugs
SCHEDULED: <2025-07-31 Thu>

+ wrong IF clause (just switched arguments) on forest/assessment MHQ determination
+ replacement procedure: obob <= instead of < 1 cell (was originally only for duplication)

** TODO [#A] fieldwork_id conflict
some script updates WIA and CSA,
but neglects uniqueness of the =fieldwork_id=.
Temporarily solved by increasing the fieldwork_id

see [[file:./surgery/20250924_fieldwork_id_conflict.sql]]


** restoring production database from dumps

Times this happened: ð¸ð·
+ POC update failure after successful test on staging
  + it might be that the connection was misconfigured to point to production, I cannot reproduce why
+ too eager major version upgrade postgres v17 -> v18

*Note* that dumps *do not contain functions* which are defined on the server; e.g. =sync_mod=.
In consequence, triggers get lost (throw an error upon attempted recreation).

To restore them, the database owner must get access to =pg_catalog=: =GRANT ALL ON SCHEMA pg_catalog TO <user>;=

And then
#+begin_src sql
CREATE FUNCTION sync_mod() RETURNS trigger AS $sync_mod$
BEGIN
  NEW.log_update := current_timestamp;
  NEW.log_user := current_user;

  RETURN NEW
  ;
END;
$sync_mod$ LANGUAGE plpgsql;
#+end_src


* General (II)

** REMINDER fk columns which allow NULL
SCHEDULED: <2025-10-14 Thu>

by default, all fk are allowed to take NULL value
required because they are set NULL on cascaded update during re-upload
general concept:
+ there are "characteristic columns" which define unique entry
+ these are used to keep keys up to date; consistency check is valuable
+ key is used for quick joins


** DONE Replacement Unit logic
CLOSED: [2025-07-10 Thu 12:38] SCHEDULED: <2025-06-16 Mon>

/brainstorm:/
+ LocationCalendar -> must be linked
+ geometry -> must be updated
+ Visits -> must be adjusted, e.g. by linking in =inbound.Replacements=
+ join =Replacements= to =Visits= if they have an entry


** python updates

update all requirements:
#+begin_src sh :eval no
cp python_requirements.txt python_requirements.bak
cat python_requirements.txt | cut -f1 -d= | xargs pip install -U
pip freeze > python_requirements.txt
#+end_src


GDAL requires special attention:
python bindings version always needs to match system version (which is not generally the latest one available on PyPI).
Its line can be deleted from the =requirements.txt= prior to the update.

#+begin_src sh :eval no
pip install --no-cache --force-reinstall gdal[numpy]=="$(gdal-config --version).*" --break-system-packages
#+end_src


** quick checks

#+begin_src sql :eval no
SET search_path TO public,"metadata","outbound","inbound";
SELECT DISTINCT assessment_done, COUNT(*) AS n FROM "outbound"."LocationAssessments" GROUP BY assessment_done;
SELECT * FROM "outbound"."FieldActivityCalendar" WHERE done_planning;
SELECT * FROM "outbound"."Replacements" WHERE is_inappropriate OR is_selected OR (notes IS NOT NULL);
SELECT * FROM "inbound"."Visits" WHERE NOT (log_user = 'update') AND NOT (log_user = 'falk');
SELECT * FROM "inbound"."FreeFieldNotes" WHERE NOT log_creator = 'falk';
SELECT * FROM "inbound"."CellMaps";


SELECT visit_id, log_update, sampleunit_id, notes FROM "inbound"."Visits"
  WHERE visit_done
  AND notes IS NOT NULL;

SELECT sampleunit_id, recovery_hints FROM "outbound"."SampleUnits"
  WHERE recovery_hints IS NOT NULL;

-- replacements
SELECT
  UNIT.sampleunit_id,
  UNIT.grts_address,
  REP.grts_address_replacement
FROM "outbound"."Replacements" AS REP
LEFT JOIN "outbound"."SampleUnits" AS UNIT
  ON UNIT.sampleunit_id = REP.sampleunit_id
WHERE REP.is_selected
  AND NOT REP.is_inappropriate
  AND UNIT.is_replaced
;
#+end_src


#+begin_src sql :eval no
-- mnmgwdb
SELECT * FROM "outbound"."FieldworkPlanning" WHERE done_planning;

SELECT * FROM "inbound"."FieldworkPlanning" WHERE samplelocation_id IS NULL;


SELECT
  VST.log_user,
  VST.log_update,
  FWCAL.fieldworkcalendar_id,
  SLOC.samplelocation_id,
  -- LOC.location_id,
  -- sspstapa_id,
  SLOC.grts_address
FROM "inbound"."Visits" AS VST
LEFT JOIN "outbound"."FieldworkCalendar" AS FWCAL
  ON FWCAL.fieldworkcalendar_id = VST.fieldworkcalendar_id
LEFT JOIN "outbound"."SampleLocations" AS SLOC
  ON SLOC.samplelocation_id = VST.samplelocation_id
;


SELECT DISTINCT eval_source, count(*) FROM "outbound"."LocationEvaluations" GROUP BY eval_source;
#+end_src
