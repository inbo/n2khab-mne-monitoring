#+title: Steps Journal
#+author: Falk Mielke

This file documents the steps I undertook to set up the MNE database structure.

* General
** server setup
SCHEDULED: <2025-05-31 Sat>

+ set up the SQL server on a web hosting service
+ running on arch linux; super user and normal user
+ access via =ssh=:
    + =ssh -p <port> <normaluser>@<host-ip>=
+ postgres database initialized and postgis installed

full details: [[https://github.com/falkmielke/agenda/blob/main/notes/20250602135718-inbopostgis.org]]

#+begin_quote
*Currently, each new IP must be registered in* =/var/lib/postgres/data/pg_hba.conf= *to be able to connect to the database.*
#+end_quote
/cf./ [[https://wiki.archlinux.org/title/PostgreSQL#Optional_configuration]]

** sql user management
SCHEDULED: <2025-05-31 Sat>

... is done directly on the server.

#+begin_src sh :eval no
ssh -p <port> <normaluser>@<host-ip>
su - root
su - postgres
createuser -p <port> --interactive

psql -p <port>
ALTER USER <user> WITH ENCRYPTED PASSWORD '<password>';
#+end_src

+ new username needs to be added to =pg_hba.conf=!
+ consider updating =TeamMembers= table.


** database setup
SCHEDULED: <2025-06-02 Mon>

connect (ssh via keys):
#+begin_src sh :eval no
ssh -p <port> <normaluser>@<host-ip>
su - root
su - postgres
#+end_src


drop-create:
#+begin_src sh :eval no
# dropdb <database> -p <port>
createdb <database> -O <owner> -p <port>
#+end_src


postgis extension:

#+begin_src sh :eval no
psql -U <owner> -h <host-ip> -p <port> -d <database> -W
#+end_src

#+begin_src sql :eval no
\c <database>
CREATE EXTENSION postgis;
CREATE EXTENSION postgis_topology;
CREATE EXTENSION fuzzystrmatch;
CREATE EXTENSION postgis_tiger_geocoder;

#+end_src


** database structure
SCHEDULED: <2025-06-02 Mon>-<2025-06-05 Thu>

A python script is available here:
+ =n2khab-mne-monitoring/900_database_organization/GenerateDatabase.py=

This ideally uses a virtual environment, set up as follows:
#+begin_src sh :eval no
cd <project_folder>
python -m venv .dbinit
source .dbinit/bin/activate
pip install --upgrade pip
pip install --upgrade -r python_requirements.txt
# pip freeze > python_requirements.txt # to feed back updated requirements
#+end_src


Then to run it, with a database connection config file (as described in the script) in place:
#+begin_src sh :eval no
source .dbinit/bin/activate
# python GenerateDatabase.py
python 110_init_loceval.py
#+end_src


The script works on "=csv="s which have a fixed structure to define shema's, tables, columns.

An example google file can be found [[https://docs.google.com/spreadsheets/d/1BTtG-A2ASjbF7IhYYb8FDcQLBZtSQbLbElUUYeofHNM/edit?usp=drive_link][here]],
+the =csv='s are generated by "File >> Download >> Comma Separated Values (.csv)" for each sheet in the document.+
the =csv='s are generated by "File >> Download >> OpenDocument (.ods)" which can be converted to =.csv= with the =ODStoCSVs= python function.
Those comma separated text files are stored in the =db_structure= subfolder.


Because tables and roles are organized in schema's,
it might be necessary to
#+begin_src sql :eval no
SET search_path TO public,<schema>;
SHOW search_path;
-- \dt+
#+end_src



* Biotieker, outbound

** setup database
SCHEDULED: <2025-06-05 Thu>

| database name | loceval          |
| schema        | loceval_outbound |
| peer user     | ward             |

#+begin_src sh :eval no
psql -h <host> -p <port> -U <user> -d <database> -W
#+end_src

** location/activity calendar
SCHEDULED: <2025-06-05 Thu>

*goal:*
+ get a table with point geometry of sample target locations to the database
+ options to filter by date, potentially FAs, ...

Database creation in [[file:020_create_databases.py]]
Gradual refinement in [[file:100_sample_location_tests.qmd][100_sample_location_tests.qmd]]

TODO:
+ [X] =n2khab_data/10_raw/grtsmaster_habitats= issue: =n2khab::read_GRTSmh()= requires capitals -> was my fault
+ [X] =loceval_outbound=: =Calendar= links to =ActivityGroups=, but not to =ActivitySequences=?
  -> the sequence is in the dates.


** ODS download and conversion
SCHEDULED: <2025-06-06 Fri>

It is now possible to *download the google sheet as ODS* and convert it to csv automatically.

** constraints
SCHEDULED: <2025-06-06 Fri>

The option to specify constraints was added.
More info here: [[https://www.postgresql.org/docs/current/ddl-constraints.html]]

Most useful are
+ =UNIQUE= (e.g. for columns which are pk, but blocked by geometry fid)
+ value ranges, e.g. =CHECK (year > 0)=.


** multiple schemas
SCHEDULED: <2025-06-06 Fri>

In- and outbound schemas can be defined within the same file.
foreign keys across schema's are possible.


** pgmodeler import
SCHEDULED: <2025-06-06 Fri>

[[https://pgmodeler.io]]

A great tool to visualize relational databases.

It crashed earlier on syncing the "Plantentuin" pilot project,
but it worked fine now for importing the database and visualizing relations.

Will be re-used ad hoc later.

** sequence issues
SCHEDULED: <2025-06-06 Fri>

Due to a misconfiguration, qgis attempts to create a new location id upon attribute save.

Added an "=EXPOST=" sheet to run after succesful creation.
#+begin_src sql :eval no
GRANT USAGE ON SEQUENCE "loceval_outbound"."seq_locationcalendar_id" TO ward;
#+end_src

/update:/ Because this was such a common issue, the GRANT USAGE is now integrated for all sequences.

(sequences can be listed with =\ls+=)

** binaries, images, BLOB
SCHEDULED: <2025-06-06 Fri>

Managed to upload a binary with qgis.
Will require download/restoration tests and good disk space management.

** Visits
SCHEDULED: <2025-06-06 Fri>
a derived table showing a subset of the calendar

** stratum et al. to Location Calendar
SCHEDULED: <2025-06-10 Tue>

+ used an upstream object of =fag_grts_calendar_...= to have scheme/stratum/module/panel for calendar
+ TODO: some locations were not unique

#+begin_src r :eval no
  fag_stratum_grts_calendar_2025_attribs_sf %>%
    select(
      scheme,
      module_combo_code,
      panel_set,
      stratum,
      targetpanel,
      field_activity_group,
      grts_address_final,
      date_start
    )
#+end_src


** Schema Re-Organisation
SCHEDULED: <2025-06-10 Tue>

We now use simple names:
+ metadata
+ outbound
+ inbound

separation is based on user rights and purpose


** TeamMembers
SCHEDULED: <2025-06-10 Tue>

to refer to users who upload stuff


** N2kHabTypes
SCHEDULED: <2025-06-10 Tue>

reference to habitat types and strata
+ adding =stratum_to_expect= to LocCal
+ adding =stratum_assigned= to =Visits=


** Activities - overhaul
SCHEDULED: <2025-06-11 Wed>

Some changes and discussions with Floris about the "Activities" logic.

+ removed =ActivityGroups= and =ActivitySequences=
  + sequences are already included in the calendar data via =rank=
+ ... for the sake of =GroupedActivities=
  + contains activities in different groups
+ link to calendar via =grouped_activity_id=
  + and sequence determined by =activity_sequence=
  + =LocationCalendar= and =Visits= will have multiple lines now (all scheduled activities per visit)
  + will have to see how this plays out in qgis.

*** REMINDER: Activities/Groups DO NOT SPLIT
SCHEDULED: <2025-06-13 Fri>

It seems tempting to split out the table =ActivityGroups=.
However,
=GroupedActivities= contains duplicates of some activities,
because there are also =ActivitySequences= involved.

For the future:
a. stick with the simplified =GroupedActivites=
   + advantage: single table simplicity
   + disadvantage: queries for activities or groups get complicated (DISTINCT/GROUP BY/UNIQUE)
b. split out =Activities= + =ActivityGroups= + =ActivitySequences=
   + less workable for practical purposes
   + duplication of sequence info (also comes with the calendar)
   + but: easier reference to =ActivityGroups=, which are work units for fieldwork

*** chat protocol
You, 9:50â€¯AM
#+begin_quote
FYI er zijn activities die niet in een activity sequence terecht komen:

    activities %>%
      anti_join(
      activity_sequences,
      join_by(activity)
    )

->
    activity         activity_name        is_datacollection_meâ€¦Â¹ is_field_activity
    <fct>            <fct>                <lgl>                  <lgl>
    1 GWSURFINSTALLMAT installatiemateriaaâ€¦ FALSE                  FALSE
    2 GWMAT            staalnamemateriaal â€¦ FALSE                  FALSE
    3 SOILMAT          staalnamemateriaal â€¦ FALSE                  FALSE
    4 SURFMAT          staalnamemateriaal â€¦ FALSE                  FALSE
    5 SECDATACOLL      data uit bestaande â€¦ TRUE                   FALSE
    # â„¹ abbreviated name: Â¹ is_datacollection_method
    # â„¹ 3 more variables: is_prep_activity <lgl>, is_lab_activity <lgl>,
    #   protocol <fct>

... maar het zijn maar vijf; geen "field" activities; ik veronderstel dat ze niet in de calender terecht komen.
Ik ga ze toch meenemen naar de databank, voor de app oplossen door een "fake sequence" met naam van de activity toe te kennen, maar later kijken of ze er in qgis verschijnen of niet.
#+end_quote

Floris Vanderhaeghe, 9:51â€¯AM
#+begin_quote
Klopt. Je hebt die zaken niet nodig, enkel de locatie-attributen en de veldwerkkalender. Ik had ze niet toegevoegd als element van de data, maar ivm tonen van inhoudelijke samenhang tussen field activities. Als je gewoon activity_sequences gaat joinen aan activities, verwacht ik dat je er te veel gaat krijgen. Cf semi-joins hieronder, die ze filteren, en conditioneel stellen aan module en scheme. Puur achtergrond!

    faseqs <- [...]
    faseqs_fag_fa <- [...]
#+end_quote

You, 9:54â€¯AM
#+begin_quote
Ja, precies bij die semi-joins was ik begonnen :)
Het "te veel krijgen" is voor activities niet het gevaar, maar een activity of groep missen die later niet in de metadata-tabel staat kan de databank-logica crashen ivm. "NOT NULL"-constraints.
En ook voorbereidend desktop-werk zou in de app als een "Notitie"-laag meegevoerd kunnen worden, met een link naar een "activity".
Daarom wil ik ze er liever allemaal mee hebben.
#+end_quote

Floris Vanderhaeghe, 10:03â€¯AM, Edited
#+begin_quote
Snap ik wel vanuit databankopzicht, maar volgens mij hebben de activity sequences geen toegevoegde meerwaarde voor de uitvoerder op terrein, tov de kalender + rank. Achterwege laten is dus volgens mij even goed. Niet alles uit de POC hoeft in een databank lijkt me. Niet dat het niet mag natuurlijk.

Wat wel relevant is, is om 'overdue' taken te markeren en te prioriteren, als die op dezelfde locatie en voor hetzelfde stratum al hadden moeten gebeuren tov de daar geplande taken in een huidig datuminterval taak te kunnen uitvoeren. Met andere woorden, omgaan met veldwerkvertraging op een locatie (grts_address) x stratum, op basis van datuminterval en rank. We laten niets 'achter', vertraagde zaken zijn prioritairder en blijven op de planning staan.

Let er ook op dat field acts in eenzelfde FAG uitgevoerd moeten worden tijdens eenzelfde bezoek. Dat is de reden waarom FAG de eenheid is van de veldwerkkalender.

Wellicht vertel ik hier niets nieuws ðŸ™‚
#+end_quote

You, 10:13â€¯AM
#+begin_quote
> hebben de activity sequences geen toegevoegde meerwaarde
klopt, maar activity_sequences is de enige variabele waarin ik group en activity samen vindt. Mijn bericht boven was maar een "check", want ik begin uiteraard bij de activities tabel.

> Wat wel relevant is, is om 'overdue' taken te markeren en te prioriteren
Inderdaad, dit komt er in.

> field acts in eenzelfde FAG uitgevoerd moeten worden tijdens eenzelfde bezoek
Ja, zo ga ik het ook implementeren. Ward krijgt een takenlijst, met takengroepen en onderstappen.
#+end_quote

Floris Vanderhaeghe, 10:17â€¯AM
#+begin_quote
Goed, merci!
#+end_quote



** qgis refresher
SCHEDULED: <2025-06-11 Wed>

qgis startup recently began to throw some python errors

#+begin_src python :eval no
from osgeo import gdal, ogr, osr
#+end_src


** postgres VIEWs
SCHEDULED: <2025-06-11 Wed>

working on the database from qgis would cause confusion with all the unused, technical fields.
Views are a great way out.

#+begin_src sql :eval no
DROP VIEW IF EXISTS "outbound"."TestView";
CREATE VIEW "outbound"."TestView" AS
SELECT
    ogc_fid,
    wkb_geometry,
    locationcalendar_id,
    stratum,
    teammember_assigned,
    stratum_to_expect,
    date_start,
    date_end,
    visit_planned,
    notes
FROM "outbound"."LocationCalendar";
#+end_src

Views are now part of the database definition.
I tested that notes can be stored back to the database. HOORAY!

** first qgis tests
SCHEDULED: <2025-06-11 Wed>

+ seems nice
+ live update with the server
+ forms design
+ conditional formatting

missing qfield export.


** gdal error (qgis / qfieldsync plugin)
SCHEDULED: <2025-06-12 Thu>

solved by
#+begin_src sh :eval no
pip install --no-cache --force-reinstall gdal[numpy]=="$(gdal-config --version).*" --break-system-packages
#+end_src

Then re-install =sf= and =terra= in R:
#+begin_src r :eval no
install.packages(c("sf", "terra", "lwgeom"))
#+end_src

** qfield cloud issues
SCHEDULED: <2025-06-12 Thu>

-> go with distributing zip files for now.
[[testing/documenting qgis export][see below]] for procedure.

*** (a) copy file directly
+ pg login prompted upon opening
+ but: no error messages (â˜‡ =pg_hba.conf= must allow user)

*** (b) via qfieldcloud
+ /seems/ to work fine, BUT:
+ postgis layers only supported with subscription (12EUR/month)

** filter activities for Ward
SCHEDULED: <2025-06-12 Thu>

There are many activities.
Only some are relevant for Ward.
I intuitively selected src_r[:eval no]{c("LOCEVALAQ", "LOCEVALAQ", "LOCEVALAQ", "LOCEVALTERR", "LSVIAQ", "LSVITERR")}
and @KW, @FV added src_r[:eval no]{c("SURFLENTSAMPLPOINT", "SURFLOTSAMPLPOINT")}

https://docs.google.com/spreadsheets/d/1VPloImiATO6jjxfMmd-LVKnBUjpVEc7sAtb4yL08sIk/edit?usp=drive_link

|------+-----------------------+--------------------------------------------------------------------------------------+-------------------|
| team | activity              | activity_name                                                                        | is_field_activity |
|------+-----------------------+--------------------------------------------------------------------------------------+-------------------|
| ward | LOCEVALAQ             | locatie in aquatisch habitat evalueren en eventueel vervangen                        | true              |
| ward | LOCEVALAQ             | locatie in aquatisch habitat evalueren en eventueel vervangen                        | true              |
| ward | LOCEVALAQ             | locatie in aquatisch habitat evalueren en eventueel vervangen                        | true              |
| ward | LOCEVALTERR           | locatie in terrestrisch habitat evalueren en eventueel vervangen                     | true              |
| ward | LSVIAQ                | LSVI bepalen in aquatisch habitat                                                    | true              |
| ward | LSVITERR              | LSVI bepalen in terrestrisch habitat                                                 | true              |
| ward | SURFLENTSAMPLPOINT    | vast staalnamepunt selecteren in stilstaande wateren indien er nog geen bestond      | true              |
| ward | SURFLOTSAMPLPOINT     | vast staalnamepunt selecteren in stromende wateren indien er nog geen bestond        | true              |
|------+-----------------------+--------------------------------------------------------------------------------------+-------------------|

** prune location calendar and smush duplicate locations
SCHEDULED: <2025-06-12 Thu>

were due to multiple strata and multiple target panels

+ scheme filtered (only "GW03.3" for now)
+ activities filtered: only =LOCEVAL*=
+ multi-strata pasted as =stratum1+stratum2+...=


** testing/documenting qgis export
SCHEDULED: <2025-06-12 Thu>

(at least temporarily) stored here:
https://drive.google.com/drive/folders/1VgjQ5YZ6AxYo5fz6iAkXQiOGd4Q7uZlV?usp=drive_link

*** on server:
remember to configure =pg_hba.conf=

*** on providing computer:
requires qgis with working "QFieldSync" plugin (beware of [[gdal error (qgis / qfieldsync plugin)][gdal errors]]).

1. prepare a qgis project with connected postgis layers
2. double-check qfield layer settings: =directly access data source=
3. on qfieldsync panel: =package for qfield= -> store project in a location
4. zip the export folder
5. distribute to target device

*** on target device:
requires qgis (desktop work) or qfield (fieldwork)

1. import data set
   a. qfield: "open local file", "(+)" on lower right, "import from zip"
   b. qgis: unzip, then open folder


** layer filtering by targetpanel
SCHEDULED: <2025-06-12 Thu>

syntax:
#+begin_src SQL
'targetpanel' LIKE '%03%'
#+end_src

there are helpful "show sample"/"show all" views on (optionally unfiltered) data


** postgres minor version upgrade
SCHEDULED: <2025-06-12 Thu>

> /moment of hesitation/
Well...
... as I have nothing better on the list.

#+begin_src sh :eval no
pacman -Syu
reboot
# crossing fingers!
#+end_src

Everything seems still intact.

** two databases: dev|production
SCHEDULED: <2025-06-12 Thu>

+ =loceval= is now controlled by a different superuser than =loceval_dev=.
+ adjusted config and scripts for being able to upload to both databases
+ ... also from R


** database schema's and mirrors
SCHEDULED: <2025-06-13 Fri>

+ [X] will require additional schema's (=metadata=, e.g. for =username=, =activity_groups=, or sequences) and =outbound= / =inbound= database schemes
+ [X] database mirrors (=loceval_dev= / =loceval=)

** minor fixes
SCHEDULED: <2025-06-13 Fri>

+ boolean defaults wrong due to dbWriteTable
+ indentation fault in CreateViews loop
+ worked on the wrong qgis project :/


** inbound tables and views: FreeFieldNotes, VisitNotes, PriorVisits
SCHEDULED: <2025-06-13 Fri>

+ new view: =VisitNotes=
+ renamed table: =FreeFieldNotes=
+ LocationCalendar / FieldPreparation are now aggregated by =activity_group=, which must be DISTINCTed from the =GroupedActivities=
+ =Visits= table is linked to =LocationCalendar=, therefore does not need another geometry
+ =Visits= have each activity split up! (as opposed to =activity_group= in =LocationCalendar=)
+ There is now code to recurrently update =Visits=, without deleting prior data.
+ only retaining =grts_address_final=
+ rudimentary preparation of sample replacements
+ Views: =PriorVisits= and =FreeFieldNotes= to field preparation project
+ qgis layer styles are now stored on the github repo

... and then this happens:
#+begin_quote
Layer veldbezoeksnotities:
PostGIS error while changing attributes: ERROR: cannot update view "VisitNotes"
DETAIL: Views that do not select from a single table or view are not automatically updatable.
HINT: To enable updating the view, provide
    an INSTEAD OF UPDATE trigger
    or an unconditional ON UPDATE DO INSTEAD rule.
#+end_quote
+ added functionality to use postgres RULES
+ added to "fieldwork preparation" = outbound qgis project
+ created qgis project for inbound work (draft): happy field-working!


** database change tracking
SCHEDULED: <2025-06-16 Mon>

there are new database fields:
+ =log_creator= and =log_creation= to track creation of =FieldNotes=
+ =log_user= and =log_update= to track last change of =LocationCalendar=, =Visits=, =FieldNotes=
implemented by trigger (see =expost=)


** photo attachments
SCHEDULED: <2025-06-16 Mon>

+ blob/bytea upload in qfield did not work
+ instead, separate folder (default: DCIM) and manual storage sync
+ tested and working, though not ideal


** meaningful attribute labels
SCHEDULED: <2025-06-16 Mon>

qfield seems to use the first field as identifier
+ is there a qgis setting to override?
+ otherwise re-arrange database columns

-> on Layer properties >> Labels, work with "expressions", e.g.

#+begin_src sql
"note_date" || '  (' || represent_value("teammember_id") || ')'
#+end_src

#+begin_src sql
 CASE WHEN "visit_date_planned" IS NULL THEN '' ELSE  "visit_date_planned" || ' '  END
|| "stratum"
|| ' ' || "activity_group"
|| ' (' || CASE WHEN "teammember_id" IS NULL THEN 'unassigned' ELSE represent_value("teammember_id") END || ')'
#+end_src


** backups (1): dumps
SCHEDULED: <2025-06-16 Mon>-<2025-06-18 Wed>

*** database daily diffs

#+begin_src sh :eval no

# first: dump the diff to a patch file
pg_dump -U <user> -h <host> -p <port> -d loceval_dev -N tiger -N public -W \
    | diff latest_dump.sql - \
    > $(date +"%Y%m%d")_loceval_diff.patch

# then: patch the changes into `latest`
patch latest_dump.sql -i $(date +"%Y%m%d")_loceval_diff.patch

#+end_src

Add the above to a cronjob:
https://wiki.archlinux.org/title/Cron
+ install cronie/patch/vi with =pacman -Sy cronie patch vi=
+ start&enable system process: =systemctl start cronie.service && systemctl enable cronie.service=
+ create a =.pgpass= file ([[=.pgpass= for backups][see here]]) and enter credentials; chmod it to 600.
+ =crontab -e=
  + according to https://wiki.archlinux.org/title/Cron#Crontab_format
  + to add a new line: =22 0 * * * sh ~/backups/backup_loceval.sh 2>&1 |tee -a ~/backups/backup_loceval.log=
  + (for daily 2:22 o'clock backup; note that the server is 2h behind)

here is the backup shell script:
(for the fish shell)
#+begin_src sh :eval no
# go to the backup folder
backup_folder="~/backups/loceval/"

echo ""
echo "________________________________________________________________________________"
echo "running backup $(date +"%Y%m%d%H%M") -> ${backup_folder}"

# dump database content
pg_dump -U <user> -h <host> -p <port> -d loceval -N tiger -N public \
    | diff "${backup_folder}loceval_latest.sql" - \
    > "${backup_folder}loceval_diff_$(date +"%Y%m%d%H%M").patch"

# store incrementally
patch "${backup_folder}loceval_latest.sql" -i "${backup_folder}loceval_diff_$(date +"%Y%m%d%H%M").patch"
#+end_src

-> test was good.


*** monthly dumpall madness

There also is a script to dump all databases once monthly.
We will se how that turns out.

#+begin_src sh :eval no
# go to the backup folder
backup_folder="/home/mnm/backups/dumps"

echo ""
echo "________________________________________________________________________________"
echo "monthly full dumps $(date +"%Y%m%d") -> ${backup_folder}"

# dump loceval
pg_dump -U <user> -h <host> -p <port> -d loceval -N tiger -N public \
    > "${backup_folder}/loceval_dump_$(date +"%Y%m%d").sql"
#+end_src


*** bring 'em home

just scp it to a local folder.

#+begin_src sh :eval no
scp -r -P 2406 "<ssh_user>@<host>:~/backups/*" "/data/mnm_db_backups/"

cd /data/ && tar -cvzf mnm_db_backup.tar.gz mnm_db_backups/*
#+end_src

(optionally copy to aws s3)


*** test restore
SCHEDULED: <2025-06-19 Thu>

#+begin_src sh :eval no
[postgres@host ~]$ dropdb loceval_dev -p 2407
[postgres@host ~]$ createdb loceval_dev -O <admin> -p 2407
[postgres@host ~]$ psql -p 2407 -d loceval_dev
loceval_dev=# CREATE EXTENSION postgis;
loceval_dev=# CREATE EXTENSION postgis_topology;
loceval_dev=# CREATE EXTENSION fuzzystrmatch;
loceval_dev=# CREATE EXTENSION postgis_tiger_geocoder;
loceval_dev=# \q
[user@host backup_folder]$ psql -U <admin> -h <host> -p <port> -d loceval_dev < loceval_latest.sql
#+end_src

everything seems to be in order!


** =.pgpass= for backups
SCHEDULED: <2025-06-17 Tue>

We use a [`.pgpass` file](https://stackoverflow.com/a/2893979) for credential handling.
+ create the file
+ chmod it to 600.


** backups (2): table surgery
SCHEDULED: <2025-06-18 Wed>

The more complex problem are table-wise backups and restores.
Complex because of constraints (e.g. foreign keys).

** selectively update table data
SCHEDULED: <2025-06-18 Wed>

*** concept
i.e. change table content
+ without losing connections to other tables
+ without losing prior data and ist connection

*** DONE for Python
CLOSED: [2025-06-19 Thu 12:13] SCHEDULED: <2025-06-18 Wed>

*** DONE for R
CLOSED: [2025-06-23 Mon 16:33] SCHEDULED: <2025-06-20 Fri>

+ can change table with content from `.csv`
+ can change table with content from other database
+ characteristic columns can be chosen
+ can rename characteristic columns
+ function documentation

** data update on *production*
SCHEDULED: <2025-06-24 Tue>

... to get the latest columns
+ [X] some change suggestions @FV
  + stratum -> type
  + +targetpanel+
+ [X] apply changes


** database changes with data persistence
SCHEDULED: <2025-06-24 Tue>

i.e. we can replace tables and keep the data
as outlined above, but in one script


** adjustments Orthophoto workpackage
SCHEDULED: <2025-06-24 Tue>

=LocationCalendar= -> =PriorEvaluations=

DONE: exclude/hold columns -> Py
DONE: Views store in file and adjust


** store table relation structure
SCHEDULED: <2025-06-19 Thu>

I figured that we will need the [[selectively update table data]] in R as well,
but would have trouble getting the table relation structure.

Solution:
The structure is exported from python upon database creation and can now be read to R as well.
/(Once more, I feel clumsy in R, but it does the job.)/

** extra layers qgis orthophoto assessment
SCHEDULED: <2025-06-26 Thu>

*** orthophotos
https://geo.api.vlaanderen.be/okz/wcs
okz: orthofoto kleinschaal zomer
omz: orthofoto midschaal zomer
omw: orthofoto midschaal winter

*** beheer ANB
https://metadata.vlaanderen.be/srv/dut/catalog.search#/metadata/2d9fe2d9-c204-4f02-b228-45658e7ff7fb
(as wfs)

*** preliminary habitat map
(via Floris)

*** public habitat map
from here: https://metadata.vlaanderen.be/srv/dut/catalog.search#/metadata/45cde39a-5421-85ec-d52c-4b46-3891-e876-549ea870

non-fluent (many features x wfs)


** copying a database
SCHEDULED: <2025-06-27 Fri>

+ via script =210_copy_database.org=
+ sub-scripts are tangled from that file
  + =211_create_empty_testing.py=
  + =212_production_to_testing_example.R=
  + =213_populate_testing_db.R=
+ e.g. "production" to "testing"



** TODO issues with =Locations.ogc_fid= uniqueness
SCHEDULED: <2025-06-27 Fri>

+ problems with updating sample due to temporarily lost lookup
  + cascading of =location_id= is non-trivial (but would work)
  + yet then the =ogc_fid= is the official pk (for qgis compatibility)
  + however, =ogc_fid= produces duplicates when auto-filling

+ grts address is sufficient and necessary
+ historic field assessments (from prev samples) should also remain in the "locations"

For some reason, I could apped to the "Locations" table.
+ =location_id= is not the official pk
+ reason: qgis requires =ogc_fid= for =wkb= geometry tables
+ =ogc_fid= should be handled automatically (sequence!)
  yet =dbplyr= fails trying to upload dublicates

It might be that the "append" logic of dbplyr is wrong,
or that I shoud rather use some sort of =sf::write=.

*temporary solution:*
+ disabled the fk constraints on dependent tables
+ denial strategy: "load to memory, delete all, re-upload"


** Permanent Data Captation
CLOSED: [2025-06-30 Mon] SCHEDULED: <2025-06-16 Mon>

*The database =loceval= is kind of transient (read: active, dynamic, beautiful).*
Data prepared at home or generated in the field must be captured and moved to a completely different database: =mnmdb=.
+ =LocationCalendar= is augmented and changed dynamically -> append diffs to =mnmdb=
+ =Visits= have a field to indicate they are done, but should be moved/pruned regularly
+ =FieldNotes= must be stored irrespective of the =hide= tag; images linked and moved to storage

Solved for now:
+ python script has optional =tabula_rasa= boolean arg; if =False=, data will be (re)stored during database adjustment.
+ for R, =101_update_loceval_dev.R= was surgically adjusted
+ we can copy data between databases, e.g. via a "testing"/"staging" database


** orthophoto database project
SCHEDULED: <2025-06-30 Mon>

+ incorporated adjustments suggested by Ward
+ help on-the-fly


** n2khab type "gh"
SCHEDULED: <2025-06-30 Mon>

upon request by @WT,
an extra type is added upon upload of =N2kHabTypes=:
"gh" ("geen habitat")
to tag sample units in which the expected habitat is not present any more


** =location_id= implicit foreign key
SCHEDULED: <2025-07-07 Mon>

=location_id= is not instantiated as a foreign key to avoid conflicts due to =ON DELETE SET NULL=.

Therefore, it remains as a false lookup upon data update.

+ [X] adjusted =MNMDatabaseToolbox.py::dbTable.ListDependencies(...)=
+ [X] TODO the "old" ones also require a new location_id -> write a function to UPDATE the lookup key by grts_address

The location_id can now be restored for tables with a grts_address.


** major change =GroupedActivities=
SCHEDULED: <2025-07-09 Wed>

+ found duplicate error on work with field activity calendar
+ =*_id= in =GroupedActivities= are no sequences!
+ because they were sequences, they got overwritten upon INSERT
+ which caused the same =activity_group= on different lines to get two different id's

-> corrected.

** field activity calendar
SCHEDULED: <2025-07-09 Wed>

critical request by @WT.

+ [X] structure in place
+ [X] excel sheet
+ [X] qgis project
+ [ ] forward updates to =ExtraVisits=


** replacement locations
SCHEDULED: <2025-07-09 Wed>-<2025-07-10 Thu>

critical request by @WT.

+ [X] structure in place
+ [X] view to show only active replacements
+ [X] separate update rules for different fields -> double-rule
+ [X] qgis project
+ [ ] routine to keep previous replacements intact

+ [X] re-try dev creation and then live update


** The Great Rename
SCHEDULED: <2025-07-11 Fri>

with some requests by Karen, I found it opportune to rename a lot of things.
To be applied in
+ [X] gsheet
+ [X] upload script
+ [X] sample update procedure
+ [X] all sql query scripts (especially views)

The following replacements:
| %s/SampleLocations/SampleUnits/%g       |
| %s/samplelocation_id/sampleunit_id/%g   |
| %s/SLOC/UNIT/%g                         |
| %s/SamplePolygons/SampleCells/%g        |
| %s/ExtraVisits/Visits/%g                |
| %s/extravisit_id/visit_id/%g            |
| %s/ReplacementLocations/Replacements/%g |
| %s/lut_N2k/N2k/%g                       |

+ [X] testing on =dev=
+ [X] go live
+ [X] adjust qgis project


** SampleUnitPolygons
SCHEDULED: <2025-07-11 Fri>

KW asked for polygons for WT.
So I figured where to find the polygons.

+ Floris loads polygons for replacements
  from =file.path(locate_n2khab_data(), "10_raw/habitatmap/habitatmap.gpkg")=
+ using =sf::.[]= subsetting, I get the SampleUnits polygons
+ polygon is taken for each sampleunit
  + polygons are redundantly stored if they contain multiple sample units
+ upload to table.

Added to =EXPOST= statements:
#+begin_src sql
ALTER TABLE "outbound"."SampleUnitPolygons" DROP CONSTRAINT IF EXISTS fk_SampleUnits_SampleUnitPolygons CASCADE;
ALTER TABLE "outbound"."SampleUnitPolygons" ADD CONSTRAINT fk_SampleUnits_SampleUnitPolygons FOREIGN KEY (sampleunit_id)
REFERENCES "outbound"."SampleUnits" (sampleunit_id) MATCH SIMPLE
ON DELETE CASCADE ON UPDATE CASCADE;
#+end_src


** Replacement Archives
SCHEDULED: <2025-07-11 Fri>

Replacements are tabula-rasa'd in the sample update process.
We still want to keep the original information.

The reason we move these aside is that the replacement information has to be post-processed to adjust the SampleUnits anyways, which makes the replacement visits obsolete.


** Replacement Cells
SCHEDULED: <2025-07-11 Fri>

Use of =UNION= to display the "ongoing" replacement cells together with the regular cells.


** CellMaps
SCHEDULED: <2025-07-11 Fri>

A new table for freestyle cell mapping.
Happy drawing!


** major update
SCHEDULED: <2025-07-11 Fri>

+ changes above pushed to production
+ time to trash the qgis project... was not too bad
+ uploaded and mail sent


** mirror re-organization
SCHEDULED: <2025-07-14 Mon>

order and function to the database mirrors.

*** development
+ the dev database mirror is used for structural adjustments and development of
+ new features, mostly empty, and unstable.

*** staging
+ "staging" is a rather accurate mirror of the production database,
+ but used ad hoc in times of change to back-up the data or to test the effects
+ of structural adjustments.

*** testing
+ The testing mirror is an exact copy of the production database, and
+ regularly re-copied over. Changes to the data on "testing" are non-permanent.

*** production
+ This is the live environment with real data.
+ It is the least volatile, best backed-up of our database mirrors.


** populate testing db
SCHEDULED: <2025-07-14 Mon>

+ qgis connection changing plugin
+ extra permissions for test user

** extra info for Ward
SCHEDULED: <2025-07-14 Mon>

e.g. ANB; from "accessibility" preliminary analysis

** MILESTONE: first field data by Ward
SCHEDULED: <2025-07-14 Mon>

** minor change requests Ward
SCHEDULED: <2025-07-16 Wed>

+ ++ =recovery_hints=
+ rename accessibility_revisit
+ colors
+ accessibility to SampleUnits

#+begin_src R :eval no
rename_FieldActivityCalendar <- function(fac) {
fac <- fac %>% dplyr::rename(accessibility_revisit = acceccibility_revisit)
return(fac)
}
table_modification <- c(
"FieldActivityCalendar" = function (fac) rename_FieldActivityCalendar(fac) # (almost) anything you like
)

#+end_src


** TODO [#A] accessibility across locations

+ accessibility is +now stored on SampleUnits+ THIS was a bad idea.
+ but it is informative for all locations on a GRTS

-> and therefore must be stored separately.
[[file:./surgery/202507_locationinfos_loceval.sql]]

+ [ ] prepare + test
+ [ ] apply


** TODO rtkgps accuracy

+ extra field


** TODO replacement notes to sample unit

Ward is storing notes about the SampleUnits in Replacements.


* mnmgwdb

** the next database
SCHEDULED: <2025-06-30 Mon>

copied =loceval= structure
to re-activate the fieldwork calendar

+ create databases =mnmfield= and =mnmfield_dev=
+ create and authorize extra users

#+begin_src sh
createuser  -p 2407 -S <user>

psql -p <port>
ALTER USER <user> WITH ENCRYPTED PASSWORD '<password>';

vim /var/lib/postgres/data/pg_hba.conf
systemctl ...
#+end_src

+ append =~/.pgpass=
+ instantiate backups (extra script, cronjob, logs, tested)
+ database creation script


** file renames
SCHEDULED: <2025-06-30 Mon>

for better organization.


** debug init script
SCHEDULED: <2025-07-01 Tue>

working!


** fill with data
SCHEDULED: <2025-07-02 Wed>

+ [X] Locations
+ [X] LocationAssessments
+ [X] FieldActivityCalendar
+ [ ] Visits


** create tester user
SCHEDULED: <2025-07-02 Wed>

(user who can only access the =_dev= databases,
so that I do not have to hijack other people's accounts all the time)


** qgis outbound app
SCHEDULED: <2025-07-02 Wed>

TODO:
+ ++landowner
+ re-upload bug


** RESTART (5): mnmgwdb
SCHEDULED: <2025-07-14 Mon>

+ with experience and input from =loceval=
+ new name: =mnmgwdb=

+ remove local replacement logic
+ all work based on =stratum=, not on =type=
  + e.g. =N2kHabType= --> =++stratum= --> =N2kHabStrata=
+ =LocationEvaluations=
  + assembled notes on all prior notes and field visits
  + hung up on the SampleLocations, which is where they go
+ unfilter GroupedActivities
+ +CellMaps+ (on hold)

** TODO adjusted db structure
SCHEDULED: <2025-07-15 Wed>-<2025-07-17 Wed>

+ does not work on SampleUnits, but distinct SampleLocations
+ sufficient to plan fieldwork by activity_group for now,
  + later, there might be specific star-structure tables with extra data per activity group, linked to/branching from =Visits=
+ =SSPSTaPas=, a simple lookup table.
+ [ ]


** replacements: ad hoc solution

+ via query from loceval and lookup
=sample_locations <- sample_locations %>% relocate_grts_replacements()=


* learning the hard way

** =dbWriteTable(..., overwrite = TRUE, ...)= is a bad idea!
SCHEDULED: <2025-07-07 Mon>

Overwriting a table is a "DROP/CREATE" procedure and therefore leads to a loss in access roles.

+ Just don't do it!
+ Instead, manually "DELETE+INSERT",
  wherein the insert can be =dbWriteTable(..., overwrite = FALSE, append = TRUE, ...)=.

** =dbReadTable(...)= is better avoided, too
SCHEDULED: <2025-07-07 Mon>

... as it leads to data table inconsistencies with =dplyr.=
Specifically, there is trouble with combining =DBI::dbReadTable= and =dplyr::tbl= (example: get =grts_address= for combined =Locations=.
+ =distinct= does not mean =unique= :/


** loss of links due to "=append_tabledata="
SCHEDULED: <2025-07-07 Mon>

The function =append_tabledata= checks which data rows are present in the table,
and only uploads the novel ones.
Problem is that foreign keys are neglected:
+ If the old data was linked to another table,
+ and that table was also updated prior to the re-upload,
+ then links will be lost.

Actually, I had =update_datatable_and_dependent_keys()= in place for this.
Time to use it.

+ [X] defined =update_cascade_lookup= to bring the procedures together and handle reference columns.

** TODO update rule avoidance
SCHEDULED: <2025-07-08 Tue>

Update rules are nice,
but for some technical tasks, it is better to circumvent them.

+ [ ] implement (de-)activating update rules


** TODO Polygon Memory Size
SCHEDULED: <2025-07-11 Fri>

The habitat map spatraster is quite big;
and laptop memory is limited.

I got trouble once because too little memory was left to get sample unit polygons.
-> is there a long term solution?


* General

** TODO fk columns which allow NULL
SCHEDULED: <2025-07-17 Thu>

e.g. FieldworkCalendar.sspstapa_id
required because they are set NULL on cascaded update during re-upload
should rather deactivate/activate the constraint


** DONE Replacement Unit logic
CLOSED: [2025-07-10 Thu 12:38] SCHEDULED: <2025-06-16 Mon>

/brainstorm:/
+ LocationCalendar -> must be linked
+ geometry -> must be updated
+ Visits -> must be adjusted, e.g. by linking in =inbound.Replacements=
+ join =Replacements= to =Visits= if they have an entry


** python updates

update all requirements:
#+begin_src sh :eval no
cp python_requirements.txt python_requirements.bak
cat python_requirements.txt | cut -f1 -d= | xargs pip install -U
pip freeze > python_requirements.txt
#+end_src


GDAL requires special attention:
python bindings version always needs to match system version (which is not generally the latest one available on PyPI).
Its line can be deleted from the =requirements.txt= prior to the update.

#+begin_src sh :eval no
pip install --no-cache --force-reinstall gdal[numpy]=="$(gdal-config --version).*" --break-system-packages
#+end_src


** quick checks

#+begin_src sql :eval no
SET search_path TO public,"metadata","outbound","inbound";
SELECT DISTINCT assessment_done, COUNT(*) AS n FROM "outbound"."LocationAssessments" GROUP BY assessment_done;
SELECT * FROM "outbound"."FieldActivityCalendar" WHERE done_planning;
SELECT * FROM "outbound"."Replacements" WHERE is_inappropriate OR is_selected OR (notes IS NOT NULL);
SELECT * FROM "inbound"."Visits" WHERE NOT (log_user = 'update') AND NOT (log_user = 'falk');
SELECT * FROM "inbound"."FreeFieldNotes" WHERE NOT log_creator = 'falk';
SELECT * FROM "inbound"."CellMaps";


SELECT visit_id, sampleunit_id, notes FROM "inbound"."Visits"
  WHERE visit_done
  AND notes IS NOT NULL;

SELECT sampleunit_id, recovery_hints FROM "outbound"."SampleUnits"
  WHERE recovery_hints IS NOT NULL;

-- replacements
SELECT
  UNIT.sampleunit_id,
  UNIT.grts_address,
  REP.grts_address_replacement
FROM "outbound"."Replacements" AS REP
LEFT JOIN "outbound"."SampleUnits" AS UNIT
  ON UNIT.sampleunit_id = REP.sampleunit_id
WHERE REP.is_selected
  AND NOT REP.is_inappropriate
  AND UNIT.is_replaced
;


-- mnmgwdb
SELECT * FROM "outbound"."FieldworkPlanning" WHERE done_planning;

#+end_src
