---
title: "Data Re-Upload"
date: "2025-06-05"
format:
  html:
    toc: true
    html-math-method: katex
    code-fold: false
    embed-resources: true
knitr:
  opts_chunk:
    echo: true
---



:::{.callout-note title="Purpose"}
Let's bring the sample to the field!
:::

<https://github.com/inbo/n2khab-mne-monitoring/pull/5>

<https://docs.google.com/spreadsheets/d/12dWpyS2Wsjog3-z3q6-pUzlAnY4MuBbh6igDWH9bEZw/edit?usp=drive_link>

# preparation

## libraries and variables

```{r libraries}
library("dplyr")
library("tidyr")
library("stringr")
library("purrr")
library("lubridate")
library("sf")
library("terra")
library("n2khab")
library("googledrive")
library("readr")
library("rprojroot")
library("keyring")

library("configr")
library("DBI")
library("RPostgres")

library("mapview")
# mapviewOptions(platform = "mapdeck")

projroot <- find_root(is_rstudio_project)
working_dbname <- "loceval_dev" 

# you might want to run the following prior to sourcing or rendering this script:
# keyring::key_set("DBPassword", "db_user_password")

source("MNMDatabaseToolbox.R")

# Load some custom GRTS functions
# source(file.path(projroot, "R/grts.R"))
# TODO: rebase once PR#5 gets merged
source("/data/git/n2khab-mne-monitoring_support/020_fieldwork_organization/R/grts.R")

```


## info stream from POC

```{r load-sample-rdata}


# Setup for googledrive authentication. Set the appropriate env vars in
# .Renviron and make sure you ran drive_auth() interactively with these settings
# for the first run (or to renew an expired Oauth token).
# See ?gargle::gargle_options for more information.
if (Sys.getenv("GARGLE_OAUTH_EMAIL") != "") {
  options(gargle_oauth_email = Sys.getenv("GARGLE_OAUTH_EMAIL"))
}
if (Sys.getenv("GARGLE_OAUTH_CACHE") != "") {
  options(gargle_oauth_cache = Sys.getenv("GARGLE_OAUTH_CACHE"))
}

# Download and load R objects from the POC into global environment
reload <- FALSE
poc_rdata_path <- file.path("./data", "objects_panflpan5.RData")
if (reload || !file.exists(poc_rdata_path)){
  drive_download(
    as_id("1a42qESF5L8tfnEseHXbTn9hYR1phqS-S"),
    path = poc_rdata_path,
    overwrite = reload
  )
}
load(poc_rdata_path)
```


## Test Case: FAG Calendar

In [this useful file](https://github.com/inbo/n2khab-mne-monitoring/blob/fieldworkdata_supportbyfloris/020_fieldwork_organization/code_snippets.R), I found the following interesting object stack:

- `fag_stratum_grts_calendar_2025_attribs_sf`
  - `fag_stratum_grts_calendar_2025_attribs`
    - `fag_stratum_grts_calendar` (raw)
    - `scheme_moco_ps_stratum_targetpanel_spsamples`
      - `scheme_moco_ps_spsubset_targetfag_stratum_sppost_spsamples_calendar` (raw)
      - `n2khab_strata` (raw)
      - `n2khab_types_expanded_properties` (raw)
  - `grts_mh` (n2khab)
  - `grts_mh_index`
    - (`grts_mh`)
    
(would be cool to have a dependence graph.)


```{r load-fag-grts-calender-2025-attribs-sf}

grts_mh <- read_GRTSmh()
# create a spatial index of the GRTS addresses
grts_mh_index <- tibble(
  id = seq_len(ncell(grts_mh)),
  grts_address = values(grts_mh)[, 1]
) %>%
  filter(!is.na(grts_address))


# attributes of spatial sampling units (~grts_address_final), useful in maps,
# selections and decisions. Note that we *identify* sampling units as stratum x
# grts_address; a unit_id is not needed provided that units don't share the same
# GRTS address (if some still do, it means that the GRTS raster is too coarse
# for those types, and will eventually need extra levels inside those specific
# cells)
scheme_moco_ps_stratum_targetpanel_spsamples <-
  scheme_moco_ps_spsubset_targetfag_stratum_sppost_spsamples_calendar %>%
  inner_join(
    n2khab_strata,
    join_by(stratum),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  inner_join(
    n2khab_types_expanded_properties %>%
      select(type, grts_join_method, sample_support_code),
    join_by(type),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  mutate(
    is_forest = str_detect(type, "^9|^2180|^rbbppm")
  ) %>%
  distinct(
    scheme,
    module_combo_code,
    panel_set,
    stratum,
    # 'aquatic' column will be improved for 7220 later on (now it simply has a
    # duplication (TRUE + FALSE) of all locations)
    is_aquatic = in_aquatic_subset,
    is_forest,
    grts_join_method,
    sample_support_code,
    grts_address,
    grts_address_final,
    targetpanel,
    last_type_assessment = assessment_date,
    last_type_assessment_in_field = assessed_in_field,
    last_inaccessible = inaccessible
  ) %>%
  arrange(pick(scheme:grts_address))


# merging scheme:module_combo_code:panel_set:targetpanel, still distinguishing
# strata separately (even though they may share their location: this is unreal
# in the case of multiple cell-centered strata). For now, not distinguishing
# module_combo as explained above.
stratum_schemepstargetpanel_spsamples <-
  scheme_moco_ps_stratum_targetpanel_spsamples %>%
  select(-module_combo_code) %>%
  mutate(scheme_ps_targetpanel = str_glue(
    "{ scheme }:PS{ panel_set }{ targetpanel }"
  )) %>%
  select(-scheme, -panel_set, -targetpanel) %>%
  nest(scheme_ps_targetpanels = scheme_ps_targetpanel) %>%
  mutate(
    scheme_ps_targetpanels = map_chr(scheme_ps_targetpanels, \(df) {
      str_flatten(df$scheme_ps_targetpanel, collapse = " | ")
    }) %>%
      factor()
  ) %>%
  relocate(scheme_ps_targetpanels) %>%
  arrange(pick(stratum:grts_address))

# Note: if grts_address_final differs from grts_address, then this means a local
# replacement took place already in the past. If now it appears that the stratum
# is no longer present in the field, then a new replacement procedure must take
# place using grts_address as the anchor, provided that the type still occurs in
# the polygon. If not, the absence must be noted and sampling frame + sample are
# to be updated.
scheme_moco_ps_stratum_targetpanel_spsamples %>%
  filter(grts_address != grts_address_final) %>%
  glimpse


# cell centers of the terrestrial sampling units (excluding 7220):
units_cell_cellcenter <-
  stratum_schemepstargetpanel_spsamples %>%
  filter(str_detect(sample_support_code, "cell")) %>%
  add_point_coords_grts(
    grts_var = "grts_address_final",
    spatrast = grts_mh,
    spatrast_index = grts_mh_index
  )

# sampling units as raster cells:
units_cell_rast <-
  stratum_schemepstargetpanel_spsamples %>%
  filter(str_detect(sample_support_code, "cell")) %>%
  pull(grts_address_final) %>%
  filter_grtsraster_by_address(spatrast = grts_mh, spatrast_index = grts_mh_index)
set.names(units_cell_rast, "grts_address_final")

# the number of non-NA cells matches the number of unique GRTS addresses
stratum_schemepstargetpanel_spsamples %>%
  filter(str_detect(sample_support_code, "cell")) %>%
  distinct(grts_address_final) %>%
  nrow() %>%
  all.equal(global(units_cell_rast, "notNA")[1, 1])

# representing this limited number of cells as polygons: useful for plotting etc
units_cell_polygon <-
  units_cell_rast %>%
  as.polygons(aggregate = FALSE) %>%
  st_as_sf() %>%
  # to prefer the tibble approach in sf, we need to convert forth and back
  as_tibble() %>%
  # it appears that the CRS is actually retrieved from the tibble, but I don't
  # understand how (so the crs argument below isn't needed)
  st_as_sf(crs = "EPSG:31370")


# mapview(units_cell_polygon)


# This section is primarily intended as support for fieldwork planning by the
# compartment scheme responsible, who will use these R objects directly.

# Derive the FAG calendar for 2025 at the stratum x location x FAG occasion, and
# include some of the location attributes.
fag_stratum_grts_calendar_2025_attribs <-
  fag_stratum_grts_calendar %>%
  select(
    scheme_moco_ps,
    stratum,
    grts_address,
    starts_with("date"),
    field_activity_group,
    rank
  ) %>%
  filter(year(date_start) < 2026) %>%
  # count(date_start, date_end, date_interval) %>%
  # move the fieldwork that was kept for 2024, to 2025, since that is indeed
  # its meaning
  mutate(
    across(c(date_start, date_end), \(x) {
      if_else(year(date_start) == 2024, x + years(1), x)
    }),
    date_interval = interval(
      force_tz(date_start, "Europe/Brussels"),
      force_tz(date_end, "Europe/Brussels")
    )
  ) %>%
  unnest(scheme_moco_ps) %>%
  # adding location attributes
  inner_join(
    scheme_moco_ps_stratum_targetpanel_spsamples %>%
      select(
        scheme,
        module_combo_code,
        panel_set,
        stratum,
        grts_join_method,
        grts_address,
        grts_address_final,
        targetpanel
      ) %>%
      # deduplicating 7220:
      distinct(),
    join_by(scheme, module_combo_code, panel_set, stratum, grts_address),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  relocate(grts_address_final, .after = grts_address) %>%
  select(-module_combo_code) %>%
  # flatten scheme x panel set x targetpanel to unique strings per stratum x
  # location x FAG occasion. Note that the scheme_ps_targetpanels attribute is a
  # shrinked version of the one at the level of the whole sample (see sampling
  # unit attributes in the beginning), since we limited the activities to those
  # planned before 2026, and then generate stratum_scheme_ps_targetpanels as a
  # location attribute. So it says specifically which schemes x panel sets x
  # targetpanels are served by the specific fieldwork at a specific date
  # interval.
  mutate(scheme_ps_targetpanel = str_glue(
    "{ scheme }:PS{ panel_set }{ targetpanel }"
  )) %>%
  select(-scheme, -panel_set, -targetpanel) %>%
  nest(scheme_ps_targetpanels = scheme_ps_targetpanel) %>%
  mutate(
    scheme_ps_targetpanels = map_chr(scheme_ps_targetpanels, \(df) {
      str_flatten(
        unique(df$scheme_ps_targetpanel),
        collapse = " | "
      )
    }) %>%
      factor()
  ) %>%
  relocate(scheme_ps_targetpanels)

```

Contains a `POINT` geometry and columns:

| stratum                     | <fct>       |
| scheme                      | <fct>       |
| targetpanel                 | <fct>       |
| grts_address                | <int>       |
| grts_address_final          | <int>       |
| date_start                  | <date>      |
| date_end                    | <date>      |
| date_interval               | <varchar>   |
| field_activity_group        | <fct>       |
| rank                        | <int>       |
| geometry                    | <POINT [m]> |


:::{.callout-warning title="Mind the note by Floris:"}
Beware that more locations will emerge due to local replacement, so this is
misleading for counting & planning (but useful in spatial visualization)
:::


## Field Activities, Activity Groups (and Sequences)

```{r view-activity-groups-and-sequences}
#| eval: false

# ... requires more `code_snippets.R` which are not run here

# activity groups, sequences, ...:
actseqs_actgroups_acts <-
  activity_sequences %>%
  semi_join(faseqs, join_by(activity_rank)) %>%
  inner_join(
    activities,
    join_by(activity),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  arrange(
    activity_group,
    activity_rank,
    rank,
    activity
  ) %>% 
  group_by(activity_rank) %>%
  mutate(activity_rank_id = cur_group_id()) %>%
  ungroup %>%
  group_by(activity_group) %>%
  mutate(activity_group_id = cur_group_id()) %>%
  ungroup %>%
  group_by(
    activity_group,
    activity_rank,
    activity
  ) %>%
  mutate(activity_id = cur_group_id()) %>%
  ungroup %>%
  relocate(
    activity_id,
    activity_group_id,
    activity_rank_id,
    activity,
    rank,
    .before = 1
  )

View(
  faseqs_fag_fa %>%
  distinct(field_activity, field_activity_group, activity_rank) %>%
  arrange(field_activity_group) %>% 
  filter(
    TRUE |
    field_activity_group == "GWINSTPIEZWELL"
  )
)

```

:::{.callout-tip title="Feedback Floris:"}
Activity **sequence** is included in the calendar via the `rank` and that rank already comes delivered in the **calendar**. 
For all practical purposes, everything else of the `ActivitySequences` can be ignored downstream.

For database organization, I rename the `rank` to `activity_rank`
:::

```{r prepare-activities}
# # TODO there are some activities with different ranks within a sequence.
# activity_groupcount <- activity_sequences %>%
#   distinct(activity, activity_group, rank) %>%
#   count(activity, activity_group) %>%
#   arrange(desc(n))

activity_group_lookup <-
  activity_sequences %>%
    distinct(activity_group, activity)

grouped_activities <-
  activities %>%
  left_join(
    activity_group_lookup,
    join_by(activity),
    relationship = "one-to-many"
  )

# knitr::kable(grouped_activities %>% distinct(activity_group, activity))

# replace group non-sequenced activities with activity name
grouped_activities <- grouped_activities %>%
  mutate_at(
    vars(activity_group, activity, activity_name, protocol),
    as.character
  ) %>%
  mutate(
    activity_group = ifelse(is.na(activity_group), activity, activity_group)
  )
# %>% select(activity, activity_group) %>% tail(10) %>% knitr::kable()


grouped_activities <- grouped_activities %>%
  arrange(activity) %>%
  group_by(activity) %>%
  mutate(activity_id = cur_group_id()) %>%
  ungroup %>%
  arrange(activity_group) %>%
  group_by(activity_group) %>%
  mutate(activity_group_id = cur_group_id()) %>%
  ungroup %>%
  arrange(activity_group, activity) %>%
  group_by(
    activity_group,
    activity
  ) %>%
  mutate(grouped_activity_id = cur_group_id()) %>%
  ungroup %>%
  relocate(
    grouped_activity_id,
    activity_group_id,
    activity_group,
    activity_id,
    activity,
    .before = 1
  )

# knitr::kable(grouped_activities %>% distinct(activity_group, activity, activity_name))

# activities %>%
#   anti_join(
#     activity_sequences,
#     join_by(activity)
#   )
# # non-field activities; of no relevance for the calendar
# 
glimpse(grouped_activities)
```

## stratum

We need to get at least the stratum for the target field activity.

- always `join_by(sample_support_code, unit_id)`
- `stratum_scheme_targetpanels := stratum (grts_join_method) [scheme_targetpanels]` -> replaced (went to upstream table)

| `fag_stratum_grts_calendar_2025_attribs` | the object from which the calendar was derived, |
|                                          | with `stratum` still in there                   |
| `n2khab_types_expanded_properties`       | links type and typelevel to sample support code |
| `stratum_schemetargetpanel_spsamples`    | has scheme, grts, and ssc                       |

```{r join-stratum}
# scheme_moco_ps_spsubset_targetfag_stratum_sppost_spsamples_calendar
# scheme_moco_ps_stratum_targetpanel_spsamples
# stratum_schemetargetpanel_spsamples
# stratum_units_non_cell_n2khab


fag_stratum_grts_calendar_2025_attribs_sf <-
  fag_stratum_grts_calendar_2025_attribs %>%
  add_point_coords_grts(
    grts_var = "grts_address_final",
    spatrast = grts_mh,
    spatrast_index = grts_mh_index
  )

if (FALSE){
  fag_stratum_grts_calendar_2025_attribs_sf %>%
    head(32) %>%
    sf::st_geometry() %>%
    plot()
}

# mapview::mapview(
#   fag_stratum_grts_calendar_2025_attribs_sf %>%
#     head(100) %>% 
#     sf::st_geometry()
#   # , col.regions = "steelblue"
#   # , platform = "mapdeck"
#   # , zcol = "stratum"
#   )
```



## database connection

Working out a way to load the config.
Config has the following structure:

```
    [test]
    server = localhost
    port = 5439
    user = test
    database = playground
    password = <the password you entered IN PLAIN TEXT>
```

:::{.callout-warning}
That `.conf`/`.ini` file contains password in plain text!

- Do not print its content in this notebook.
- Make sure to `.gitignore` it!
:::


We can use `configr` (<https://cran.r-project.org/web/packages/configr/readme/README.html>)

```{r load-config}

config_filepath <- file.path("./inbopostgis_server.conf")

if (working_dbname == "loceval") {
  db_connection <- connect_database_configfile(
    config_filepath,
    database = "loceval",
    profile = "inbopostgis"
  )

} else {
  db_connection <- connect_database_configfile(
    config_filepath,
    database = working_dbname,
    profile = "inbopostgis-dev"
  )

}

```


# Upload Data

```{r preview-work-to-do}
#| eval: false
glimpse(activities)
glimpse(activity_sequences)
glimpse(fag_stratum_grts_calendar_2025_attribs_sf)
```

## MetaData


### TeamMembers

A list of team members who may collect data
related to database usernames.

```{r upload-teammembers}
members <- read_csv(here::here("db_structure", "data_TeamMembers.csv"))

member_lookup <- upload_and_lookup(
  db_connection,
  DBI::Id(schema = "metadata", table = "TeamMembers"),
  members,
  ref_cols = "username",
  index_col = "teammember_id"
)


```

### Protocols

```{r upload-protocols}

protocols <- activities %>%
  select(protocol) %>%
  distinct() %>%
  arrange(protocol) %>%
  filter(!is.na(protocol)) %>%
  mutate(
    protocol_id = 1:n(),
    protocol = as.character(protocol),
    description = NA
  )

protocol_lookup <- upload_and_lookup(
  db_connection,
  DBI::Id(schema = "metadata", table = "Protocols"),
  protocols,
  ref_cols = "protocol",
  index_col = "protocol_id"
)

```


### (Grouped) Activities


```{r upload-grouped-activities}
 
grouped_activities_upload <- grouped_activities %>%
  lookup_join(protocol_lookup, "protocol")

# append_tabledata(
#   db_connection,
#   DBI::Id(schema = "metadata", table = "GroupedActivities"),
#   grouped_activities_upload,
#   reference_columns = "grouped_activity_id"
# )

# done manually to get multiple columns as unique lookup

db_table <- DBI::Id(schema = "metadata", table = "GroupedActivities")
ga_content <- DBI::dbReadTable(db_connection, db_table)

existing <- ga_content %>%
  select(activity_group, activity, activity_group_id, activity_id)
to_upload <- grouped_activities_upload %>%
  anti_join(
    existing,
    join_by(activity_group, activity, activity_group_id, activity_id)
)

if (nrow(to_upload) > 0){
  rs <- DBI::dbWriteTable(
    db_connection,
    db_table,
    to_upload,
    overwrite = FALSE,
    append = TRUE
  )
  # DBI::dbClearResult(rs)
}

grouped_activity_lookup <-
  dplyr::tbl(db_connection, db_table) %>%
  select(activity_group, activity, grouped_activity_id, activity_group_id, activity_id) %>%
  collect

```


### Habitat Types
`n2khab_types_expanded_properties` -> `N2kHabTypes`

```{r upload-n2khabtype}

n2khab_types_upload <- bind_rows(
  as_tibble(list(
    type = c("gh"),
    typelevel = c("main_type"),
    main_type = c("gh")
  )),
  n2khab_types_expanded_properties
  )


n2khabtype_lookup <- upload_and_lookup(
  db_connection,
  DBI::Id(schema = "metadata", table = "N2kHabTypes"),
  n2khab_types_upload,
  ref_cols = "type",
  index_col = "n2khabtype_id"
)

# SELECT DISTINCT type FROM "metadata"."N2kHabTypes" ORDER BY type;
```


## working data
### TODO LocationAssessments

- [!] TODO we need a list with previous field assessments, uploaded before sample_locations.


### SampleLocations

Table which stores mission control information, filled *in praetorio*.


Just like the "pilot" `LocationCalendar`, this starts from the field activity calendar with GRTS info. 
Floris has kindly prepared and pruned the list of locations, under the topic of "orthophoto assessment".


```{r prepare-orthophoto-assessment-table}
# NOTE: renamed for some persistence

# Making a list of terrestrial locations to be assessed using orthophotos in
# 2025. The procedure evaluates somewhat larger areas in which the unit is
# situated, so we rather have a polygon evaluation which says: can this be the
# targeted stratum or not? Because of expected negative results and hence the
# need for replacements at polygon level (dropping the unit without a local
# field replacement), the locations that are scheduled for field evaluation in
# both 2025 and 2026 are provided for orthophoto evaluation.

orthophoto_type_grts <-
  fag_stratum_grts_calendar %>%
  filter(
    str_detect(field_activity_group, "LOCEVAL"),
    year(date_start) < 2027
  ) %>%
  distinct(
    scheme_moco_ps,
    stratum,
    grts_address,
    date_start
  ) %>%
  unnest(scheme_moco_ps) %>%
  # adding location attributes
  inner_join(
    scheme_moco_ps_stratum_targetpanel_spsamples %>%
      select(
        scheme,
        module_combo_code,
        panel_set,
        stratum,
        grts_join_method,
        grts_address,
        grts_address_final,
        targetpanel
      ) %>%
      # deduplicating 7220:
      distinct(),
    join_by(scheme, module_combo_code, panel_set, stratum, grts_address),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  filter(
    # only consider schemes scheduled in 2025:
    str_detect(scheme, "^(GW|HQ)"),
    # only keep cell-based types (aquatic & 7220 will be more reliable or simply
    # not possible to evaluate on orthophoto)
    str_detect(grts_join_method, "cell")
  ) %>%
  # also join the spatial poststratum, since we need this in setting
  # GRTS-address based priorities
  inner_join(
    scheme_moco_ps_stratum_sppost_spsamples %>%
      unnest(sp_poststr_samples) %>%
      select(-sample_status),
    join_by(scheme, module_combo_code, panel_set, stratum, grts_address),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  # add MHQ assessment metadata
  inner_join(
    stratum_grts_n2khab_phabcorrected_no_replacements %>%
      select(stratum, grts_address, assessed_in_field, assessment_date),
    join_by(stratum, grts_address),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  # converting stratum to type (in the usual way, although for the cell-based
  # units the values - but not the factor levels - are identical)
  inner_join(
    n2khab_strata,
    join_by(stratum),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  select(-stratum) %>%
  relocate(grts_address_final, .after = grts_address) %>%
  relocate(type, grts_join_method, .after = panel_set) %>%
  select(-module_combo_code) %>%
  distinct() %>%
  mutate(
    scheme_ps_targetpanel = str_glue(
      "{ scheme }:PS{ panel_set }{ targetpanel }"
    ),
    loceval_year = ifelse(year(date_start) < 2025, 2025, year(date_start)) %>%
      as.integer()
  ) %>%
  select(-targetpanel, -date_start) %>%
  relocate(panel_set, .after = grts_join_method) %>%
  # set priorities based on loceval_year; for 2026 differentiate according to
  # GRTS address (because lower GRTS addresses have more chance to end up as
  # replacement). The latter is done within spatial poststratum & panel set
  mutate(
    priority_orthophoto = case_when(
      # priority 10: in 2025 there may not be time left to do these LOCEVALs in
      # the field (and secondly, this is currently not yet ready XXXXXXXXXXX)
      str_detect(scheme, "^HQ") ~ 10L,
      loceval_year == 2025 ~ 1L,
      grts_address <= median(grts_address) ~ 2L,
      .default = 3L
    ),
    .by = c(type, loceval_year, scheme, panel_set, sp_poststratum)
  ) %>%
  # collapse scheme & panel_set since these can have different values for the
  # same location
  summarize(
    # Note that the scheme_ps_targetpanels attribute is a shrinked version of
    # the one at the level of the whole sample (see sampling unit attributes in
    # the beginning), since we limited the activities to LOCEVAL activities
    # planned before 2027, and then generate stratum_scheme_ps_targetpanels as a
    # location attribute.
    scheme_ps_targetpanels = str_flatten(
      sort(unique(scheme_ps_targetpanel)),
      collapse = " | "
    ) %>%
      factor(),
    loceval_year = min(loceval_year),
    priority_orthophoto = min(priority_orthophoto),
    .by = c(
      type,
      grts_join_method,
      grts_address,
      grts_address_final,
      starts_with("assess"),
      sp_poststratum
    )
  ) %>%
  arrange(
    loceval_year,
    priority_orthophoto,
    type,
    sp_poststratum,
    grts_address
  )

glimpse(orthophoto_type_grts)
```


- convert factors to string
- rename `grts_address_final` -> `grts_address`
- rename `assessment_in_field` -> `previous_assessment`
- rename `assessment_date` -> `previous_assessment_date`
- geometry -> `wkb_geometry`


```{r orhtophoto-to-db-sample-locations-conversion}
sample_locations <- orthophoto_type_grts %>% 
  select(-grts_address) %>%
  rename(
    grts_address = grts_address_final,
    previous_assessment = assessed_in_field,
    previous_assessment_date = assessment_date
  ) %>%
  mutate(
    across(c(
        type,
        grts_join_method,
        sp_poststratum,
        scheme_ps_targetpanels
      ),
      as.character
    )
  )

glimpse(sample_locations)
```


**Upload Spatial Locations:**

```{r extract-and-upload-locations-from-sample}

# clean up LocationAssessments
verb <- "DELETE "
sql_command <- glue::glue(
  '{verb} FROM "outbound"."LocationAssessments"
    WHERE ((log_user = \'yoda\') OR (log_user = \'falk\') OR (log_user = \'update\'))
      AND (NOT cell_disapproved)
      AND (revisit_disapproval IS NULL)
      AND (disapproval_explanation IS NULL)
      AND (type_suggested IS NULL)
      AND (implications_habitatmap IS NULL)
      AND (feedback_habitatmap IS NULL)
      AND (notes IS NULL)
      AND (NOT assessment_done)
    ;')
# print(sql_command)
rs <- DBI::dbExecute(
  db_connection,
  sql_command
  )
# print(rs)

# DBI::dbReadTable(
#   db_connection,
#   DBI::Id(schema = "outbound", table = "LocationAssessments"),
#   ) %>% collect() %>%
#     head() %>% knitr::kable()

# add location for previous LocationAssessment

previous_locations <- DBI::dbReadTable(
  db_connection,
  DBI::Id(schema = "outbound", table = "LocationAssessments"),
  ) %>% collect() %>%
  pull("grts_address") %>%
  as.integer()

# **Upload Spatial Locations:**
locations <- c(
    sample_locations %>% pull(grts_address) %>% as.integer(),
    previous_locations
  ) %>%
  tibble(grts_address = .) %>%
  distinct() %>%
  add_point_coords_grts(
    grts_var = "grts_address",
    spatrast = grts_mh,
    spatrast_index = grts_mh_index
  )

sf::st_geometry(locations) <- "wkb_geometry"

rs <- DBI::dbExecute(
  db_connection,
  'DELETE FROM "metadata"."Locations";'
  )

locations_lookup <- upload_and_lookup(
  db_connection,
  DBI::Id(schema = "metadata", table = "Locations"),
  locations,
  ref_cols = "grts_address",
  index_col = "location_id"
)


```


**Upload Location Polygons:**

```{r location-cells}
#| eval: true

units_cell_polygon[["grts_address_final"]] <-
  as.integer(units_cell_polygon[["grts_address_final"]])

# unit geometries (cells):
location_cells <-
  units_cell_polygon %>%
  inner_join(
    locations_lookup,
    by = join_by(grts_address_final == grts_address),
    relationship = "one-to-many",
    unmatched = "drop"
  ) %>%
  select(-grts_address_final) %>%
  relocate(geometry, .after = last_col())

sf::st_geometry(location_cells) <- "wkb_geometry"

append_tabledata(
  db_connection,
  DBI::Id(schema = "metadata", table = "LocationCells"),
  location_cells,
  reference_columns = "location_id"
)
```


**Upload Location Polygons:**

```{r upload-sample-location-polygons}


if ("location_id" %in% names(sample_locations)) {
  # should not be the case in a continuous script;
  # this is extra safety for debugging and de-serial execution
  sample_locations <- sample_locations %>% 
    select(-location_id)
}
sample_locations <- sample_locations %>%
  left_join(
    locations_lookup,
    by = join_by(grts_address),
    relationship = "many-to-one"
  )

append_tabledata(
  db_connection,
  DBI::Id(schema = "outbound", table = "SampleLocations"),
  sample_locations,
  reference_columns =
    c("type", "grts_address", "scheme_ps_targetpanels", "loceval_year")
)
```

**Append Location Assessments:**

```{r append-location-assessments}


new_location_assessments <- sample_locations %>%
  select(
    location_id,
    grts_address,
    type
  ) %>%
  mutate(
    log_user = "update",
    log_update = as.POSIXct(Sys.time()),
    cell_disapproved = FALSE,
    assessment_done = FALSE
  )

previous_location_assessments <- DBI::dbReadTable(
  db_connection,
  DBI::Id(schema = "outbound", table = "LocationAssessments"),
  ) %>% collect()
# nrow(previous_location_assessments)

new_location_assessments <- new_location_assessments %>%
  anti_join(
    previous_location_assessments,
    by = join_by(type, grts_address)
  )

location_assessments <- bind_rows(
  previous_location_assessments,
  new_location_assessments
  ) %>% 
  select(-locationassessment_id)
# nrow(location_assessments)


# # append the LocationAssessments with empty lines for new sample units
# append_tabledata(
#   db_connection,
#   DBI::Id(schema = "outbound", table = "LocationAssessments"),
#   location_assessments,
#   reference_columns =
#     c("type", "grts_address")
# )

rs <- DBI::dbExecute(
  db_connection,
  'DELETE FROM "outbound"."LocationAssessments";'
  )

# re-upload
sf::dbWriteTable(
  db_connection,
  DBI::Id(schema = "outbound", table = "LocationAssessments"),
  location_assessments,
  row.names = FALSE,
  overwrite = FALSE,
  append = TRUE,
  factorsAsCharacter = TRUE,
  binary = TRUE
  )

```

```{r check-polygons-on-map}
#| eval: false

opa_check <- sf::st_read(db_connection,
  DBI::Id(schema = "outbound", table = "OrthophotoAssessment"),
  ) %>%
  collect()
opa_check %>%
  mapview(zcol = "type")
```


# ARCHIVE

## `sf` table

### OBSOLETE LocationCalendar

:::{.callout-note}
`fag_stratum_grts_calendar` defines the needed visits of the spatial sampling
units and is organized at the FAG level. The rank is an indication of the needed
order of different FAGs at one location, in the same cycle. In some cases
repetitions do happen for certain FAGs in a scheme, not all FAGs, as prescribed
by the date interval.
:::


:::{.callout-warning}
Note:

- `rank` is renamed to `activity_rank`.
- `grts_address`: only "final" vaule retained

:::


```{r upload-calendar}
#| eval: false

location_calendar_raw <- fag_stratum_grts_calendar_2025_attribs_sf %>% 
  select(-grts_address) %>%
  rename(grts_address = grts_address_final) %>% 
  mutate(
    across(c(
        scheme,
        module_combo_code,
        stratum,
        date_interval
      ),
      as.character
    )
  ) %>% 
  left_join(
    grouped_activity_lookup %>% distinct(activity_group, activity_group_id),
    by = join_by(field_activity_group == activity_group),
    relationship = "many-to-many"
  ) %>%
  select(-field_activity_group, -grts_join_method) %>% # , -date_interval
  rename(activity_rank = rank, type = stratum) %>% 
  distinct()

sf::st_geometry(location_calendar_raw) <- "wkb_geometry"
glimpse(location_calendar_raw)

```



:::{.callout-note title="calendar filtering."}
Not everything has to be visible.

- only certain activities per user
- only "GW_03.3" for a start.
- sample can be assigned to multiple strata -> union column

:::

```{r filter-loccal}
#| eval: false
relevant_activities <- grouped_activity_lookup %>%
  filter(
    activity %in% c(
      "LOCEVALAQ",
      "LOCEVALAQ",
      "LOCEVALAQ",
      "LOCEVALTERR",
      "LSVIAQ",
      "LSVITERR",
      "SURFLENTSAMPLPOINT",
      "SURFLOTSAMPLPOINT"
    )
  ) %>%
  distinct(activity_group_id) %>%
  pull(activity_group_id)

# remove sequence for fieldwork preparation
location_calendar <- location_calendar_raw %>%
  select(-activity_rank) %>%
  filter(
    activity_group_id %in% relevant_activities,
    scheme %in% c("GW_03.3")
  ) %>% 
  distinct()

if (FALSE){
  location_calendar %>%
    count(grts_address_final) %>%
    filter(n>1)
}

location_calendar <- location_calendar %>%
  summarize(
    type = paste(type, collapse = "+"),
    .by = c(
      scheme,
      module_combo_code,
      panel_set,
      activity_group_id,
      grts_address,
      date_start, date_end,
      # activity_rank,
      wkb_geometry
    )
  ) %>%
  mutate(done_planning = FALSE)

# TODO prior data
#  %>% mutate(done_planning = tidyr::replace_na(done_planning, FALSE))

#   count(grts_address) %>%
#   arrange(desc(n))
# %>% filter(grts_address == 452914) # multiple schemes -> filtered
# %>% filter(grts_address == 31225) # multiple strata -> retained
glimpse(location_calendar)
```


Upload:

```{r re-upload-loccal}
#| eval: false

# delete all previously uploaded table content
rs <- DBI::dbExecute(
  db_connection,
  'DELETE FROM "inbound"."Visits";'
  )

rs <- DBI::dbExecute(
  db_connection,
  'DELETE FROM "outbound"."LocationCalendar";'
  )

# re-upload
sf::dbWriteTable(
  db_connection,
  DBI::Id(schema = "outbound", table = "LocationCalendar"),
  location_calendar,
  row.names = FALSE,
  overwrite = FALSE,
  append = TRUE,
  factorsAsCharacter = TRUE,
  binary = TRUE
  )
```


The rows in the calendar were not unique!
Here is how to find duplicates:

```{r unique-loccal}
#| eval: false
# SELECT * FROM "LocationCalendar";

# DONE: why are these non-unique? -> stratum, targetpanel

location_calendar_unique_count <- location_calendar %>%
  select(
    scheme,
    module_combo_code,
    panel_set,
    type,
    activity_group_id,
    grts_address,
    date_start
  ) %>%
  count(
    scheme,
    module_combo_code,
    panel_set,
    type,
    activity_group_id,
    grts_address,
    date_start
  ) %>%
  arrange(desc(n))
knitr::kable(head(
  location_calendar_unique_count
  ))
```


```{r join-lookup-loccal}
#| eval: false

loccal_lookup <- dplyr::tbl(
    db_connection,
    DBI::Id(schema = "outbound", table = "LocationCalendar")
  ) %>%
  select(
    scheme,
    module_combo_code,
    panel_set,
    type,
    activity_group_id,
    grts_address,
    date_start,
    locationcalendar_id
  ) %>% 
  collect


location_calendar <- location_calendar %>%
  left_join(
    loccal_lookup,
    by = join_by(
      scheme,
      module_combo_code,
      panel_set,
      type,
      activity_group_id,
      grts_address,
      date_start
    ),
    relationship = "one-to-one",
    unmatched = "error"
  ) 

```


# After Data Collection

All the above code is useful to reset a database with fresh data.
This is the purpose of OUTBOUND: we, the "back office", can freely manipulate and update the data,
whereas field workers cannot.


Now comes the part of handling tables which might already include data.

:::{.callout-warning}
The tasks below are recurrent tasks which have to be executed

- on regular intervals
- whenever desktop field planning is done (on-demand)
:::

## Visits

The locations to visit will be stored in a separate table: `inbound.Visits`.



```{r upcoming-visits}
#| eval: false
# we must first save the visits to our internal database

loccal_lookup <- dplyr::tbl(
    db_connection,
    DBI::Id(schema = "outbound", table = "LocationCalendar")
  ) %>%
  select(
    scheme,
    module_combo_code,
    panel_set,
    type,
    activity_group_id,
    grts_address,
    date_start,
    locationcalendar_id,
    # teammember_assigned,
    # visit_date_planned,
    # type_adjusted,
    # done_planning
  ) %>% 
  collect


visit_preparation <- location_calendar_raw %>%
  sf::st_drop_geometry() %>% 
  inner_join(
    loccal_lookup,
    by = join_by(
      scheme,
      module_combo_code,
      panel_set,
      type,
      activity_group_id,
      grts_address,
      date_start
    ),
    relationship = "many-to-one",
    unmatched = "drop"
  )


# here, I will split activity groups to activities
visit_preparation <- visit_preparation %>%
  left_join(
    grouped_activity_lookup %>%
      select(activity_group_id, grouped_activity_id),
    by = join_by(activity_group_id),
    relationship = "many-to-many"
  ) %>%
  select(-activity_group_id)

visits <- visit_preparation %>%
  select(
    locationcalendar_id,
    activity_rank,
    grouped_activity_id
  )

glimpse(visits)

# TODO join prior data
visits <- visits %>%
  mutate(
    visit_due = FALSE,
    visit_overdue = FALSE,
    visit_done = FALSE
  )

sf::dbWriteTable(
  db_connection,
  DBI::Id(schema = "inbound", table = "Visits"),
  visits,
  row.names = FALSE,
  overwrite = FALSE,
  append = TRUE,
  factorsAsCharacter = TRUE,
  binary = TRUE
  )


```


# Onwards, to QGIS!

- rename `GenerateDatabase` to `MNMDatabaseToolbox`
- via a mandatory, explicit data dump prior to re-upload in `MNMDatabaseToolbox`:
  - TODO retain existing `LocationCalendar` data
  - TODO retain existing `Visits`
  - TODO retain existing `FieldNotes`

- [ ] outbound project: FieldPreparation
  - [ ] update _dev
  - [ ] push to production
  - testing phase ongoing
- [ ] qgis design of inbound project
  - views
  - project
  - extra layers
  - microlocation (drawing template etc.)
