#+title: Copy Database
#+author: Falk Mielke


* Purpose

Databases are astonishingly dynamic and volatile.
We would like to be able to move ours, in design and content, from one place to the other.

Here I document the required steps.


Terminology is quite straight-forward:
+ *source* is the database from which we migrate the data
+ *target*, on the otherhand, will mirror the source after this procedure


This document is an [[https://orgmode.org][orgmode]] document because, like the whole branch, it uses =sh=, =R=, and =Python=.
Orgmode allows me to "tangle" codeblocks to subsidiary files in the push of a button.


* Target Database

** create
First of all, unless you have one already, it makes sense to create a new database.

The following steps are to be taken on the postgis server of choice.
Assuming you =ssh= into the server and switch to the =postgres= user.


#+begin_src sh :eval no :tangle no
# switch to database maintainence
su - postgres

# drop-create
# dropdb <database> -p <port>
createdb <database> -O <owner> -p <port>
#+end_src


** extend
Next, we require the *postgis extension*:

#+begin_src sh :eval no :tangle no
# switch to database maintainence
su - postgres

# either log in as maintainer:
psql -p <port> -d <target database>

# or use the database owner, via the whole connection specs:
# psql -U <owner> -h <host-ip> -p <port> -d <target database> -W
#+end_src

#+begin_src sql :eval no :tangle no
CREATE EXTENSION postgis;
CREATE EXTENSION postgis_topology;
CREATE EXTENSION fuzzystrmatch;
CREATE EXTENSION postgis_tiger_geocoder;

#+end_src


** automate
Some procedures, such as database backups, might need to be instantiated for the novel /target/.


For that purpose, occasional convenience is provided by [[https://www.postgresql.org/docs/current/libpq-pgpass.html][a =.pgpass= file]].

It can be appended by editing the =~/.pgpass=, appending a line for /target/:

#+begin_src sql :eval no :tangle no
<target-host>:<port>:<target-database>:<read-only-user>:<password>
#+end_src


If the /target/ is of permanent relevance, consider setting up a backup cronjob.
For now, that procedure remains documented in =000_steps_journal.org= >>> "=database daily diffs"=.


* Use Case 0: /tabula rasa/

** re-create empty

Trivially, you can just create the new database as you created the original one.

#+begin_src python :eval no :tangle 031_create_empty_testing.py
# DO NOT MODIFY
# this file is "tangled" automatically from `030_copy_database.org`.

import MNMDatabaseToolbox as DTB

# database:
base_folder = DTB.PL.Path(".")
structure_folder = base_folder/"loceval_db_structure"
DTB.ODStoCSVs(base_folder/"loceval_db_structure.ods", structure_folder)

db_target = DTB.ConnectDatabase(
    "inbopostgis_server.conf",
    connection_config = "loceval-testing",
    database = "loceval_testing"
    )
db = DTB.Database( \
    structure_folder = structure_folder, \
    definition_csv = "TABLES.csv", \
    lazy_creation = False, \
    db_connection = db_target, \
    tabula_rasa = False
    )
#+end_src


** limitation

Obviously, this misses the point:
no database content is copied here, only the skeleton of the database is mirrored.


I can imagine certain situations in which you would like to restart empty.
And this might be a preliminary step for [[Use Case 2: /tabula semi-replenta/][the second use case, below]].


* Use Case 1: /tabula plena/

** dump-reload

Now, it turns out that you can achieve the result we attempt below by a simple *dump-reload*.

#+begin_src sh :eval no :tangle no
pg_dump -U <user1> -h <host1> -p <port1> -d <source> -W \
    > $(date +"%Y%m%d")_migration_dump.sql

psql -U <user2> -h <host2> -p <port2> -d <source> -W \
    < $(date +"%Y%m%d")_migration_dump.sql
#+end_src


** limitation

The dump-reload strategy might be a bit too drastic in certain situations, e.g.:
+ if there is already a structure and data on /target/
+ if you would like to slightly alter the /target/ structure
+ if user/role permissions differ on the two databases


* Use Case 2: /tabula semi-replenta/

** purpose

This is the most surgical of the procedures.
Situation is that you have a /target/ structure, possibly with valuable data,
but you would like to copy or append from a /source/.



Imagine you would like to copy over the =Protocols= table.

1. Get the original data.
2. (optional) Modify / adjust / update the data.
3. Then, move it over.


** source

The source of your data can be any table which has approximately the same fields as the target database table.
You could use a =.csv= file, or another database.

Getting the data is as simple as establishing a connection and querying the table content.


#+begin_src R :eval no :tangle 032_production_to_testing_example.R
# DO NOT MODIFY
# this file is "tangled" automatically from `030_copy_database.org`.

source("MNMLibraryCollection.R")
load_database_interaction_libraries()

source("MNMDatabaseConnection.R")
source("MNMDatabaseToolbox.R")
# keyring::key_set("DBPassword", "db_user_password")

migrating_table_label <- "Protocols"

config_filepath <- file.path("./inbopostgis_server.conf")

source_db <- connect_mnm_database(
  config_filepath,
  database_mirror = "loceval-dev"
)

source_data <- source_db$query_table(migrating_table_label)

dplyr::glimpse(source_data)

#+end_src


** modify

This is open to the concrete task.
Nevertheless, I find it useful to functionalize a bit, for reasons to be clarified below.

In the example case, we will just sort the protocols.


#+begin_src R :eval no :tangle 032_production_to_testing_example.R
#_______________________________________________________________________________
### ENTER YOUR CODE here to modify the data!

sort_protocols <- function(prt) {
  prt <- prt %>% dplyr::arrange(dplyr::desc(protocol))
  return(prt)
}
source_data <- sort_protocols(source_data)

source_data <- source_data %>%
  select(-protocol_id)
#_______________________________________________________________________________
#+end_src


** target


The third step was derived in detail in the =200_organized_backups.qmd= notebook,
and the =MNMDatabaseToolbox.R= contains a convenient function for it.

#+begin_src R :eval no :tangle 032_production_to_testing_example.R

upload_data_and_update_dependencies(
  source_db,
  table_label = migrating_table_label,
  data_replacement = source_data,
  verbose = FALSE
)
#+end_src


* Generalization

Above is a simple example of table content moving from one to the other place.
Simple enough to be script-used heavily.

Make sure a read-only user is in =~/.pgpass=!


This requires minimal preparations, modification catalogue, and a loop.

#+begin_src R :eval no :tangle 033_populate_testing_db.R
# DO NOT MODIFY
# this file is "tangled" automatically from `030_copy_database.org`.

source("MNMLibraryCollection.R")
load_database_interaction_libraries()

source("MNMDatabaseConnection.R")
source("MNMDatabaseToolbox.R")
# keyring::key_set("DBPassword", "db_user_password")

# credentials are stored for easy access
config_filepath <- file.path("./inbopostgis_server.conf")

# database_label <- "mnmgwdb"
database_label <- "loceval"
source_mirror <- glue::glue("{database_label}")
target_mirror <- glue::glue("{database_label}-dev")


# from source...
source_db <- connect_mnm_database(
  config_filepath,
  database_mirror = source_mirror,
  user = "monkey",
  password = NA
)


# ... to target
target_db <- connect_mnm_database(
  config_filepath,
  database_mirror = target_mirror
)

#+end_src


We certainly need to modify some tables.

#+begin_src R :eval no :tangle 033_populate_testing_db.R

# TODO limitation: we should leave the primary and foreign keys unchanged!

#_______________________________________________________________________________
### define functions here to modify the data!
# modification is "on the go":
#   each of these functions should receive exactly one data frame,
#   just to give exactly one back.
sort_protocols <- function(prt) {
  prt <- prt %>% dplyr::arrange(dplyr::desc(protocol))
  return(prt)
}

rename_FieldActivityCalendar <- function(fac) {
  fac <- fac %>% dplyr::rename(accessibility_revisit = acceccibility_revisit)
  return(fac)
}

#_______________________________________________________________________________
### associate the functions with table names

table_modification <- c(
  "Protocols" = function (prt) sort_protocols(prt) # (almost) anything you like
  # "FieldActivityCalendar" = function (fac) rename_FieldActivityCalendar(fac) # (almost) anything you like
)

#_______________________________________________________________________________

#+end_src


#+begin_quote
Nothing ever changes if noone uploads any data.
#+end_quote
Here we go.

#+begin_src R :eval no :tangle 033_populate_testing_db.R

copy_over_single_table <- function(table_label, new_data, ...) {
  # parametrization of the `upload_data_and_update_dependencies` functions
  # just to make the loop code below look a little less convoluted.

  # push the update
  upload_data_and_update_dependencies(
    target_db,
    table_label = table_label,
    data_replacement = new_data,
    verbose = FALSE,
    ...
  )

}

#+end_src


Actually, what do we want to do to which tables?
Here we find out.

#+begin_src R :eval no :tangle 033_populate_testing_db.R

table_list_file <- file.path(glue::glue("{source_db$folder}/TABLES.csv"))
table_list <- read.csv(table_list_file)

process_db_table_copy <- function(table_idx) {

  table_schema <- table_list[[table_idx, "schema"]]
  table_label <- table_list[[table_idx, "table"]]
  table_exclusion <- !is.na(table_list[[table_idx, "excluded"]]) && table_list[[table_idx, "excluded"]] == 1

  # print(table_list[[table_idx, "excluded"]])

  if (table_exclusion) return()

  print(glue::glue("processing {table_schema}.{table_label}"))

  # download
  source_data <- source_db$query_table(table_label)

  # modify
  if (table_label %in% names(table_modification)){
    source_data <- table_modification[[table_label]](source_data)
  }

  copy_over_single_table(table_label, source_data)

}


#+end_src




Apply to all the table (never tired of reminding: *order matters*):

#+begin_src R :eval no :tangle 033_populate_testing_db.R

# TODO due to ON DELETE SET NULL from "Locations", location_id's temporarily become NULL.
#      Updating would be cumbersome.
constraints_mod <- function(do = c("DROP", "SET")){

  toggle_null_constraint <- function(schema, table_label, column){
    # {dis/en}able fk for these tables
    target_db$execute_sql(
      glue::glue('ALTER TABLE "{schema}"."{table_label}" ALTER COLUMN {column} {do} NOT NULL;'),
      verbose = FALSE
    ) # /sql
  } # /toggle_mod


  if (database_label == "loceval") {
    # To prevent failure, I temporarily remove the constraint.
    for (table_label in c("LocationAssessments", "SampleUnits", "LocationInfos")){
      toggle_null_constraint("outbound", table_label, "location_id")
    } # /loop

    toggle_null_constraint("inbound", "Visits", "location_id")
    toggle_null_constraint("outbound", "ReplacementCells", "replacement_id")
  }

  if (database_label == "mnmgwdb") {
    # To prevent failure, I temporarily remove the constraint.
    for (table_label in c("SampleLocations", "LocationInfos")){
      toggle_null_constraint("outbound", table_label, "location_id")
    } # /loop

    toggle_null_constraint("inbound", "Visits", "location_id")
  }

} #/constraints_mod

#_______________________________________________________________________________
# Finally, COPY ALL DATA

constraints_mod("DROP")

invisible(lapply(seq_len(nrow(table_list)), FUN = process_db_table_copy))

constraints_mod("SET")

#+end_src


This is tested by comparing =pg_dump= of both databases after copying.


You might want to modify permissions for certain users on the target database.

#+begin_src sql :eval no :tangle no

SET search_path TO public,"metadata","outbound","inbound","archive","analysis";

GRANT USAGE ON SCHEMA "metadata" TO tester;
GRANT SELECT ON ALL TABLES IN SCHEMA "metadata" TO tester;

#+end_src


* Brief Data Inspection

Some code snippets to check on relevant data (specific to =loceval=):

#+begin_src sql :tangle no :eval no
SET search_path TO public,"metadata","outbound","inbound";
SELECT DISTINCT assessment_done, COUNT(*) AS n FROM "outbound"."LocationAssessments" GROUP BY assessment_done;
SELECT * FROM "inbound"."FreeFieldNotes";
SELECT * FROM "inbound"."Visits" WHERE NOT (log_user = 'update');
SELECT * FROM "inbound"."CellMaps";
SELECT * FROM "archive"."ReplacementArchives";

#+end_src


* Summary

We now have a surgical method to transfer data from one to the other database.
Note the flexibility of the procedure above:
+ =table_modifications= can be applied on the way to match database structures in development
+ =update_datatable_and_dependent_keys= has some unused keywords which enable better data matching and column renaming.


One immediate purpose of these functions is to process updates of the POC.
