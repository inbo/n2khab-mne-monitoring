#+title: Copy Database
#+author: Falk Mielke


* Purpose

Databases are astonishingly dynamic and volatile.
We would like to be able to move ours, in design and content, from one place to the other.

Here I document the required steps.


Terminology is quite straight-forward:
+ *source* is the database from which we migrate the data
+ *target*, on the otherhand, will mirror the source after this procedure


This document is an [[https://orgmode.org][orgmode]] document because, like the whole branch, it uses =sh=, =R=, and =Python=.
Orgmode allows me to "tangle" codeblocks to subsidiary files in the push of a button.


* Target Database

** create
First of all, unless you have one already, it makes sense to create a new database.

The following steps are to be taken on the postgis server of choice.
Assuming you =ssh= into the server and switch to the =postgres= user.


#+begin_src sh :eval no :tangle no
# switch to database maintainence
su - postgres

# drop-create
# dropdb <database> -p <port>
createdb <database> -O <owner> -p <port>
#+end_src


** extend
Next, we require the *postgis extension*:

#+begin_src sh :eval no :tangle no
# switch to database maintainence
su - postgres

# either log in as maintainer:
psql -p <port> -d <target database>

# or use the database owner, via the whole connection specs:
# psql -U <owner> -h <host-ip> -p <port> -d <target database> -W
#+end_src

#+begin_src sql :eval no :tangle no
CREATE EXTENSION postgis;
CREATE EXTENSION postgis_topology;
CREATE EXTENSION fuzzystrmatch;
CREATE EXTENSION postgis_tiger_geocoder;

#+end_src


** automate
Some procedures, such as database backups, might need to be instantiated for the novel /target/.


For that purpose, occasional convenience is provided by [[https://www.postgresql.org/docs/current/libpq-pgpass.html][a =.pgpass= file]].

It can be appended by editing the =~/.pgpass=, appending a line for /target/:

#+begin_src sql :eval no :tangle no
<target-host>:<port>:<target-database>:<read-only-user>:<password>
#+end_src


If the /target/ is of permanent relevance, consider setting up a backup cronjob.
For now, that procedure remains documented in =000_steps_journal.org= >>> "=database daily diffs"=.


* Use Case 0: /tabula rasa/

** re-create empty

Trivially, you can just create the new database as you created the original one.

#+begin_src python :eval no :tangle 031_create_empty_testing.py
# DO NOT MODIFY
# this file is "tangled" automatically from `030_copy_database.org`.

import MNMDatabaseToolbox as DTB

# database:
base_folder = DTB.PL.Path(".")
structure_folder = base_folder/"loceval_db_structure"
DTB.ODStoCSVs(base_folder/"loceval_db_structure.ods", structure_folder)

db_target = DTB.ConnectDatabase(
    "inbopostgis_server.conf",
    connection_config = "loceval-testing",
    database = "loceval_testing"
    )
db = DTB.Database( \
    structure_folder = structure_folder, \
    definition_csv = "TABLES.csv", \
    lazy_creation = False, \
    db_connection = db_target, \
    tabula_rasa = False
    )
#+end_src


** limitation

Obviously, this misses the point:
no database content is copied here, only the skeleton of the database is mirrored.


I can imagine certain situations in which you would like to restart empty.
And this might be a preliminary step for [[Use Case 2: /tabula semi-replenta/][the second use case, below]].


* Use Case 1: /tabula plena/

** dump-reload

Now, it turns out that you can achieve the result we attempt below by a simple *dump-reload*.

#+begin_src sh :eval no :tangle no
pg_dump -U <user1> -h <host1> -p <port1> -d <source> -W \
    > $(date +"%Y%m%d")_migration_dump.sql

psql -U <user2> -h <host2> -p <port2> -d <source> -W \
    < $(date +"%Y%m%d")_migration_dump.sql
#+end_src


** limitation

The dump-reload strategy might be a bit too drastic in certain situations, e.g.:
+ if there is already a structure and data on /target/
+ if you would like to slightly alter the /target/ structure
+ if user/role permissions differ on the two databases


* Use Case 2: /tabula semi-replenta/

** purpose

This is the most surgical of the procedures.
Situation is that you have a /target/ structure, possibly with valuable data,
but you would like to copy or append from a /source/.



Imagine you would like to copy over the =Protocols= table.

1. Get the original data.
2. (optional) Modify / adjust / update the data.
3. Then, move it over.


** source

The source of your data can be any table which has approximately the same fields as the target database table.
You could use a =.csv= file, or another database.

Getting the data is as simple as establishing a connection and querying the table content.


#+begin_src R :eval no :tangle 032_production_to_testing_example.R
# DO NOT MODIFY
# this file is "tangled" automatically from `030_copy_database.org`.

library("dplyr")
source("MNMDatabaseToolbox.R")
# keyring::key_set("DBPassword", "db_user_password")

migrating_table_key <- "Protocols"
migrating_table <- DBI::Id(schema = "metadata", table = migrating_table_key)

source_db_connection <- connect_database_configfile(
  config_filepath = file.path("./inbopostgis_server.conf"),
  profile = "loceval-dev",
  database = "loceval_dev"
)

protocols_data <- dplyr::tbl(
    source_db_connection,
    migrating_table
  ) %>%
  collect() # collecting is necessary to modify offline and to re-upload

dplyr::glimpse(protocols_data)

#+end_src


** modify

This is open to the concrete task.
Nevertheless, I find it useful to functionalize a bit, for reasons to be clarified below.

In the example case, we will just sort the protocols.


#+begin_src R :eval no :tangle 032_production_to_testing_example.R
#_______________________________________________________________________________
### ENTER YOUR CODE here to modify the data!

sort_protocols <- function(prt) {
  prt <- prt %>% dplyr::arrange(dplyr::desc(protocol))
  return(prt)
}
protocols_data <- sort_protocols(protocols_data)

protocols_data <- protocols_data %>%
  select(-protocol_id)
#_______________________________________________________________________________
#+end_src


** target


The third step was derived in detail in the =200_organized_backups.qmd= notebook,
and the =MNMDatabaseToolbox.R= contains a convenient function for it.

#+begin_src R :eval no :tangle 032_production_to_testing_example.R

update_datatable_and_dependent_keys(
  config_filepath = file.path("./inbopostgis_server.conf"),
  working_dbname = "loceval_testing",
  table_key = migrating_table_key,
  new_data = protocols_data,
  profile = "loceval-testing",
  dbstructure_folder = "loceval_db_structure",
  verbose = FALSE
)
#+end_src


* Generalization

Above is a simple example of table content moving from one to the other place.
Simple enough to be script-used heavily.


This requires minimal preparations, modification catalogue, and a loop.

#+begin_src R :eval no :tangle 033_populate_testing_db.R
# DO NOT MODIFY
# this file is "tangled" automatically from `030_copy_database.org`.

library("dplyr")
source("MNMDatabaseToolbox.R")
# keyring::key_set("DBPassword", "db_user_password") # <- for source database

# credentials are stored for easy access
config_filepath <- file.path("./inbopostgis_server.conf")
dbstructure_folder <- "loceval_dev_structure"

# from source...
source_db_connection <- connect_database_configfile(
  config_filepath = config_filepath,
  profile = "loceval",
  user = "monkey",
  password = NA
)

# ... to target
target_db_name <- "loceval_testing"
target_connection_profile <- "loceval-testing"
target_db_connection <- connect_database_configfile(
  config_filepath = config_filepath,
  profile = target_connection_profile,
)

#+end_src


We certainly need to modify some tables.

#+begin_src R :eval no :tangle 033_populate_testing_db.R

# TODO limitation: we should leave the primary and foreign keys unchanged!

#_______________________________________________________________________________
### define functions here to modify the data!
# modification is "on the go":
#   each of these functions should receive exactly one data frame,
#   just to give exactly one back.
sort_protocols <- function(prt) {
  prt <- prt %>% dplyr::arrange(dplyr::desc(protocol))
  return(prt)
}

rename_FieldActivityCalendar <- function(fac) {
  fac <- fac %>% dplyr::rename(accessibility_revisit = acceccibility_revisit)
  return(fac)
}

#_______________________________________________________________________________
### associate the functions with table names

table_modification <- c(
  "Protocols" = function (prt) sort_protocols(prt) # (almost) anything you like
  # "FieldActivityCalendar" = function (fac) rename_FieldActivityCalendar(fac) # (almost) anything you like
)

#_______________________________________________________________________________

#+end_src


#+begin_quote
Nothing ever changes if noone uploads any data.
#+end_quote
Here we go.

#+begin_src R :eval no :tangle 033_populate_testing_db.R

copy_over_single_table <- function(table_key, new_data) {
  # just to make the loop code below look a little less convoluted.

  # push the update
  update_datatable_and_dependent_keys(
    config_filepath = config_filepath,
    working_dbname = target_db_name,
    table_key = table_key,
    new_data = new_data,
    profile = target_connection_profile,
    dbstructure_folder = dbstructure_folder,
    db_connection = target_db_connection,
    verbose = FALSE
  )

}

#+end_src


Actually, what do we want to do to which tables?
Here we find out.

#+begin_src R :eval no :tangle 033_populate_testing_db.R

table_list_file <- file.path(glue::glue("{dbstructure_folder}/TABLES.csv"))
table_list <- read.csv(table_list_file)

process_db_table_copy <- function(table_idx){

  table_schema <- table_list[[table_idx, "schema"]]
  table_key <- table_list[[table_idx, "table"]]
  table_exclusion <- !is.na(table_list[[table_idx, "excluded"]]) && table_list[[table_idx, "excluded"]] == 1

  print(table_list[[table_idx, "excluded"]])

  if (table_exclusion) return()

  print(glue::glue("processing {table_schema}.{table_key}"))

  # download
  source_data <- dplyr::tbl(
      source_db_connection,
      DBI::Id(schema = table_schema, table = table_key)
    ) %>%
    collect() # collecting is necessary to modify offline and to re-upload

  # modify
  if (table_key %in% names(table_modification)){
    source_data <- table_modification[[table_key]](source_data)
  }

  copy_over_single_table(table_key, source_data)

}


#+end_src




Apply to all the table (never tired of reminding: *order matters*):

#+begin_src R :eval no :tangle 033_populate_testing_db.R

# TODO due to ON DELETE SET NULL from "Locations", location_id's temporarily become NULL.
#      Updating would be cumbersome.
constraints_mod <- function(do = c("DROP", "SET")){

  toggle_null_constraint <- function(schema, table_key, column){
    # {dis/en}able fk for these tables
    execute_sql(
      target_db_connection,
      glue::glue('ALTER TABLE "{schema}"."{table_key}" ALTER COLUMN {column} {do} NOT NULL;'),
      verbose = FALSE
    ) # /sql
  } # /toggle_mod

  # To prevent failure, I temporarily remove the constraint.
  for (table_key in c("LocationAssessments", "SampleUnits")){
    toggle_null_constraint("outbound", table_key, "location_id")
  } # /loop

  toggle_null_constraint("inbound", "Visits", "location_id")
  toggle_null_constraint("outbound", "ReplacementCells", "replacement_id")

} #/constraints_mod

#_______________________________________________________________________________
# Finally, COPY ALL DATA

constraints_mod("DROP")

invisible(lapply(1:nrow(table_list), FUN = process_db_table_copy))

constraints_mod("SET")

#+end_src


This is tested by comparing =pg_dump= of both databases after copying.


You might want to modify permissions for certain users on the target database.

#+begin_src sql :eval no :tangle no

SET search_path TO public,"metadata","outbound","inbound","archive","analysis";

GRANT USAGE ON SCHEMA "metadata" TO tester;
GRANT SELECT ON ALL TABLES IN SCHEMA "metadata" TO tester;

#+end_src


* Brief Data Inspection

Some code snippets to check on relevant data (specific to =loceval=):

#+begin_src sql :tangle no :eval no
SET search_path TO public,"metadata","outbound","inbound";
SELECT DISTINCT assessment_done, COUNT(*) AS n FROM "outbound"."LocationAssessments" GROUP BY assessment_done;
SELECT * FROM "inbound"."FreeFieldNotes";
SELECT * FROM "inbound"."Visits" WHERE NOT (log_user = 'update');
SELECT * FROM "inbound"."CellMaps";
SELECT * FROM "archive"."ReplacementArchives";

#+end_src


* Summary

We now have a surgical method to transfer data from one to the other database.
Note the flexibility of the procedure above:
+ =table_modifications= can be applied on the way to match database structures in development
+ =update_datatable_and_dependent_keys= has some unused keywords which enable better data matching and column renaming.


One immediate purpose of these functions is to process updates of the POC.
