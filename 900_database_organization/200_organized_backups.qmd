---
title: "Organized Backups and Data Persistence"
date: "2025-06-16"
format:
  html:
    toc: true
    html-math-method: katex
    code-fold: false
    embed-resources: true
knitr:
  opts_chunk:
    echo: true
---


Data persistence will tackle two aspects.

1. long-term backups
  - using `pg_dump`
  - automated with `cronjob` and/or diffs
  - tested restoration
  - ... but hopefully never required
2. ad-hoc re-uploads
  - use case: changes in database structure
  - want to be able to drop/create without loss of data
  - crucial: identifier cascades
  - via R scripts


:::{.callout-note}
We might use a [`.pgpass` file](https://stackoverflow.com/a/2893979) for credential handling.
:::


# Long-Term Backups

## continuous

:::{.callout-tip title="Strategy"}
- Store the latest sql
- also store diffs from previous dumps (so that we could go back in time)
:::


```{sh}
#| eval: false

# if not file.exists latest_dump.sql then touch latest_dump.sql

# first: dump the diff to a patch file
pg_dump -U <user> -h <host> -p <port> -d loceval_dev -N tiger -N public -W \
    | diff latest_dump.sql - \
    > $(date +"%Y%m%d")_loceval_diff.patch

# then: patch the changes into `latest`
patch latest_dump.sql -i $(date +"%Y%m%d")_loceval_diff.patch

# # to reverse, apply in reversed order:
# patch -R latest_dump.sql -i $(date +"%Y%m%d")_loceval_diff.patch

```

## monthly

```{sh}
#| eval: false

# first: dump the diff to a patch file
pg_dump -U <user> -h <host> -p <port> -d loceval_dev -N tiger -N public -W \
    > $(date +"%Y%m")_loceval.sql

```

## **TODO** test-restore

After latest structural changes are pushed to production, 
I should attempt to dump/restore the playground data.


# Ad-Hoc Backup/Restore

## connection

```{r libraries-and-paths}
library("dplyr")
library("dbplyr")
library("rprojroot")
library("keyring")
library("configr")
library("DBI")
library("RPostgres")

projroot <- find_root(is_rstudio_project)
working_dbname <- "loceval_dev" 
dbstructure_folder <- "devdb_structure"
connection_profile <- "inbopostgis-dev" 

# unless the password is stored in the config,
# you might want to run the following prior to sourcing or rendering this script:
# keyring::key_set("DBPassword", "db_user_password")

source("MNMDatabaseToolbox.R")

```


For safe testing, I will query data from "production", and store it to "dev".

```{r connect-database}
#| eval: false

config_filepath <- file.path("./inbopostgis_server.conf")
db_source <- connect_database_configfile(
  config_filepath,
  database = "loceval",
  profile = connection_profile
)
db_target <- connect_database_configfile(
  config_filepath,
  database = working_dbname,
  profile = connection_profile
)
```


## data re-loading challenge

start simple, with the `Protocols` table.
No foreign keys, static data.

```{r test-table}
#| eval: false

db_table <- DBI::Id(schema = "metadata", table = "Protocols")
protocols_backup <- dplyr::tbl(db_source, db_table) %>%
  collect
glimpse(protocols_backup)

```

Problem is: we need to keep the `protocol_id` persistent.

If we change the protocols, and upload it, the indices will change because they are serials.

```{r write-table-example}
#| eval: false

rs <- DBI::dbWriteTable(
  db_target,
  db_table,
  protocols_backup,
  row.names = FALSE,
  overwrite = TRUE,
  append = FALSE,
  factorsAsCharacter = TRUE,
  binary = TRUE
)

```

- we cannot append, because that changes the indices
  > Error: COPY returned error : ERROR:  duplicate key value violates unique constraint "Protocols_pkey"
  > DETAIL:  Key (protocol_id)=(1) already exists.
  > CONTEXT:  COPY Protocols, line 1

- we cannot overwrite, because there are dependent tables prohibiting the drop
  > Error: Failed to fetch row : ERROR:  cannot drop table metadata."Protocols" because other objects depend on it
  > DETAIL:  constraint fk_protocols_groupedactivities on table metadata."GroupedActivities" depends on table metadata."Protocols"
  > HINT:  Use DROP ... CASCADE to drop the dependent objects too.


:::{.callout-warning}
We actually have `ON DELETE SET NULL [...]` in place for foreign keys, 
so this *should* work.

However, the problem is that `DBI::dbWriteTable` seems to drop/create the table with `overwrite = TRUE`; dropping is prohibited.

:::


## Solution

:::{.callout-tip title="Strategy"}
We need a framework to dump the whole data and lookup/re-establish connections.
:::

... which is too intricate to be solved in R.
But, with a little help of the Python part... (see `MNMDatabaseToolbox.py::Database.GetDatabaseRelations()`)

### (0) test case

Here a test case for adjusting data in the database table.
```{r testcase-parameters}
db_connection <- connect_database_configfile(
  config_filepath,
  database = working_dbname,
  profile = "inbopostgis-dev"
)

table_key <- "Protocols"
schemas <- read.csv(here::here("devdb_structure", "TABLES.csv")) %>%
  select(table, schema, geometry)

# These are clumsy, temporary, provisional helpers.
# But, hey, there will be time later.
get_schema <- function(tablelabel) {
  return(schemas %>%
    filter(table == tablelabel) %>%
    pull(schema)
  )
}
get_namestring <- function(tablelabel) glue::glue('"{get_schema(tablelabel)}"."{tablelabel}"')
get_tableid <- function(tablelabel) DBI::Id(schema = get_schema(tablelabel), table = tablelabel)

print(get_namestring(table_key))

# test data
new_data <- read.csv(here::here("dumps", "Protocols_new.csv"))

```


### (1) dump all data, for safety

```{r step1-safety-dump}
now <- format(Sys.time(), "%Y%m%d%H%M")
dump_all(
    here::here(glue::glue("dumps", "safedump_{now}.sql")),
    config_filepath = config_filepath,
    database = working_dbname,
    profile = "dumpall",
    user = "monkey",
    exclude_schema = c("tiger", "public")
    )
```

### (2) load current data

```{r step2-load-table-relations}

table_relations <- read_table_relations_config(
  storage_filepath = file.path("./devdb_structure/table_relations.conf")
  ) %>%
  filter(relation_table == tolower(table_key))

dependent_tables <- table_relations %>% pull(dependent_table)

table_data_list <- query_all_existing_data(
    db_connection,
    database = "loceval_dev",
    tables = lapply(c(table_key, dependent_tables), FUN = get_tableid)
)
```

### (3) store key lookup of dependent table

```{r step3-store-lookups}
source("MNMDatabaseToolbox.R")

get_primary_key <- function(tablelabel){
  pk <- load_table_info(dbstructure_folder, tablelabel) %>%
    filter(primary_key == "True") %>%
    pull(column)
  return(pk)
}

get_characteristic_columns <- function(tablelabel){

  # excluded from checks
  logging_columns <- c("log_user", "log_update", "geometry", "wkb_geometry")

  full_table_info <- load_table_info(dbstructure_folder, tablelabel)

  pk <- full_table_info %>%
    filter(primary_key == "True") %>%
    pull(column)

  characteristic_columns <- full_table_info %>%
    filter(
      !(column %in% logging_columns),
      !(column %in% pk),
    ) %>%
    pull(column)
    
  return(characteristic_columns)
}


# query_columns(db_connection, get_tableid(table_key), c("protocol_id", "description"))
# deptab_key <- "GroupedActivities"
query_dependent_columns <- function(table_key, deptab_key) {

  # with a little help from my Python
  deptab_pk <- get_primary_key(deptab_key)

  # get the foreign key columns
  dependent_key <- table_relations %>%
    filter(
      relation_table == tolower(table_key),
      dependent_table == deptab_key
    ) %>%
    pull(dependent_column)

  # lookup the key columns
  key_lookup <- query_columns(
    db_connection,
    get_tableid(deptab_key),
    c(deptab_pk, dependent_key)
  )

  return(key_lookup)
}

lookups <- lapply(
  dependent_tables,
  FUN = function(deptab_key) query_dependent_columns(table_key, deptab_key)
) %>% setNames(dependent_tables)

```

### (4) retrieve old data

```{r step4-old-data}
pk <- get_primary_key(table_key)
characteristic_columns <- get_characteristic_columns("Protocols")

old_data <- query_columns(
  db_connection,
  get_tableid(table_key),
  c(characteristic_columns, pk)
)

restore_data <- dplyr::tbl(db_connection, get_tableid(table_key)) %>% collect()
# TODO: what about `sf` data?

```

### (5) UPLOAD/replace the data

```{r step5-delete-existing}
# TODO continue
```


# TODO

*(persistent)*

- handle user permission exceptions 
  - sterr `2>` to separate file
  - check with cronjob or send an e-mail


*(ad-hoc)*

- make sure this works with `sf` and geometries

# ARCHIVE

## notes

- `pg_dumpall ...` -> apply pg_dump to all databases on a server


## database connection

Working out a way to load the config.
Config has the following structure:

```
    [test]
    host = localhost
    port = 5439
    user = test
    database = playground
    password = <optional: the password IN PLAIN TEXT>
```

:::{.callout-warning}
That `.conf`/`.ini` file can contain password in plain text!

- Do not print its content in this notebook.
- Make sure to `.gitignore` it!
:::
