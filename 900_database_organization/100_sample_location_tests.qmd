---
title: "Sample Location Targeting"
date: "2025-06-05"
format:
  html:
    toc: true
    html-math-method: katex
    code-fold: false
    embed-resources: true
knitr:
  opts_chunk:
    echo: true
---



:::{.callout-note title="Purpose"}
Let's bring the sample to the field!
:::

<https://github.com/inbo/n2khab-mne-monitoring/pull/5>

<https://docs.google.com/spreadsheets/d/12dWpyS2Wsjog3-z3q6-pUzlAnY4MuBbh6igDWH9bEZw/edit?usp=drive_link>

# preparation

## info stream from POC

```{r libraries}
library("dplyr")
library("tidyr")
library("stringr")
library("purrr")
library("lubridate")
library("sf")
library("terra")
library("n2khab")
library("googledrive")
library("readr")
library("rprojroot")

library("configr")
library("DBI")
library("RPostgres")

library("mapview")
# mapviewOptions(platform = "mapdeck")

projroot <- find_root(is_rstudio_project)

# Load some custom GRTS functions
# source(file.path(projroot, "R/grts.R"))
# TODO: rebase once PR#5 gets merged
source("/data/git/n2khab-mne-monitoring_support/020_fieldwork_organization/R/grts.R")


# Setup for googledrive authentication. Set the appropriate env vars in
# .Renviron and make sure you ran drive_auth() interactively with these settings
# for the first run (or to renew an expired Oauth token).
# See ?gargle::gargle_options for more information.
if (Sys.getenv("GARGLE_OAUTH_EMAIL") != "") {
  options(gargle_oauth_email = Sys.getenv("GARGLE_OAUTH_EMAIL"))
}
if (Sys.getenv("GARGLE_OAUTH_CACHE") != "") {
  options(gargle_oauth_cache = Sys.getenv("GARGLE_OAUTH_CACHE"))
}

```


```{r load-sample-rdata}
# Download and load R objects from the POC into global environment
reload <- FALSE
path <- file.path("./data", "objects_panflpan5.RData")
if (reload || !file.exists(path)){
  drive_download(as_id("1a42qESF5L8tfnEseHXbTn9hYR1phqS-S"), path = path)
}
load(path)
```


## Test Case: FAG Calendar

In [this useful file](https://github.com/inbo/n2khab-mne-monitoring/blob/fieldworkdata_supportbyfloris/020_fieldwork_organization/code_snippets.R), I found the following interesting object stack:

- `fag_stratum_grts_calendar_2025_attribs_sf`
  - `fag_stratum_grts_calendar_2025_attribs`
    - `fag_stratum_grts_calendar` (raw)
    - `scheme_moco_ps_stratum_targetpanel_spsamples`
      - `scheme_moco_ps_spsubset_targetfag_stratum_sppost_spsamples_calendar` (raw)
      - `n2khab_strata` (raw)
      - `n2khab_types_expanded_properties` (raw)
  - `grts_mh` (n2khab)
  - `grts_mh_index`
    - (`grts_mh`)
    
(would be cool to have a dependence graph.)


```{r load-fag-grts-calender-2025-attribs-sf}

grts_mh <- read_GRTSmh()
# create a spatial index of the GRTS addresses
grts_mh_index <- tibble(
  id = seq_len(ncell(grts_mh)),
  grts_address = values(grts_mh)[, 1]
) %>%
  filter(!is.na(grts_address))


scheme_moco_ps_stratum_targetpanel_spsamples <-
  scheme_moco_ps_spsubset_targetfag_stratum_sppost_spsamples_calendar %>%
  inner_join(
    n2khab_strata,
    join_by(stratum),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  inner_join(
    n2khab_types_expanded_properties %>%
      select(type, grts_join_method, sample_support_code),
    join_by(type),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  mutate(
    is_forest = str_detect(type, "^9|^2180|^rbbppm")
  ) %>%
  distinct(
    scheme,
    module_combo_code,
    panel_set,
    stratum,
    # 'aquatic' column will be improved for 7220 later on (now it simply has a
    # duplication (TRUE + FALSE) of all locations)
    is_aquatic = in_aquatic_subset,
    is_forest,
    grts_join_method,
    sample_support_code,
    grts_address,
    grts_address_final,
    targetpanel,
    last_type_assessment = assessment_date,
    last_type_assessment_in_field = assessed_in_field,
    last_inaccessible = inaccessible
  ) %>%
  arrange(pick(scheme:grts_address))


fag_stratum_grts_calendar_2025_attribs <-
  fag_stratum_grts_calendar %>%
  select(
    scheme_moco_ps,
    stratum,
    grts_address,
    starts_with("date"),
    field_activity_group,
    rank
  ) %>%
  filter(year(date_start) < 2026) %>%
  # count(date_start, date_end, date_interval) %>%
  # move the fieldwork that was kept for 2024, to 2025, since that is indeed
  # its meaning
  mutate(
    across(c(date_start, date_end), \(x) {
      if_else(year(date_start) == 2024, x + years(1), x)
    }),
    date_interval = interval(
      force_tz(date_start, "Europe/Brussels"),
      force_tz(date_end, "Europe/Brussels")
    )
  ) %>%
  unnest(scheme_moco_ps) %>%
  # adding location attributes
  inner_join(
    scheme_moco_ps_stratum_targetpanel_spsamples %>%
      select(
        scheme,
        module_combo_code,
        panel_set,
        stratum,
        grts_join_method,
        grts_address,
        grts_address_final,
        targetpanel
      ),
    join_by(scheme, module_combo_code, panel_set, stratum, grts_address),
    relationship = "many-to-many",
    unmatched = c("error", "drop")
  ) %>%
  relocate(grts_address_final, .after = grts_address)

  # %>%
  # select(-module_combo_code, -panel_set) %>%
  # # flatten scheme x targetpanel to unique strings per stratum x location x FAG
  # # occasion. Note that the scheme_targetpanels attribute is a shrinked version
  # # of the one at the level of the whole sample (see sampling unit attributes in
  # # the beginning), since we limited the activities to those planned before
  # # 2026, and then generate stratum_scheme_targetpanels as a location attribute.
  # # So it says specifically which schemes & targetpanels are served by the
  # # specific fieldwork at a specific date interval.
  # unite(scheme_targetpanel, scheme, targetpanel, sep = ":") %>%
  # nest(scheme_targetpanels = scheme_targetpanel) %>%
  # mutate(
  #   scheme_targetpanels = map_chr(scheme_targetpanels, \(df) {
  #     str_flatten(
  #       unique(df$scheme_targetpanel),
  #       collapse = " | "
  #     )
  #   }) %>%
  #     factor()
  # ) %>%
  # relocate(scheme_targetpanels)

# # Derive an object where stratum x scheme_targetpanels is flattened per location
# # x FAG occasion. Beware that more locations will emerge due to local
# # replacement, so this is misleading for counting & planning (but useful in
# # spatial visualization)
# fag_grts_calendar_2025_attribs <-
#   fag_stratum_grts_calendar_2025_attribs %>%
#   mutate(
#     stratum_scheme_targetpanels = str_c(
#       stratum,
#       " (",
#       grts_join_method,
#       ") ",
#       " [",
#       scheme_targetpanels,
#       "]"
#     ),
#     .keep = "unused"
#   ) %>%
#   summarize(
#     stratum_scheme_targetpanels =
#       str_flatten(
#         unique(stratum_scheme_targetpanels),
#         collapse = " \u2588 "
#       ) %>%
#       factor(),
#     .by = !stratum_scheme_targetpanels
#   ) %>%
#   relocate(stratum_scheme_targetpanels)
# 
# # A simple derived spatial object (as points; see earlier for the actual unit
# # geometries). Points are still repeated because of different date_interval &
# # FAG values at the same location.
# fag_grts_calendar_2025_attribs_sf <-
#   fag_grts_calendar_2025_attribs %>%
#   add_point_coords_grts(
#     grts_var = "grts_address_final",
#     spatrast = grts_mh,
#     spatrast_index = grts_mh_index
#   )

# glimpse(fag_grts_calendar_2025_attribs_sf)
#
# fag_grts_calendar_2025_attribs_sf %>%
#     mapview::mapview(zcol = "rank")
#     # sf::st_geometry() %>% 
#     # plot()

# knitr::kable(head(
#   sf::st_drop_geometry(fag_grts_calendar_2025_attribs_sf)
# ))
```

Contains a `POINT` geometry and columns:

| stratum                     | <fct>       |
| scheme                      | <fct>       |
| targetpanel                 | <fct>       |
| grts_address                | <int>       |
| grts_address_final          | <int>       |
| date_start                  | <date>      |
| date_end                    | <date>      |
| date_interval               | <varchar>   |
| field_activity_group        | <fct>       |
| rank                        | <int>       |
| geometry                    | <POINT [m]> |


:::{.callout-warning title="Mind the note by Floris:"}
Beware that more locations will emerge due to local replacement, so this is
misleading for counting & planning (but useful in spatial visualization)
:::


## Field Activities, Activity Groups (and Sequences)

```{r view-activity-groups-and-sequences}
#| eval: false

# activity groups, sequences, ...:
actseqs_actgroups_acts <-
  activity_sequences %>%
  semi_join(faseqs, join_by(activity_sequence)) %>%
  inner_join(
    activities,
    join_by(activity),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  arrange(
    activity_group,
    activity_sequence,
    rank,
    activity
  ) %>% 
  group_by(activity_sequence) %>%
  mutate(activity_sequence_id = cur_group_id()) %>%
  ungroup %>%
  group_by(activity_group) %>%
  mutate(activity_group_id = cur_group_id()) %>%
  ungroup %>%
  group_by(
    activity_group,
    activity_sequence,
    activity
  ) %>%
  mutate(activity_id = cur_group_id()) %>%
  ungroup %>%
  relocate(
    activity_id,
    activity_group_id,
    activity_sequence_id,
    activity,
    rank,
    .before = 1
  )

View(
  faseqs_fag_fa %>%
  distinct(field_activity, field_activity_group, activity_sequence) %>%
  arrange(field_activity_group) %>% 
  filter(
    TRUE |
    field_activity_group == "GWINSTPIEZWELL"
  )
)
```

:::{.callout-tip title="Feedback Floris:"}
Activity **sequence** is included in the calendar via the `rank` and that rank already comes delivered in the **calendar**. 
For all practical purposes, everything else of the `ActivitySequences` can be ignored downstream.

For database organization, I rename the `rank` to `activity_sequence`
:::

```{r prepare-activities}
# # TODO there are some activities with different ranks within a sequence.
# activity_groupcount <- activity_sequences %>%
#   distinct(activity, activity_group, rank) %>%
#   count(activity, activity_group) %>%
#   arrange(desc(n))

activity_group_lookup <-
  activity_sequences %>%
    distinct(activity_group, activity)

grouped_activities <-
  activities %>% 
  left_join(
    activity_group_lookup,
    join_by(activity),
    relationship = "one-to-many"
  )

# knitr::kable(grouped_activities %>% distinct(activity_group, activity))

# replace group non-sequenced activities with activity name
grouped_activities <- grouped_activities %>%
  mutate_at(
    vars(activity_group, activity, activity_name, protocol),
    as.character
  ) %>% 
  mutate(
    activity_group = ifelse(is.na(activity_group), activity, activity_group)
  )
# %>% select(activity, activity_group) %>% tail(10) %>% knitr::kable()


grouped_activities <- grouped_activities %>%
  arrange(activity) %>% 
  group_by(activity) %>%
  mutate(activity_id = cur_group_id()) %>%
  ungroup %>%
  arrange(activity_group) %>% 
  group_by(activity_group) %>%
  mutate(activity_group_id = cur_group_id()) %>%
  ungroup %>%
  arrange(activity_group, activity) %>% 
  group_by(
    activity_group,
    activity
  ) %>%
  mutate(grouped_activity_id = cur_group_id()) %>%
  ungroup %>%
  relocate(
    grouped_activity_id,
    activity_group_id,
    activity_group,
    activity_id,
    activity,
    .before = 1
  )

# knitr::kable(grouped_activities %>% distinct(activity_group, activity, activity_name))

# activities %>% 
#   anti_join(
#     activity_sequences,
#     join_by(activity)
#   )
# # non-field activities; of no relevance for the calendar
# 
glimpse(grouped_activities)
```

## stratum

We need to get at least the stratum for the target field activity.

- always `join_by(sample_support_code, unit_id)`
- `stratum_scheme_targetpanels := stratum (grts_join_method) [scheme_targetpanels]` -> replaced (went to upstream table)

| `fag_stratum_grts_calendar_2025_attribs` | the object from which the calendar was derived, |
|                                          | with `stratum` still in there                   |
| `n2khab_types_expanded_properties`       | links type and typelevel to sample support code |
| `stratum_schemetargetpanel_spsamples`    | has scheme, grts, and ssc                       |

```{r join-stratum}
# scheme_moco_ps_spsubset_targetfag_stratum_sppost_spsamples_calendar
# scheme_moco_ps_stratum_targetpanel_spsamples
# stratum_schemetargetpanel_spsamples
# stratum_units_non_cell_n2khab


fag_stratum_grts_calendar_2025_attribs_sf <-
  fag_stratum_grts_calendar_2025_attribs %>%
  add_point_coords_grts(
    grts_var = "grts_address_final",
    spatrast = grts_mh,
    spatrast_index = grts_mh_index
  )

fag_stratum_grts_calendar_2025_attribs_sf %>%
  head(32) %>% 
  sf::st_geometry() %>%
  plot()

# mapview::mapview(
#   fag_stratum_grts_calendar_2025_attribs_sf %>%
#     head(100) %>% 
#     sf::st_geometry()
#   # , col.regions = "steelblue"
#   # , platform = "mapdeck"
#   # , zcol = "stratum"
#   )
```



## database connection

Working out a way to load the config.
Config has the following structure:

```
    [test]
    server = localhost
    port = 5439
    user = test
    database = playground
    password = <the password you entered IN PLAIN TEXT>
```

:::{.callout-warning}
That `.conf`/`.ini` file contains password in plain text!

- Do not print its content in this notebook.
- Make sure to `.gitignore` it!
:::


We can use `configr` (<https://cran.r-project.org/web/packages/configr/readme/README.html>)

```{r load-config}

connect_database_configfile <- function(config_filepath) {

  # read connection info from a config file
  config <- configr::read.config(file = config_filepath)[[1]] # only connects with first config block

  # connect to database
  database_connection <- DBI::dbConnect(
    RPostgres::Postgres(),
    dbname = config$database,
    host = config$server,
    port = config$port,
    user = config$user,
    password = config$password
  )


  # store a label for verbose disconnection at exit
  db_label <- sprintf("%s@%s/%s", config$user, config$server, config$database)

  # remove the config: we do not want to expose credentials further
  # down in this notebook
  rm(config)

  # register disconnect for finalization
  # https://stackoverflow.com/a/41179916
  reg.finalizer(
    .GlobalEnv,
    function(e){
      DBI::dbDisconnect(database_connection)
      message(sprintf("Database %s gracefully disconnected.", db_label))
    },
    onexit = TRUE
  )
  
  return(invisible(database_connection))
}

config_filepath <- file.path("./inbopostgis_server.conf")
db_connection <- connect_database_configfile(config_filepath)

```


# Upload Data

```{r preview-work-to-do}
#| eval: false
glimpse(activities)
glimpse(activity_sequences)
glimpse(fag_stratum_grts_calendar_2025_attribs_sf)
```

## MetaData

A general function to append metadata tables (i.e. upload rows which are missing):

```{r append-tabledata}
# TODO: option to drop; but mind cascading

append_tabledata <- function(conn, db_table, data_to_append, reference_column = NA){
  content <- DBI::dbReadTable(conn, db_table)

  if (is.na(reference_column)) {
    reference_column <- names(data_to_append)[[1]]
  }

  refcol <- enquo(reference_column)
  existing <- content %>% select(!!refcol)
  to_upload <- data_to_append %>%
    anti_join( existing, join_by(!!refcol)
  )

  rs <- DBI::dbWriteTable(conn, db_table, to_upload, overwrite = FALSE, append = TRUE)
  # res <- DBI::dbFetch(rs)
  # DBI::dbClearResult(rs)

  message(sprintf(
    "%s: %i rows uploaded, %i/%i existing judging by '%s'.",
    toString(db_table),
    nrow(to_upload),
    nrow(existing),
    nrow(data_to_append),
    reference_column
  ))
  return(invisible(rs))

}


upload_and_lookup <- function(conn, db_table, data, ref_col, index_col) {

  append_tabledata(conn, db_table, data, reference_column = ref_col)
  
  lookup <- dplyr::tbl(conn, db_table) %>%
    select(!!enquo(ref_col), !!enquo(index_col)) %>% 
    collect

  return(lookup)
}

lookup_join <- function(.data, lookup_tbl, join_column){
  joined_tbl <- .data %>%
    left_join(
      lookup_tbl,
      by = join_by(!!enquo(join_column))
      # relationship = "many-to-one",
      # unmatched = "drop"
    ) %>%
  select(-!!enquo(join_column))

  return(joined_tbl)

}

# DONE there were unmatched activities, not any more.

```

### TeamMembers

A list of team members who may collect data
related to database usernames.

```{r upload-teammembers}
members <- read_csv(here::here("db_structure", "data_TeamMembers.csv"))

member_lookup <- upload_and_lookup(
  db_connection,
  DBI::Id(schema = "metadata", table = "TeamMembers"),
  members,
  ref_col = "username",
  index_col = "teammember_id"
)


```

### Protocols

```{r upload-protocols}

# conn <- db_connection
# db_table <- DBI::Id(schema = "outbound", table = "Protocols")
# data_to_append <- protocols
# reference_column <- "protocol"

protocols <- activities %>%
  select(protocol) %>%
  distinct() %>%
  arrange(protocol) %>%
  filter(!is.na(protocol)) %>%
  mutate(
    protocol_id = 1:n(),
    protocol = as.character(protocol),
    description = NA
  )

protocol_lookup <- upload_and_lookup(
  db_connection,
  DBI::Id(schema = "metadata", table = "Protocols"),
  protocols,
  ref_col = "protocol",
  index_col = "protocol_id"
)

```


### Activity Groups

```{r upload-activity-groups}
#| eval: false

# activity_groups <- activity_sequences %>%
#   select(activity_group) %>%
#   distinct() %>%
#   arrange(activity_group) %>%
#   filter(!is.na(activity_group)) %>%
#   mutate(
#     activity_group_id = 1:n(),
#     activity_group = as.character(activity_group)
#   )

# activity_group_lookup <- upload_and_lookup(
#   db_connection,
#   DBI::Id(schema = "metadata", table = "ActivityGroups"),
#   activity_groups,
#   ref_col = "activity_group",
#   index_col = "activity_group_id"
# )
```


- Activity Sequences

```{r upload-activity-sequences}
#| eval: false

# activity_sequences_to_upload <- activity_sequences %>%
#   lookup_join(activity_lookup, "activity") %>% 
#   lookup_join(activity_group_lookup, "activity_group")
# 
# activity_sequence_lookup <- upload_and_lookup(
#   db_connection,
#   DBI::Id(schema = "metadata", table = "ActivitySequences"),
#   activity_sequences_to_upload,
#   ref_col = "activity_sequence",
#   index_col = "activity_sequence_id"
# )
```

This can be simplified: `group` and `sequence` are attributes of activities anyways.
-> upload into a single table


## POC data

### (Grouped) Activities


```{r upload-grouped-activities}
 
grouped_activities_upload <- grouped_activities %>% 
  lookup_join(protocol_lookup, "protocol")

# append_tabledata(
#   db_connection,
#   DBI::Id(schema = "metadata", table = "GroupedActivities"),
#   grouped_activities_upload,
#   reference_column = "grouped_activity_id"
# )

# done manually to get multiple columns as unique lookup

db_table <- DBI::Id(schema = "metadata", table = "GroupedActivities")
ga_content <- DBI::dbReadTable(db_connection, db_table)

existing <- ga_content %>%
  select(activity_group, activity, activity_group_id, activity_id)
to_upload <- grouped_activities_upload %>%
  anti_join(
    existing,
    join_by(activity_group, activity, activity_group_id, activity_id)
)

if (nrow(to_upload) > 0){
  rs <- DBI::dbWriteTable(
    db_connection,
    db_table,
    to_upload,
    overwrite = FALSE,
    append = TRUE
  )
  # DBI::dbClearResult(rs)
}

grouped_activity_lookup <-
  dplyr::tbl(db_connection, db_table) %>%
  select(activity_group, activity, grouped_activity_id, activity_group_id, activity_id) %>% 
  collect

```





### Habitat Types
`n2khab_types_expanded_properties` -> `N2kHabTypes`

```{r upload-n2khabtype}

n2khabtype_lookup <- upload_and_lookup(
  db_connection,
  DBI::Id(schema = "metadata", table = "N2kHabTypes"),
  n2khab_types_expanded_properties,
  ref_col = "type",
  index_col = "n2khabtype_id"
)

# SELECT DISTINCT type FROM "metadata"."N2kHabTypes" ORDER BY type;
```

## `sf` table

### LocationCalendar

:::{.callout-note}
`fag_stratum_grts_calendar` defines the needed visits of the spatial sampling
units and is organized at the FAG level. The rank is an indication of the needed
order of different FAGs at one location, in the same cycle. In some cases
repetitions do happen for certain FAGs in a scheme, not all FAGs, as prescribed
by the date interval.
:::


:::{.callout-warning}
Note that `rank` is renamed to `activity_sequence`.
:::


```{r upload-calendar}
location_calendar <- fag_stratum_grts_calendar_2025_attribs_sf %>% 
  mutate(
    scheme = as.character(scheme),
    module_combo_code = as.character(module_combo_code),
    stratum = as.character(stratum),
    targetpanel = as.character(targetpanel),
    date_interval = as.character(date_interval)
  ) %>% 
  left_join(
    grouped_activity_lookup %>% distinct(activity_group, grouped_activity_id),
    by = join_by(field_activity_group == activity_group),
    relationship = "many-to-many"
  ) %>%
  select(-field_activity_group, -grts_join_method) %>% # , -date_interval
  rename(activity_sequence = rank) %>% 
  distinct()

sf::st_geometry(location_calendar) <- "wkb_geometry"
glimpse(location_calendar)

```



:::{.callout-note title="calendar filtering."}
Not everything has to be visible.

- only certain activities per user
- only "GW_03.3" for a start.
- sample can be assigned to multiple strata -> union column

:::

```{r filter-loccal}
relevant_activities <- grouped_activity_lookup %>%
  filter(
    activity %in% c(
      "LOCEVALAQ",
      "LOCEVALAQ",
      "LOCEVALAQ",
      "LOCEVALTERR",
      "LSVIAQ",
      "LSVITERR"
    )
  ) %>%
  distinct(grouped_activity_id) %>%
  pull(grouped_activity_id)
    
location_calendar <- location_calendar %>%
  filter(
    grouped_activity_id %in% relevant_activities,
    scheme %in% c("GW_03.3")
  )

if (FALSE){
  location_calendar %>%
    count(grts_address_final) %>%
    filter(n>1)
}

# location_calendar <-
# group_by(across(c(-stratum, -targetpanel))) %>%
location_calendar <- location_calendar %>%
  summarize(
    stratum = paste(stratum, collapse = "+"),
    targetpanel = paste(targetpanel, collapse = "+"),
    .by = c(
      scheme,
      module_combo_code,
      panel_set,
      grouped_activity_id,
      grts_address, grts_address_final,
      date_start, date_end,
      activity_sequence,
      wkb_geometry
    )
  )
#   count(grts_address_final) %>%
#   arrange(desc(n))
# %>% filter(grts_address_final == 452914) # multiple schemes -> filtered
# %>% filter(grts_address_final == 31225) # multiple strata -> retained
glimpse(location_calendar)
```


Upload:

```{r re-upload-loccal}

# delete all previously uploaded table content
rs <- DBI::dbSendStatement(
  db_connection,
  'DELETE FROM "inbound"."Visits";'
  )
DBI::dbClearResult(rs)

rs <- DBI::dbSendStatement(
  db_connection,
  'DELETE FROM "outbound"."LocationCalendar";'
  )
DBI::dbClearResult(rs)

# re-upload
sf::dbWriteTable(
  db_connection,
  DBI::Id(schema = "outbound", table = "LocationCalendar"),
  location_calendar,
  row.names = FALSE,
  overwrite = FALSE,
  append = TRUE,
  factorsAsCharacter = TRUE,
  binary = TRUE
  )
```


The rows in the calendar were not unique!
Here is how to find duplicates:

```{r unique-loccal}
# SELECT * FROM "LocationCalendar";

# DONE: why are these non-unique? -> stratum, targetpanel

location_calendar_unique_count <- location_calendar %>%
  select(
    scheme,
    module_combo_code,
    panel_set,
    stratum,
    targetpanel,
    grouped_activity_id,
    grts_address_final,
    date_start
  ) %>%
  count(
    scheme,
    module_combo_code,
    panel_set,
    stratum,
    targetpanel,
    grouped_activity_id,
    grts_address_final,
    date_start
  ) %>%
  arrange(desc(n))
knitr::kable(head(
  location_calendar_unique_count
  ))
```


```{r join-lookup-loccal}

loccal_lookup <- dplyr::tbl(
    db_connection,
    DBI::Id(schema = "outbound", table = "LocationCalendar")
  ) %>%
  select(
    scheme,
    module_combo_code,
    panel_set,
    stratum,
    targetpanel,
    grouped_activity_id,
    grts_address_final,
    date_start,
    locationcalendar_id
  ) %>% 
  collect


location_calendar <- location_calendar %>%
  left_join(
    loccal_lookup,
    by = join_by(
      scheme,
      module_combo_code,
      panel_set,
      stratum,
      targetpanel,
      grouped_activity_id,
      grts_address_final,
      date_start
    ),
    relationship = "one-to-one",
    unmatched = "error"
  ) 

```


# After Data Collection

All the above code is useful to reset a database with fresh data.
This is the purpose of OUTBOUND: we, the "back office", can freely manipulate and update the data,
whereas field workers cannot.


Now comes the part of handling tables which might already include data.


## Visits

The locations to visit will be stored in a separate table: `inbound.Visits`.

```{r upcoming-visits}
# we must first save the visits to our internal database

visit_preparation <- location_calendar %>%
  filter(
    date_start < today(),
    date_end > today()
  ) %>%
  select(
    locationcalendar_id,
    grouped_activity_id,
    activity_sequence,
    wkb_geometry
  )


sf::dbWriteTable(
  db_connection,
  DBI::Id(schema = "inbound", table = "Visits"),
  visit_preparation,
  row.names = FALSE,
  overwrite = FALSE,
  append = TRUE,
  factorsAsCharacter = TRUE,
  binary = TRUE
  )


```


<!-- TODO join "activity"? --> 

# NEXT STEPS

- qgis design of inbound project
