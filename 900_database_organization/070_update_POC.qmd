---
title: "POC Data Re-Upload"
date: "2025-06-27, 2025-09-04"
format:
  html:
    toc: true
    html-math-method: katex
    code-fold: false
    embed-resources: true
knitr:
  opts_chunk:
    echo: true
---



:::{.callout-note title="Purpose"}
Let's bring the sample to the field - again!

(... and again, and again, and again...)
:::


In all brevity, I collect steps here to get a new POC to the database.


# preparation

## backup

1. Make sure to manually trigger backups.
  - `ssh` to the server
  - execute scripts in `~/backups`
  - bring files home with `scp`

2. Mirrors: try everything on `testing` and then `staging`, first.
  - The `testing` mirror (data sync with `033_populate_testing_db.R`) is open for play and can taken even rough rides, and for that user permissions differ on there. No guarantee that the data is correct.
  - On `staging`, everything is exactly identical to the production server (even permissions and user roles), because it gets synced with the dump/restore commands below.
  - use the `suffix` to control database connections below.
  
  
Syncing the `*_staging` database mirrors:

  - has to be done for `loceval` and `mnmgwdb` 
  - if necessary, re-instantiate the mirror prior to data upload via `postgres`/`dropdb`+`createdb` intervention on the host system (and then run `110_*` and `210_*`, if necessary)
  - Finally, use the `pg_dump | psql` commands as follows:

  
```{sh sync-staging}
#| eval: false
pg_dump -U <backup_user> -h <host> -p <port> -d <database> -N tiger -N public -c > /tmp/<database>_db_dump.txt \
        && psql -U <database_user> -h <host> -p <port> -d <database>_staging -W < /tmp/<database>_db_dump.txt \
        && rm /tmp/<database>_db_dump.txt
```

## libraries and variables

```{r libraries}
# libraries
source("MNMLibraryCollection.R")
load_poc_common_libraries()
load_database_interaction_libraries()

# the database connection object
source("MNMDatabaseConnection.R")

# more specific database tools
source("MNMDatabaseToolbox.R")


# library("mapview") # debugging only
# mapviewOptions(platform = "mapdeck")


# you might want to run the following prior to sourcing or rendering this script:
# keyring::key_set("DBPassword", "db_user_password")

```


## info stream from POC

First, there is the `.RData` file.

```{r load-sample-rdata}
tic <- function(toc) round(Sys.time() - toc, 1)
toc <- Sys.time()
load_poc_rdata(reload = FALSE, to_env = parent.frame())
message(glue::glue("Good morning!
  Loading the POC data took {tic(toc)} seconds today."
))

```

Some code snippets, possibly modified on the way, were provided by Floris.

```{r source-code-snippets}
# TODO: check for changes on every update, e.g. with `meld`:
#       meld 020_fieldwork_organization/code_snippets.R 900_database_organization/050_snippet_selection.R

# Load some custom GRTS functions
# project_root <- find_root(is_git_root)
# source(file.path(project_root, "R/grts.R"))
# TODO: rebase once PR#5 gets merged
snippets_path <- "/data/git/n2khab-mne-monitoring_support"

toc <- Sys.time()
load_poc_code_snippets(snippets_path)
message(glue::glue(
  "... loading/executing the code snippets took {tic(toc)}s."
))
```


Finally, check that everything was loaded correctly.

```{r poc-checks}
verify_poc_objects()
```


## database connection


```{r load-config}

config_filepath <- file.path("./inbopostgis_server.conf")
mirror <- "-staging"

# loceval
loceval_mirror <- glue::glue("loceval{mirror}")

loceval <- connect_mnm_database(
  config_filepath,
  database_mirror = loceval_mirror
) 


# ... and mnmgwdb
mnmgwdb_mirror <- glue::glue("mnmgwdb{mirror}")

mnmgwdb <- connect_mnm_database(
  config_filepath,
  database_mirror = mnmgwdb_mirror
) 
```


## Update, Cascade, Propagate, and Lookup!

Just a convenience function to pass arguments to recursive update,
using higher order function generation to simplify the signature.

```{r cascaded-update-function}

# update_cascade_loceval <- parametrize_cascaded_update(loceval)
# update_cascade_mnmgwdb <- parametrize_cascaded_update(mnmgwdb)

```


## dump all data, for safety

```{r step1-safety-dump}

now <- format(Sys.time(), "%Y%m%d%H%M")
if (isFALSE(grepl("-staging", loceval_mirror))) {
  loceval$dump_all(
    here::here("dumps", glue::glue("safedump_{loceval$database}_{now}.sql")),
    exclude_schema = c("tiger", "public")
  )
}

if (isFALSE(grepl("-staging", mnmgwdb_mirror))) {
  mnmgwdb$dump_all(
    here::here("dumps", glue::glue("safedump_{mnmgwdb$database}_{now}.sql")),
    exclude_schema = c("tiger", "public")
  )
}

```


# Gather and Upload - mnmgwdb

```{r choose-database}
mnmdb <- mnmgwdb
update_cascade_lookup <- parametrize_cascaded_update(mnmdb)
```

## database versioning

```{r version}
version_tag <- "â‰¤v0.9.0_poc0.13.0"
version_notes <- ""
date_applied <- as.integer(format(Sys.time(), "%Y%m%d"))

versions <- mnmdb$query_table("Versions")
data_iteration <- versions %>%
  filter(version_tag == version_tag) %>%
  pull(data_iteration)

if (length(data_iteration) == 0) {
  data_iteration <- 0
} else {
  data_iteration <- data_iteration + 1
}

version_upload <- as_tibble(list(
  "version_tag" = version_tag,
  "data_iteration" = data_iteration,
  "date_applied" = date_applied,
  "notes" = version_notes
))

# DO NOT update_cascade_lookup(table_label = "Versions")
# this would ruin version_ids
# ==> on "Archive", prefer direct insert
mnmdb$insert_data(
  table_label = "Versions",
  upload_data = version_upload
)

version_id <- mnmdb$query_table("Versions") %>%
  filter(
    version_tag == version_tag,
    data_iteration == data_iteration
  ) %>%
  pull(version_id)


if (FALSE) {
  table_label = "Versions"
  glue::glue("
  DELETE FROM {mnmdb$get_namestring(table_label)}
  WHERE version_tag = '{version_tag}'
    AND data_iteration = {data_iteration}
  ;
  ")
}

```


## data assembly

```{r split-data}

categorize_data_update <- function(
    mnmdb,
    table_label,
    data_future,
    characteristic_columns = NA
  ) {

  ## (1) general checks
  stopifnot("dplyr" = require("dplyr"))
    
  if (is.scalar.na(characteristic_columns)) {
    # ... or just take all characteristic columns
    characteristic_columns <- mnmdb$get_characteristic_columns(table_label)
  }

  ## (2) load database status
  data_previous <- mnmdb$query_table(table_label)
    
  ## (3) match-and-mix
  # some rows are present in pre and post
  data_match <- data_future %>%
    semi_join(
      data_previous,
      join_by(!!!characteristic_columns)
    )
  
  # ... but of those, some will need to be updated
  data_changed <- data_match %>%
    anti_join(
      data_previous,
      join_by(!!!names(data_future))
    )
  
  # some data are not relevant any more, and could be archived
  data_for_archive <- data_previous %>% 
    anti_join(
      data_future,
      join_by(!!!characteristic_columns)
    )
  
  # new data is ready for upload
  data_to_upload <- data_future %>% 
    anti_join(
      data_previous,
      join_by(!!!characteristic_columns)
    )

  return(list(
    "changed" = data_changed,
    "to_archive" = data_for_archive,
    "to_upload" = data_to_upload 
  ))
} # /categorize_data_update


upload_additional_data <- function(mnmdb, ...) {
  # parametrize and execute upload function
  update_cascade_lookup <- parametrize_cascaded_update(mnmdb)
  return(update_cascade_lookup(...))
} # /upload_additional_data



# dtypes <- bind_rows(lapply(
#   mnmdb$tables %>% pull(table),
#   FUN = function(tablab) mnmdb$load_table_info(tablab) %>% select(datatype)
# )) %>% distinct()

logging_columns <- c("log_user", "log_update", "geometry", "wkb_geometry")
sql_text <- function (txt) gsub("'", "", txt)
datatype_conversion_catalogue <- c(
  "bool" = function(val) toString(val),
  "boolean" = function(val) toString(val),
  "varchar" = function(val) glue::glue("E'{sql_text(val)}'"),
  "varchar(3)" = function(val) glue::glue("E'{sql_text(val)}'"),
  "varchar(16)" = function(val) glue::glue("E'{sql_text(val)}'"),
  "text" = function(val) glue::glue("E'{sql_text(val)}'"),
  "int" = function(val) sprintf("%.0f", val),
  "integer" = function(val) sprintf("%.0f", val),
  "smallint" = function(val) sprintf("%.0f", val),
  "bigint" = function(val) sprintf("%.0f", val),
  "double precision" = function(val) sprintf("%.8f", val),
  "timestamp" = function(val) format(val, "%Y-%m-%d %H:%M"),
  "date" = function(val) format(val, "%Y%m%d")
)

catch_nans <- function(fcn) function(val) if (is.na(val)) "NULL" else fcn(val)
datatype_conversion_catalogue <- sapply(
  datatype_conversion_catalogue,
  FUN = catch_nans
)

update_existing_data <- function(
    mnmdb,
    table_label,
    changed_data,
    index_columns = NA,
    reference_columns = NA
  ) {

  stopifnot("glue" = require("glue"))

    
  if (is.scalar.na(reference_columns)) {
    # ... or just take all characteristic columns
    reference_columns <- mnmdb$get_characteristic_columns(table_label)
  }

  update_columns <- names(changed_data)
  if (!all(reference_columns %in% update_columns)) {
    missing_refcol <- reference_columns[!(reference_columns %in% update_columns)]
    missing_refcol <- paste0(missing_refcol, collapse = ", ")
    stop(glue::glue(
      "reference columns not found in the upload data: {missing_refcol}"
    ))
  }

  if (is.scalar.na(index_columns)) {
    # ... or just take the primary key
    index_columns <- c(mnmdb$get_primary_key(table_label))
  }

  table_columns <- mnmdb$load_table_info(table_label) %>%
    select(column, datatype)
  existing_columns <- table_columns %>% pull(column)

  # find the columns to update
  update_columns <- update_columns[update_columns %in% existing_columns]
  update_columns <- update_columns[!(update_columns %in% index_columns)]
  update_columns <- update_columns[!(update_columns %in% reference_columns)]
    
  create_update_string <- function(row_nr) {

    update_md <- glue::glue(
        )

    datatype_conversion_catalogue[[]]
  }

    
} # /update_existing_data


mnmdb <- mnmgwdb
table_label <- "Protocols"
changed_data <- protocols_distribution$changed 
index_columns <- c("protocol_id")
reference_columns <- c("protocol_code", "protocol_version")

```


## test case - protocols

```{r db-diff-protocols}

## (0) relevant columns
# columns which uniquely identify a table entry
characteristic_columns <- c("protocol_code", "protocol_version")
# the index column
index_column <- "protocol_id" # mnmdb$get_primary_key("Protocols")

# columns which indicate a change from default state; used to decide whether to archive or remove
data_columns <- list() # format: "column_label" = "unchanged state"

# # TESTING
# INSERT INTO "metadata"."Protocols" (protocol_code, protocol_version, title, language, theme, manager)
# VALUES ('fmp-000-de', '2025.00', 'a test protocol to archive', 'de', 'testing', 'Falk Mielke')
# ;

## get updated raw data
protocols_raw <- read_csv(
  here::here(mnmdb$folder, "data_Protocols.csv"),
  show_col_types = FALSE
)

protocols_raw <- protocols_raw %>%
  arrange(protocol_version, protocol_code)

protocols_raw %>% print(n = Inf)

protocols_distribution <- categorize_data_update(
  mnmdb = mnmgwdb,
  table_label = "Protocols",
  data_future = protocols_raw,
  characteristic_columns = c("protocol_code", "protocol_version")
)

```

# take action

## upload

```{r upload-data}
upload_additional_data(
  mnmgwdb,
  table_label = "Protocols",
  new_data = protocols_distribution$to_upload,
  index_columns = c("protocol_id"),
  tabula_rasa = FALSE,
  characteristic_columns = c("protocol_code", "protocol_version"),
  skip_sequence_reset = FALSE,
  verbose = TRUE
)
```

