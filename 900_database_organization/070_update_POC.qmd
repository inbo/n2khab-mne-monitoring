---
title: "POC Data Re-Upload"
date: "2025-06-27, 2025-09-04"
format:
  html:
    toc: true
    html-math-method: katex
    code-fold: false
    embed-resources: true
knitr:
  opts_chunk:
    echo: true
---



:::{.callout-note title="Purpose"}
Let's bring the sample to the field - again!

(... and again, and again, and again...)
:::


In all brevity, I collect steps here to get a new POC to the database.


# preparation

## backup

1. Make sure to manually trigger backups.
  - `ssh` to the server
  - execute scripts in `~/backups`
  - bring files home with `scp`

2. Mirrors: try everything on `testing` and then `staging`, first.
  - The `testing` mirror (data sync with `033_populate_testing_db.R`) is open for play and can taken even rough rides, and for that user permissions differ on there. No guarantee that the data is correct.
  - On `staging`, everything is exactly identical to the production server (even permissions and user roles), because it gets synced with the dump/restore commands below.
  - use the `suffix` to control database connections below.
  
  
Syncing the `*_staging` database mirrors:

  - has to be done for `loceval` and `mnmgwdb` 
  - if necessary, re-instantiate the mirror prior to data upload via `postgres`/`dropdb`+`createdb` intervention on the host system (and then run `110_*` and `210_*`, if necessary)
  - Finally, use the `pg_dump | psql` commands as follows:

  
```{sh sync-staging}
#| eval: false
pg_dump -U <backup_user> -h <host> -p <port> -d <database> -N tiger -N public -c > /tmp/<database>_db_dump.txt \
        && psql -U <database_user> -h <host> -p <port> -d <database>_staging -W < /tmp/<database>_db_dump.txt \
        && rm /tmp/<database>_db_dump.txt
```

## libraries and variables

```{r libraries}
# libraries
source("MNMLibraryCollection.R")
load_poc_common_libraries()
load_database_interaction_libraries()

# the database connection object
source("MNMDatabaseConnection.R")

# more specific database tools
source("MNMDatabaseToolbox.R")


# library("mapview") # debugging only
# mapviewOptions(platform = "mapdeck")

```


## info stream from POC

First, there is the `.RData` file.

```{r load-sample-rdata}
tic <- function(toc) round(Sys.time() - toc, 1)
toc <- Sys.time()
load_poc_rdata(reload = FALSE, to_env = parent.frame())
message(glue::glue("Good morning!
  Loading the POC data took {tic(toc)} seconds today."
))

```

Some code snippets, possibly modified on the way, were provided by Floris.

```{r source-code-snippets}
# TODO: check for changes on every update, e.g. with `meld`:
#       meld 020_fieldwork_organization/code_snippets.R 900_database_organization/050_snippet_selection.R

# Load some custom GRTS functions
# project_root <- find_root(is_git_root)
# source(file.path(project_root, "R/grts.R"))
# TODO: rebase once PR#5 gets merged
snippets_path <- "/data/git/n2khab-mne-monitoring_support"

toc <- Sys.time()
load_poc_code_snippets(snippets_path)
message(glue::glue(
  "... loading/executing the code snippets took {tic(toc)}s."
))
```


Finally, check that everything was loaded correctly.

```{r poc-checks}
verify_poc_objects()
```


## database connection


```{r load-config}

config_filepath <- file.path("./inbopostgis_server.conf")
mirror <- "-staging"

# loceval
loceval_mirror <- glue::glue("loceval{mirror}")
# 
# loceval <- connect_mnm_database(
#   config_filepath,
#   database_mirror = loceval_mirror
# ) 


# ... and mnmgwdb
mnmgwdb_mirror <- glue::glue("mnmgwdb{mirror}")

mnmgwdb <- connect_mnm_database(
  config_filepath,
  database_mirror = mnmgwdb_mirror
) 
```


## dump all data, for safety

```{r step1-safety-dump}

now <- format(Sys.time(), "%Y%m%d%H%M")
if (isFALSE(grepl("-staging", loceval_mirror))) {
  loceval$dump_all(
    here::here("dumps", glue::glue("safedump_{loceval$database}_{now}.sql")),
    exclude_schema = c("tiger", "public")
  )
}

if (isFALSE(grepl("-staging", mnmgwdb_mirror))) {
  mnmgwdb$dump_all(
    here::here("dumps", glue::glue("safedump_{mnmgwdb$database}_{now}.sql")),
    exclude_schema = c("tiger", "public")
  )
}

```


# Gather and Upload - mnmgwdb

## database versioning

```{r version}

if (FALSE) {
  # manually set a new version
  version_tag <- "â‰¤v0.9.0_poc0.13.0"
  version_notes <- ""
  date_applied <- as.integer(format(Sys.time(), "%Y%m%d"))

  version_id <- mnmgwdb$tag_new_version(
    version_tag,
    version_notes,
    date_applied = NA
  )
} else {
  # per default, the latest version is taken
  version_id <- mnmgwdb$load_latest_version_id()
}

if (FALSE) {
  # ... ad hoc / testing: delete version again
  table_label <- "Versions"
  glue::glue("
  DELETE FROM {mnmgwdb$get_namestring(table_label)}
  WHERE version_tag = '{version_tag}'
    AND data_iteration = {data_iteration}
  ;
  ")
}


# TODO: I still have to make up my mind whether to archive everything.
#       For now, I will preliminarily keep it with an "archive_id" column.
```


# Test Case 1: Protocols

## diff

```{r db-diff-protocols}
#| eval: false

## relevant structural columns
# columns which uniquely identify a table entry
characteristic_columns <- c("protocol_code", "protocol_version")
# the index column
index_column <- "protocol_id" # mnmgwdb$get_primary_key("Protocols")

# for some columns, existing data may not be overwritten
# (i.e. the database is the one and only reference)
precedence_columns <- list(
  "Protocols" = c("subtitle")
)

# # columns which indicate a change from default state; used to decide whether to archive or remove
# data_columns <- list() # format: "column_label" = "unchanged state"

# # FOR TESTING
# INSERT INTO "metadata"."Protocols" (protocol_code, protocol_version, title, language, theme, manager)
# VALUES ('fmp-000-de', '2025.00', 'a test protocol to archive', 'de', 'testing', 'Falk Mielke')
# ;

## get updated raw data
protocols_raw <- read_csv(
  here::here(mnmgwdb$folder, "data_Protocols.csv"),
  show_col_types = FALSE
)

protocols_raw <- protocols_raw %>%
  arrange(protocol_version, protocol_code)

protocols_raw %>% print(n = Inf)

protocols_distribution <- categorize_data_update(
  mnmdb = mnmgwdb,
  table_label = "Protocols",
  data_future = protocols_raw,
  input_precedence_columns = precedence_columns[["Protocols"]],
  characteristic_columns = characteristic_columns
)

print_category_count(protocols_distribution, "Protocols")
```

## update

```{r update-existing}
#| eval: false

update_existing_data(
  mnmdb = mnmgwdb,
  table_label = "Protocols",
  changed_data = protocols_distribution$changed,
  input_precedence_columns = precedence_columns[["Protocols"]],
  index_columns = c(index_column),
  reference_columns = characteristic_columns
)
```

## upload

```{r upload-data}
#| eval: false

upload_additional_data(
  mnmdb = mnmgwdb,
  table_label = "Protocols",
  new_data = protocols_distribution$to_upload,
  index_columns = c(index_column),
  tabula_rasa = FALSE,
  characteristic_columns = characteristic_columns,
  skip_sequence_reset = FALSE,
  verbose = TRUE
)
```

## rows to archive

```{r archive-data}
#| eval: false

archive_ancient_data(
  mnmdb = mnmgwdb,
  table_label = "Protocols",
  data_to_archive = protocols_distribution$to_archive,
  version_id = version_id,
  reference_columns = c(index_column)
)

# ... and un-archive = reactivate
reactivate_archived_data(
  mnmdb = mnmgwdb,
  table_label = "Protocols",
  data_to_archive = protocols_distribution$reactivate,
  reference_columns = c(index_column)
)


# NOTE: un-archived rows are not subject to UPDATE
#       if in doubt, run this again.
```


# Prune Unused Rows

I would tend to delete unused calendar entries;

