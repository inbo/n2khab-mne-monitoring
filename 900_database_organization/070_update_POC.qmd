---
title: "POC Data Re-Upload"
date: "2025-06-27"
format:
  html:
    toc: true
    html-math-method: katex
    code-fold: false
    embed-resources: true
knitr:
  opts_chunk:
    echo: true
---



:::{.callout-note title="Purpose"}
Let's bring the sample to the field - again!

(... and again, and again, and again...)
:::


In all brevity, I collect steps here to get a new POC to the database.

Here are all the choices you have.

```{r upload-parameters-dev}
#| eval: true
working_dbname <- "loceval_dev"
config_filepath <- file.path("./inbopostgis_server.conf")
connection_profile <- "loceval-dev"
dbstructure_folder <- "./loceval_dev_structure"

config <- configr::read.config(file = config_filepath)[[connection_profile]]
```

```{r upload-parameters-testing}
#| eval: false
working_dbname <- "loceval_testing"
config_filepath <- file.path("./inbopostgis_server.conf")
connection_profile <- "testing"
dbstructure_folder <- "./loceval_db_structure"

config <- configr::read.config(file = config_filepath)[[connection_profile]]
```


# preparation

## libraries and variables

```{r libraries}
library("dplyr")
library("tidyr")
library("stringr")
library("purrr")
library("lubridate")
library("sf")
library("terra")
library("n2khab")
library("googledrive")
library("readr")
library("rprojroot")
library("keyring")

library("configr")
library("DBI")
library("RPostgres")

library("mapview")
# mapviewOptions(platform = "mapdeck")

projroot <- find_root(is_rstudio_project)

# you might want to run the following prior to sourcing or rendering this script:
# keyring::key_set("DBPassword", "db_user_password")

source("MNMDatabaseToolbox.R")

# Load some custom GRTS functions
# source(file.path(projroot, "R/grts.R"))
# TODO: rebase once PR#5 gets merged
source("/data/git/n2khab-mne-monitoring_support/020_fieldwork_organization/R/grts.R")
source("/data/git/n2khab-mne-monitoring_support/020_fieldwork_organization/R/misc.R")

```


## info stream from POC

```{r load-sample-rdata}


# Setup for googledrive authentication. Set the appropriate env vars in
# .Renviron and make sure you ran drive_auth() interactively with these settings
# for the first run (or to renew an expired Oauth token).
# See ?gargle::gargle_options for more information.
if (Sys.getenv("GARGLE_OAUTH_EMAIL") != "") {
  options(gargle_oauth_email = Sys.getenv("GARGLE_OAUTH_EMAIL"))
}
if (Sys.getenv("GARGLE_OAUTH_CACHE") != "") {
  options(gargle_oauth_cache = Sys.getenv("GARGLE_OAUTH_CACHE"))
}

# Download and load R objects from the POC into global environment
reload <- FALSE # in this one, we normally reload.
poc_rdata_path <- file.path("./data", "objects_panflpan5.RData")
if (reload || !file.exists(poc_rdata_path)){

  # copy the old file
  if (file.exists(poc_rdata_path)) {
    this_date <- format(Sys.time(), "%Y%m%d")
    backup_path <- file.path("./data", glue::glue("objects_panflpan5_{this_date}.bak"))
    file.copy(from = poc_rdata_path, to = backup_path, overwrite = TRUE)
  }

  drive_download(
    as_id("1a42qESF5L8tfnEseHXbTn9hYR1phqS-S"),
    path = poc_rdata_path,
    overwrite = reload
  )
}
load(poc_rdata_path)

versions_required <- c(versions_required, "habitatmap_2024_v99_interim")
verify_n2khab_data(n2khab_data_checksums_reference, versions_required)


```


## snippets!

Some code snippets, possibly modified on the way, were provided by Floris.

```{r source-code-snippets}
# TODO: check for changes on every update, e.g. with `meld`:
#       meld 020_fieldwork_organization/code_snippets.R 900_database_organization/050_snippet_selection.R

invisible(capture.output(source("050_snippet_selection.R")))
source("051_snippet_transformation_code.R")

stopifnot(
  "NOT FOUND: snip snap >> `grts_mh_index`" = exists("grts_mh_index")
)

stopifnot(
  "NOT FOUND: snip snap >> `scheme_moco_ps_stratum_targetpanel_spsamples`" =
    exists("scheme_moco_ps_stratum_targetpanel_spsamples")
)

stopifnot(
  "NOT FOUND: snip snap >> `stratum_schemepstargetpanel_spsamples`" =
    exists("stratum_schemepstargetpanel_spsamples")
)

stopifnot(
  "NOT FOUND: snip snap >> `units_cell_polygon`" =
    exists("units_cell_polygon")
)

stopifnot(
  "NOT FOUND: RData >> `activities`" =
    exists("activities")
)

stopifnot(
  "NOT FOUND: RData >> `activity_sequences`" =
    exists("activity_sequences")
)

stopifnot(
  "NOT FOUND: RData >> `n2khab_strata`" =
    exists("n2khab_strata")
)

stopifnot(
  "snip snap >> `orthophoto grts` not found" =
    exists("orthophoto_2025_type_grts")
)

# fieldwork calendar
stopifnot(
  "NOT FOUND: snip snap >> `fieldwork_2025_prioritization_by_stratum`" =
    exists("fieldwork_2025_prioritization_by_stratum")
)

# replacements
stopifnot(
  "NOT FOUND: snip snap >> `stratum_schemepstargetpanel_spsamples_terr_replacementcells`" =
    exists("stratum_schemepstargetpanel_spsamples_terr_replacementcells")
)


```


## database connection


```{r load-config}


db_connection <- connect_database_configfile(
  config_filepath,
  database = working_dbname,
  profile = connection_profile
)


schemas <- read.csv(here::here(dbstructure_folder, "TABLES.csv")) %>%
  select(table, schema, geometry)

# These are clumsy, temporary, provisional helpers.
# But, hey, there will be time later.
get_schema <- function(tablelabel) {
  return(schemas %>%
    filter(table == tablelabel) %>%
    pull(schema)
  )
}
get_namestring <- function(tablelabel) glue::glue('"{get_schema(tablelabel)}"."{tablelabel}"')
get_tableid <- function(tablelabel) DBI::Id(schema = get_schema(tablelabel), table = tablelabel)


```


## Update, Cascade, Propagate, and Lookup!

Just a convenience function to pass arguments to recursive update,
using higher order function generation to simplify the signature.

```{r cascaded-update-function}

update_cascade_lookup <- parametrize_cascaded_update(
  config_filepath,
  working_dbname,
  connection_profile,
  dbstructure_folder,
  db_connection
)

```


# dump all data, for safety

```{r step1-safety-dump}
now <- format(Sys.time(), "%Y%m%d%H%M")
dump_all(
  here::here("dumps", glue::glue("safedump_{working_dbname}_{now}.sql")),
  config_filepath = config_filepath,
  database = working_dbname,
  profile = "dumpall",
  user = "monkey",
  exclude_schema = c("tiger", "public")
)

```

# Gather and Upload

## Prune "LocationAssessments"

LocationAssessments are tightly linked to a version of the sample.
However, we do not want to loose previous assessments.

As a first step, we remove all rows from the `LocationAssessments` which are apparrently untouched.

```{r delete-undone-location-assessments}

# > you might want to add:
#  AND (NOT cell_disapproved)
#  AND (revisit_disapproval IS NULL)
#  AND (disapproval_explanation IS NULL)
#  AND (type_suggested IS NULL)
#  AND (implications_habitatmap IS NULL)
#  AND (feedback_habitatmap IS NULL)
#  AND (notes IS NULL)

# clean up LocationAssessments
verb <- "DELETE "
table_str <- '"outbound"."LocationAssessments"'
maintenance_users <- sprintf("'{update,%s}'", config$user)
cleanup_query <- glue::glue(
  "{verb} FROM {table_str}
    WHERE log_user = ANY ({maintenance_users}::varchar[])
     AND (NOT assessment_done);"
)
execute_sql(
  db_connection,
  cleanup_query,
  verbose = TRUE
)

```

The remainder are the ones we would like to store.

```{r load-previous-location-assessments}

# add location for previous LocationAssessment

previous_location_assessments <- dplyr::tbl(
  db_connection,
  DBI::Id(schema = "outbound", table = "LocationAssessments"),
  ) %>% collect()

```

## Retain "Visits"

Analogously, we can clean the table of `Visits` which never happened.

```{r prune-visits}
table_str <- '"inbound"."Visits"'
maintenance_users <- sprintf("'{update,%s}'", config$user)
cleanup_query <- glue::glue(
  "DELETE FROM {table_str}
    WHERE log_user = ANY ({maintenance_users}::varchar[])
     AND NOT visit_done;"
)
execute_sql(
  db_connection,
  cleanup_query,
  verbose = TRUE
)

previous_visits <- dplyr::tbl(
  db_connection,
  DBI::Id(schema = "inbound", table = "Visits"),
  ) %>% collect()

```


## ... and FieldActivityCalendar

```{r prune-visits}
table_str <- '"outbound"."FieldActivityCalendar"'
maintenance_users <- sprintf("'{update,%s}'", config$user)
cleanup_query <- glue::glue(
  "DELETE FROM {table_str}
    WHERE log_user = ANY ({maintenance_users}::varchar[])
     AND (NOT done_planning)
     AND (teammember_assigned IS NULL)
     AND (date_visit_planned IS NULL)
     AND (notes IS NULL)
   ;"
)
execute_sql(
  db_connection,
  cleanup_query,
  verbose = TRUE
)

previous_calendar_plans <- dplyr::tbl(
  db_connection,
  DBI::Id(schema = "outbound", table = "FieldActivityCalendar"),
  ) %>% collect()

```


## Locations

Starting from `fag_stratum_grts_calendar` to extract all info about the sample locations...

```{r db-sample-locations-assembly}
sample_locations <-
  fag_stratum_grts_calendar %>%
  common_current_calenderfilters() %>%
  distinct(
    scheme_moco_ps,
    stratum,
    grts_address
  ) %>%
  unnest(scheme_moco_ps) %>%
  # adding location attributes
  inner_join(
    scheme_moco_ps_stratum_targetpanel_spsamples %>%
      select(
        scheme,
        module_combo_code,
        panel_set,
        stratum,
        grts_join_method,
        grts_address,
        grts_address_final,
        targetpanel
      ) %>%
      # deduplicating 7220:
      distinct(),
    join_by(scheme, module_combo_code, panel_set, stratum, grts_address),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  common_current_samplefilters() %>%
  # also join the spatial poststratum, since we need this in setting
  # GRTS-address based priorities
  inner_join(
    scheme_moco_ps_stratum_sppost_spsamples %>%
      unnest(sp_poststr_samples) %>%
      select(-sample_status),
    join_by(scheme, module_combo_code, panel_set, stratum, grts_address),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  select(-module_combo_code) %>%
  nest_scheme_ps_targetpanel() %>%
  # add MHQ assessment metadata
  inner_join(
    stratum_grts_n2khab_phabcorrected_no_replacements %>%
      select(stratum, grts_address, assessed_in_field, assessment_date),
    join_by(stratum, grts_address),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  distinct() %>%
  convert_stratum_to_type() %>%
  rename_grts_address_final_to_grts_address() %>%
  rename(
    assessment = assessed_in_field,
    assessment_date = assessment_date # triv.
  ) %>%
  relocate(grts_address) %>%
  relocate(grts_join_method, .after = grts_address) %>%
  mutate(
    previous_notes = NA # FUTURE TODO
  ) %>%
  mutate(
    across(c(
        grts_join_method,
        scheme_ps_targetpanels,
        sp_poststratum,
        type
      ),
      as.character
    )
  )

```


... we take the new locations *and* the ones which were assessed before.

```{r distinct-locations}
locations <- bind_rows(
    sample_locations %>% select(grts_address),
    previous_location_assessments %>% select(grts_address),
    previous_visits %>% select(grts_address)
  ) %>%
  mutate(grts_address = as.integer(grts_address)) %>%
  distinct() %>%
  # count(grts_address) %>%
  # arrange(desc(n))
  add_point_coords_grts(
    grts_var = "grts_address",
    spatrast = grts_mh,
    spatrast_index = grts_mh_index
  )

sf::st_geometry(locations) <- "wkb_geometry"

```


**Upload Spatial Locations:**

... in a "DELETE, then INSERT" approach.
Reminder: the DELETE cascades as is to the `LocationCells`.
Note that this is possible because I avoided the use of `location_id` as a foreign key.
Which lead to further trouble.
Solved by re-establishing `location_id` via `grts_address` below,
which unfortunately triggers update rules.
Can't have everything :shrug:.

```{r upload-locations}

locations_lookup <- update_cascade_lookup(
  schema = "metadata",
  table_key = "Locations",
  new_data = locations,
  index_columns = c("location_id"),
  characteristic_columns = c("grts_address"),
  tabula_rasa = TRUE,
  verbose = TRUE
)

```

## Location Cells (Polygons)

The location cells are polygons which spatially contain the sample unit.
Up until now, these are only 32x32m squares.
For other types, we can later supplement more complex polygons.
Another `geom` table with unit centroids is possible.


All `LocationCells` can be deleted prior to update, because they are a 1-on-1 derivative of `Locations`.


```{r location-polygon-upload}
units_cell_polygon[["grts_address_final"]] <-
  as.integer(units_cell_polygon[["grts_address_final"]])

# unit geometries (cells):
location_cells <-
  units_cell_polygon %>%
  inner_join(
    locations_lookup,
    by = join_by(grts_address_final == grts_address),
    relationship = "one-to-many",
    unmatched = "drop"
  ) %>%
  select(-grts_address_final) %>%
  relocate(geometry, .after = last_col())

sf::st_geometry(location_cells) <- "wkb_geometry"

# glimpse(location_cells)

message("________________________________________________________________")
message(glue::glue("DELETE/INSERT of metadata.LocationCells"))

execute_sql(
  db_connection,
  glue::glue('DELETE  FROM "metadata"."LocationCells";'),
  verbose = TRUE
)

append_tabledata(
  db_connection,
  DBI::Id(schema = "metadata", table = "LocationCells"),
  location_cells,
  reference_columns = "location_id"
)


```

## SampleUnits

Whereas `Locations` holds the GIS information of a location,
`SampleUnits` are a collection of metadata about each sample.

```{r refresh-sample-locations}
if ("location_id" %in% names(sample_locations)) {
  # should not be the case in a continuous script;
  # this is extra safety for debugging and de-serial execution
  sample_locations <- sample_locations %>%
    select(-location_id)
}
sample_locations <- sample_locations %>%
  left_join(
    locations_lookup,
    by = join_by(grts_address),
    relationship = "many-to-one"
  )


slocs_refcols <- c(
  "type",
  "grts_address",
  "scheme",
  "panel_set",
  "targetpanel"
  # "sp_poststratum"
)

# tabula rasa: might otherwise be duplicated due to missing fk and null constraint
sample_locations_lookup <- update_cascade_lookup(
  schema = "outbound",
  table_key = "SampleUnits",
  new_data = sample_locations,
  index_columns = c("sampleunit_id"),
  characteristic_columns = slocs_refcols,
  tabula_rasa = TRUE,
  verbose = TRUE
)
```


STOP - TODO: this should not be necessary, but I will check.

```{r restore-location-id-via-grts}
#| eval: false

# restore location_id's via grts
restore_location_id_by_grts(
  db_connection,
  dbstructure_folder,
  target_schema = "outbound",
  table_key = "SampleUnits",
  retain_log = FALSE,
  verbose = TRUE
)

```

## fieldwork calendar

```{r load-activity-group-lookup}
# grouped_activities %>% distinct(activity_group, activity_group_id) %>% count(activity_group) %>% print(n=Inf)


activity_groupid_lookup <- 
  dplyr::tbl(
    db_connection,
    DBI::Id(schema = "metadata", table = "GroupedActivities"),
  ) %>%
  distinct(activity_group, activity_group_id) %>%
  collect()

# activity_groupid_lookup %>% distinct(activity_group, activity_group_id) %>% count(activity_group) %>% print(n=Inf)

```


```{r fieldwork-calendar}
fieldwork_calendar <- 
  fieldwork_2025_prioritization_by_stratum %>%
  rename_grts_address_final_to_grts_address() %>%
  relocate(grts_address) %>%
  relocate(grts_join_method, .after = grts_address) %>%
  select(
    -scheme_ps_targetpanels
  ) %>% 
  inner_join(
    n2khab_strata,
    join_by(stratum),
    relationship = "many-to-one",
    unmatched = c("error", "drop")
  ) %>%
  left_join(
    sample_locations_lookup %>%
      select(type, grts_address, sampleunit_id),
    by = join_by(type, grts_address),
    relationship = "many-to-many", # TODO
    unmatched = "drop"
  ) %>%
  select(-type) %>%
  rename(activity_rank = rank) %>%
  left_join(
    activity_groupid_lookup,
    by = join_by(field_activity_group == activity_group),
    relationship = "many-to-one"
  ) %>%
  select(-field_activity_group) %>% 
  mutate(
    across(c(
        stratum,
        grts_join_method,
        date_interval
      ),
      as.character
    )
  ) %>%
  mutate(
    log_user = "update",
    log_update = as.POSIXct(Sys.time()),
    excluded = FALSE,
    no_visit_planned = FALSE,
    done_planning = FALSE
  )
  
# fieldwork_calendar %>% glimpse
```

```{r upload-fac}
fieldwork_calendar_lookup <- update_cascade_lookup(
  schema = "outbound",
  table_key = "FieldActivityCalendar",
  new_data = fieldwork_calendar,
  index_columns = c("fieldactivitycalendar_id"),
  characteristic_columns = NULL,
  tabula_rasa = FALSE,
  verbose = TRUE
)
  
```


## Local Replacements


```{r list-replacements}
#| eval: true
# glimpse(stratum_schemepstargetpanel_spsamples_terr_replacementcells)

# TODO: store previous replacement info to another table

replacements <- stratum_schemepstargetpanel_spsamples_terr_replacementcells %>%
  select(stratum, grts_address, replacement_cells) %>%
  unnest(replacement_cells) %>%
  filter(!is.na(cellnr_replac)) %>% 
  left_join(
    n2khab_strata,
    by = join_by(stratum),
    relationship = "many-to-many" # TODO
  ) %>% 
  select(-stratum) %>% 
  rename(
    cellnr_replacement = cellnr_replac,
    grts_address_replacement = grts_address_replac,
    replacement_rank = ranknr
  ) %>%
  left_join(
    sample_locations_lookup %>%
      select(type, grts_address, sampleunit_id),
    by = join_by(type, grts_address),
    relationship = "many-to-many", # TODO
    unmatched = "drop"
  ) %>%
  select(
    -type,
    -grts_address,
    -cellnr_replacement
  ) %>%
  filter(!is.na(sampleunit_id)) %>% 
  add_point_coords_grts(
    grts_var = "grts_address_replacement",
    spatrast = grts_mh,
    spatrast_index = grts_mh_index
  ) 

sf::st_geometry(replacements) <- "wkb_geometry"

# glimpse(replacements)
```


```{r upload-replacements}
#| eval: true

replacements_lookup <- update_cascade_lookup(
  schema = "outbound",
  table_key = "Replacements",
  new_data = replacements,
  index_columns = c("replacement_id"),
  characteristic_columns = c("sampleunit_id", "grts_address_replacement"),
  tabula_rasa = TRUE,
  verbose = TRUE
)

```


## LocationAssessments

```{r refresh-location-assessments}

new_location_assessments <- sample_locations %>%
  distinct(
    grts_address,
    type
  ) %>%
  left_join(
    locations_lookup,
    by = join_by(grts_address),
  ) %>%
  mutate(
    log_user = "update",
    log_update = as.POSIXct(Sys.time()),
    cell_disapproved = FALSE,
    assessment_done = FALSE
  )

new_location_assessments <- new_location_assessments %>%
  anti_join(
    previous_location_assessments,
    by = join_by(type, grts_address)
  ) %>%
  select(-location_id) %>%
  left_join(
    locations_lookup,
    by = join_by(grts_address),
  )
# nrow(location_assessments)
```


The `update-cascade-lookup` function will retain all existing assessments, but add the new ones.
This then also requires a refreshment of the `location_id` lookup: the index of the locations linked in previous assessments might have changed.

```{r upload-location-assessments}

# append the LocationAssessments with empty lines for new sample units
locationassessment_lookup <- update_cascade_lookup(
  schema = "outbound",
  table_key = "LocationAssessments",
  new_data = new_location_assessments,
  index_columns = c("locationassessment_id"),
  characteristic_columns = c("type", "grts_address"),
  tabula_rasa = FALSE,
  verbose = TRUE
)

# restore location_id's
restore_location_id_by_grts(
  db_connection,
  dbstructure_folder,
  target_schema = "outbound",
  table_key = "LocationAssessments",
  retain_log = TRUE,
  verbose = TRUE
)

```


You can check the status/progress of location updates as follows:

```{sql test-run-sql}
#| connection: db_connection
SELECT DISTINCT assessment_done, count(*) FROM "outbound"."LocationAssessments" GROUP BY assessment_done;
```


## Visits

Analogous to LocationAssessments, we retain previous field visits and append potential new locations.
This also demands re-linking the `location_id`.
In the future, this will be replaced by a proper fieldwork planning machinery.


```{r upload-visits}

new_visits <- sample_locations_lookup %>%
  left_join(
    locations_lookup,
    by = join_by(grts_address),
    relationship = "many-to-one"
  ) %>%
  select(sampleunit_id, location_id, grts_address) %>%
  mutate(
    grouped_activity_id = NA,
    teammember_id = NA,
    date_visit = as.Date(NA),
    log_user = "update",
    log_update = as.POSIXct(Sys.time()),
    visit_done = FALSE
  )

# new_visits %>%
#   count(location_id, grts_address) %>%
#   arrange(desc(n))


# NOTE the location is still not unique?!
# -> Of course:
#     There are multiple sample units with different `type`
#     on identical locations.

# append the LocationAssessments with empty lines for new sample units
visits_lookup <- update_cascade_lookup(
  schema = "inbound",
  table_key = "Visits",
  new_data = new_visits,
  index_columns = c("visit_id"),
  characteristic_columns = c("grts_address", "sampleunit_id"),
  tabula_rasa = FALSE,
  verbose = TRUE
)

# restore location_id's
restore_location_id_by_grts(
  db_connection,
  dbstructure_folder,
  target_schema = "inbound",
  table_key = "Visits",
  retain_log = TRUE,
  verbose = TRUE
)

```
