---
title: "POC Data Re-Upload"
date: "2025-06-27"
format:
  html:
    toc: true
    html-math-method: katex
    code-fold: false
    embed-resources: true
knitr:
  opts_chunk:
    echo: true
---



:::{.callout-note title="Purpose"}
Let's bring the sample to the field - again!

(... and again, and again, and again...)
:::


In all brevity, I collect steps here to get a new POC to the database.

Here are all the choices you have.

```{r upload-parameters}
working_dbname <- "loceval_testing"
config_filepath <- file.path("./inbopostgis_server.conf")
connection_profile <- "testing"
dbstructure_folder <- "./db_structure"
```


# preparation

## libraries and variables

```{r libraries}
library("dplyr")
library("tidyr")
library("stringr")
library("purrr")
library("lubridate")
library("sf")
library("terra")
library("n2khab")
library("googledrive")
library("readr")
library("rprojroot")
library("keyring")

library("configr")
library("DBI")
library("RPostgres")

library("mapview")
# mapviewOptions(platform = "mapdeck")

projroot <- find_root(is_rstudio_project)

# you might want to run the following prior to sourcing or rendering this script:
# keyring::key_set("DBPassword", "db_user_password")

source("MNMDatabaseToolbox.R")

# Load some custom GRTS functions
# source(file.path(projroot, "R/grts.R"))
# TODO: rebase once PR#5 gets merged
source("/data/git/n2khab-mne-monitoring_support/020_fieldwork_organization/R/grts.R")

```


## info stream from POC

```{r load-sample-rdata}


# Setup for googledrive authentication. Set the appropriate env vars in
# .Renviron and make sure you ran drive_auth() interactively with these settings
# for the first run (or to renew an expired Oauth token).
# See ?gargle::gargle_options for more information.
if (Sys.getenv("GARGLE_OAUTH_EMAIL") != "") {
  options(gargle_oauth_email = Sys.getenv("GARGLE_OAUTH_EMAIL"))
}
if (Sys.getenv("GARGLE_OAUTH_CACHE") != "") {
  options(gargle_oauth_cache = Sys.getenv("GARGLE_OAUTH_CACHE"))
}

# Download and load R objects from the POC into global environment
reload <- FALSE # in this one, we normally reload.
poc_rdata_path <- file.path("./data", "objects_panflpan5.RData")
if (reload || !file.exists(poc_rdata_path)){
  drive_download(
    as_id("1a42qESF5L8tfnEseHXbTn9hYR1phqS-S"),
    path = poc_rdata_path,
    overwrite = reload
  )
}
load(poc_rdata_path)

```


## snippets!

Some code snippets, possibly modified on the way, were provided by Floris.

```{r source-code-snippets}
# TODO: check for changes on every update.

source("109_snippet_selection.R")
```


## database connection


```{r load-config}


db_connection <- connect_database_configfile(
  config_filepath,
  database = working_dbname,
  profile = connection_profile
)


schemas <- read.csv(here::here(dbstructure_folder, "TABLES.csv")) %>%
  select(table, schema, geometry)

# These are clumsy, temporary, provisional helpers.
# But, hey, there will be time later.
get_schema <- function(tablelabel) {
  return(schemas %>%
    filter(table == tablelabel) %>%
    pull(schema)
  )
}
get_namestring <- function(tablelabel) glue::glue('"{get_schema(tablelabel)}"."{tablelabel}"')
get_tableid <- function(tablelabel) DBI::Id(schema = get_schema(tablelabel), table = tablelabel)


```

# dump all data, for safety

```{r step1-safety-dump}
now <- format(Sys.time(), "%Y%m%d%H%M")
dump_all(
    here::here("dumps", glue::glue("safedump_{working_dbname}_{now}.sql")),
    config_filepath = config_filepath,
    database = working_dbname,
    profile = "dumpall",
    user = "monkey",
    exclude_schema = c("tiger", "public")
    )
```

# Gather and Upload

## Prone "LocationAssessments"

LocationAssessments are tightly linked to a version of the sample.
However, we do not want to loose previous assessments.

As a first step, we remove all rows from the `LocationAssessments` which are apparrently untouched.

```{r delete-undone-location-assessments}

# clean up LocationAssessments
verb <- "DELETE "
sql_command <- glue::glue(
  '{verb} FROM "outbound"."LocationAssessments"
    WHERE ((log_user = \'yoda\') OR (log_user = \'falk\') OR (log_user = \'update\'))
      AND (NOT cell_disapproved)
      AND (revisit_disapproval IS NULL)
      AND (disapproval_explanation IS NULL)
      AND (type_suggested IS NULL)
      AND (implications_habitatmap IS NULL)
      AND (feedback_habitatmap IS NULL)
      AND (notes IS NULL)
      AND (NOT assessment_done)
    ;')
# print(sql_command)
rs <- DBI::dbExecute(
  db_connection,
  sql_command
  )
# print(rs)

# DBI::dbReadTable(
#   db_connection,
#   DBI::Id(schema = "outbound", table = "LocationAssessments"),
#   ) %>% collect() %>%
#     head() %>% knitr::kable()
```

The remainder are the ones we would like to store.

```{r load-previous-location-assessments}

# add location for previous LocationAssessment

previous_locations <- DBI::dbReadTable(
  db_connection,
  DBI::Id(schema = "outbound", table = "LocationAssessments"),
  ) %>% collect() %>%
  pull("grts_address") %>%
  as.integer()

```

## Locations

Starting from "orthophoto" the sample locations...

```{r orhtophoto-to-db-sample-locations-conversion}
sample_locations <- orthophoto_type_grts %>%
  select(-grts_address) %>%
  rename(
    grts_address = grts_address_final,
    previous_assessment = assessed_in_field,
    previous_assessment_date = assessment_date
  ) %>%
  mutate(
    across(c(
        type,
        grts_join_method,
        sp_poststratum,
        scheme_ps_targetpanels
      ),
      as.character
    )
  )

```


... we take the new locations *and* the ones which were assessed before.

```{r distinct-locations}
locations <- c(
    sample_locations %>% pull(grts_address) %>% as.integer(),
    previous_locations
  ) %>%
  tibble(grts_address = .) %>%
  distinct() %>%
  add_point_coords_grts(
    grts_var = "grts_address",
    spatrast = grts_mh,
    spatrast_index = grts_mh_index
  )

sf::st_geometry(locations) <- "wkb_geometry"

```


**Upload Spatial Locations:**

... in a "DELETE, then INSERT" approach.
Reminder: the DELETE cascades as is to the `LocationCells`.
Note that this is possible because I avoided the use of `location_id` as a foreign key.

```{r upload-locations}

rs <- DBI::dbExecute(
  db_connection,
  'DELETE FROM "metadata"."Locations";'
  )

locations_lookup <- upload_and_lookup(
  db_connection,
  DBI::Id(schema = "metadata", table = "Locations"),
  locations,
  ref_cols = "grts_address",
  index_col = "location_id"
)

```

## Location Cells (Polygons)

```{r location-polygon-upload}
units_cell_polygon[["grts_address_final"]] <-
  as.integer(units_cell_polygon[["grts_address_final"]])

# unit geometries (cells):
location_cells <-
  units_cell_polygon %>%
  inner_join(
    locations_lookup,
    by = join_by(grts_address_final == grts_address),
    relationship = "one-to-many",
    unmatched = "drop"
  ) %>%
  select(-grts_address_final) %>%
  relocate(geometry, .after = last_col())

sf::st_geometry(location_cells) <- "wkb_geometry"

# glimpse(location_cells)

append_tabledata(
  db_connection,
  DBI::Id(schema = "metadata", table = "LocationCells"),
  location_cells,
  reference_columns = "location_id"
)

```

## SampleLocations

Whereas `Locations` holds the GIS information of a location,
`SampleLocations` are a collection of metadata about each sample.

```{r refresh-sample-locations}
if ("location_id" %in% names(sample_locations)) {
  # should not be the case in a continuous script;
  # this is extra safety for debugging and de-serial execution
  sample_locations <- sample_locations %>%
    select(-location_id)
}
sample_locations <- sample_locations %>%
  left_join(
    locations_lookup,
    by = join_by(grts_address),
    relationship = "many-to-one"
  )


rs <- DBI::dbExecute(
  db_connection,
  'DELETE FROM "outbound"."SampleLocations";'
  )

append_tabledata(
  db_connection,
  DBI::Id(schema = "outbound", table = "SampleLocations"),
  sample_locations,
  reference_columns =
    c("type", "grts_address", "scheme_ps_targetpanels", "loceval_year")
)

```

## LocationAssessments

```{r refresh-location-assessments}

new_location_assessments <- sample_locations %>%
  select(
    location_id,
    grts_address,
    type
  ) %>%
  mutate(
    log_user = "update",
    log_update = as.POSIXct(Sys.time()),
    cell_disapproved = FALSE,
    assessment_done = FALSE
  )

previous_location_assessments <- DBI::dbReadTable(
  db_connection,
  DBI::Id(schema = "outbound", table = "LocationAssessments"),
  ) %>% collect()
# nrow(previous_location_assessments)

new_location_assessments <- new_location_assessments %>%
  anti_join(
    previous_location_assessments,
    by = join_by(type, grts_address)
  )

location_assessments <- bind_rows(
  previous_location_assessments,
  new_location_assessments
  ) %>%
  select(-locationassessment_id)
# nrow(location_assessments)
```
